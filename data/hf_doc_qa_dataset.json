{
    "data": [
        {
            "context": "\n\n\n\nTransformers documentation\n\t\t\t\n\n\nTransformers\n\n\n\n\n\n\n\n\nTransformers\n\n\n\n\n\ud83c\udfe1 View all docs\nAWS Trainium & Inferentia\nAccelerate\nArgilla\nAutoTrain\nBitsandbytes\nChat UI\nDataset viewer\nDatasets\nDeploying on AWS\nDiffusers\nDistilabel\nEvaluate\nGradio\nHub\nHub Python Library\nHuggingface.js\nInference Endpoints (dedicated)\nInference Providers\nLeRobot\nLeaderboards\nLighteval\nMicrosoft Azure\nOptimum\nPEFT\nSafetensors\nSentence Transformers\nTRL\nTasks\nText Embeddings Inference\nText Generation Inference\nTokenizers\nTransformers\nTransformers.js\nsmolagents\ntimm\n\n\n\n\nSearch documentation\n\n\n\n\n\n\nmain\nv4.53.2\nv4.52.3\nv4.51.3\nv4.50.0\nv4.49.0\nv4.48.2\nv4.47.1\nv4.46.3\nv4.45.2\nv4.44.2\nv4.43.4\nv4.42.4\nv4.41.2\nv4.40.2\nv4.39.3\nv4.38.2\nv4.37.2\nv4.36.1\nv4.35.2\nv4.34.1\nv4.33.3\nv4.32.1\nv4.31.0\nv4.30.0\nv4.29.1\nv4.28.1\nv4.27.2\nv4.26.1\nv4.25.1\nv4.24.0\nv4.23.1\nv4.22.2\nv4.21.3\nv4.20.1\nv4.19.4\nv4.18.0\nv4.17.0\nv4.16.2\nv4.15.0\nv4.14.1\nv4.13.0\nv4.12.5\nv4.11.3\nv4.10.1\nv4.9.2\nv4.8.2\nv4.7.0\nv4.6.0\nv4.5.1\nv4.4.2\nv4.3.3\nv4.2.2\nv4.1.1\nv4.0.1\nv3.5.1\nv3.4.0\nv3.3.1\nv3.2.0\nv3.1.0\nv3.0.2\nv2.11.0\nv2.10.0\nv2.9.1\nv2.8.0\nv2.7.0\nv2.6.0\nv2.5.1\nv2.4.1\nv2.3.0\nv2.2.2\nv2.1.1\nv2.0.0\nv1.2.0\nv1.1.0\nv1.0.0\ndoc-builder-html\n\n\nAR\nDE\nEN\nES\nFR\nHI\nIT\nJA\nKO\nPT\nTE\nTR\nZH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet started\n\n\n\n\n\n\nTransformers\n\n\nInstallation\n\n\nQuickstart\n\n\n\n\n\n\nBase classes\n\n\n\n\n\n\nInference\n\n\n\n\n\n\nTraining\n\n\n\n\n\n\nQuantization\n\n\n\n\n\n\nExport to production\n\n\n\n\n\n\nResources\n\n\n\n\n\n\nContribute\n\n\n\n\n\n\nAPI\n\n\n\n\n\n\n\n\n\n\n\n\nJoin the Hugging Face community\n\n\nand get access to the augmented documentation experience\n\t\t\n\n\n\n\nCollaborate on models, datasets and Spaces\n\t\t\t\t\n\n\n\n\nFaster examples with accelerated inference\n\t\t\t\t\n\n\n\n\nSwitch between documentation themes\n\t\t\t\t\n\n\nSign Up\n\n\nto get started\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \nTransformers\n \n \nTransformers acts as the model-definition framework for state-of-the-art machine learning models in text, computer\nvision, audio, video, and multimodal model, for both inference and training.\n \nIt centralizes the model definition so that this definition is agreed upon across the ecosystem. \ntransformers\n is the\npivot across frameworks: if a model definition is supported, it will be compatible with the majority of training\nframeworks (Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, \u2026), inference engines (vLLM, SGLang, TGI, \u2026),\nand adjacent modeling libraries (llama.cpp, mlx, \u2026) which leverage the model definition from \ntransformers\n.\n \nWe pledge to help support new state-of-the-art models and democratize their usage by having their model definition be\nsimple, customizable, and efficient.\n \nThere are over 1M+ Transformers \nmodel checkpoints\n on the \nHugging Face Hub\n you can use.\n \nExplore the \nHub\n today to find a model and use Transformers to help you get started right away.\n \n \nFeatures\n \nTransformers provides everything you need for inference or training with state-of-the-art pretrained models. Some of the main features include:\n \nPipeline\n: Simple and optimized inference class for many machine learning tasks like text generation, image segmentation, automatic speech recognition, document question answering, and more.\n \nTrainer\n: A comprehensive trainer that supports features such as mixed precision, torch.compile, and FlashAttention for training and distributed training for PyTorch models.\n \ngenerate\n: Fast text generation with large language models (LLMs) and vision language models (VLMs), including support for streaming and multiple decoding strategies.\n \n \nDesign\n \nRead our \nPhilosophy\n to learn more about Transformers\u2019 design principles.\n \nTransformers is designed for developers and machine learning engineers and researchers. Its main design principles are:\n \nFast and easy to use: Every model is implemented from only three main classes (configuration, model, and preprocessor) and can be quickly used for inference or training with \nPipeline\n or \nTrainer\n.\n \nPretrained models: Reduce your carbon footprint, compute cost and time by using a pretrained model instead of training an entirely new one. Each pretrained model is reproduced as closely as possible to the original model and offers state-of-the-art performance.\n \n \n \nLearn\n \nIf you\u2019re new to Transformers or want to learn more about transformer models, we recommend starting with the \nLLM course\n. This comprehensive course covers everything from the fundamentals of how transformer models work to practical applications across various tasks. You\u2019ll learn the complete workflow, from curating high-quality datasets to fine-tuning large language models and implementing reasoning capabilities. The course contains both theoretical and hands-on exercises to build a solid foundational knowledge of transformer models as you learn.\n \n<\n \n>\n \nUpdate\n on GitHub\n \n\n\n\n\n\n\n\n\nInstallation\n\u2192\n\n\n\n\n\n\nTransformers\n\n\nFeatures\n\n\nDesign\n\n\nLearn\n\n\n\n",
            "question": "How do I initialize the BERT tokenizer?",
            "answers": [
                {
                    "text": "BertTokenizer.from_pretrained('bert-base-uncased')",
                    "answer_start": -1
                }
            ]
        },
        {
            "context": "\n\n\n\nTransformers documentation\n\t\t\t\n\n\nInstallation\n\n\n\n\n\n\n\n\nTransformers\n\n\n\n\n\ud83c\udfe1 View all docs\nAWS Trainium & Inferentia\nAccelerate\nArgilla\nAutoTrain\nBitsandbytes\nChat UI\nDataset viewer\nDatasets\nDeploying on AWS\nDiffusers\nDistilabel\nEvaluate\nGradio\nHub\nHub Python Library\nHuggingface.js\nInference Endpoints (dedicated)\nInference Providers\nLeRobot\nLeaderboards\nLighteval\nMicrosoft Azure\nOptimum\nPEFT\nSafetensors\nSentence Transformers\nTRL\nTasks\nText Embeddings Inference\nText Generation Inference\nTokenizers\nTransformers\nTransformers.js\nsmolagents\ntimm\n\n\n\n\nSearch documentation\n\n\n\n\n\n\nmain\nv4.53.2\nv4.52.3\nv4.51.3\nv4.50.0\nv4.49.0\nv4.48.2\nv4.47.1\nv4.46.3\nv4.45.2\nv4.44.2\nv4.43.4\nv4.42.4\nv4.41.2\nv4.40.2\nv4.39.3\nv4.38.2\nv4.37.2\nv4.36.1\nv4.35.2\nv4.34.1\nv4.33.3\nv4.32.1\nv4.31.0\nv4.30.0\nv4.29.1\nv4.28.1\nv4.27.2\nv4.26.1\nv4.25.1\nv4.24.0\nv4.23.1\nv4.22.2\nv4.21.3\nv4.20.1\nv4.19.4\nv4.18.0\nv4.17.0\nv4.16.2\nv4.15.0\nv4.14.1\nv4.13.0\nv4.12.5\nv4.11.3\nv4.10.1\nv4.9.2\nv4.8.2\nv4.7.0\nv4.6.0\nv4.5.1\nv4.4.2\nv4.3.3\nv4.2.2\nv4.1.1\nv4.0.1\nv3.5.1\nv3.4.0\nv3.3.1\nv3.2.0\nv3.1.0\nv3.0.2\nv2.11.0\nv2.10.0\nv2.9.1\nv2.8.0\nv2.7.0\nv2.6.0\nv2.5.1\nv2.4.1\nv2.3.0\nv2.2.2\nv2.1.1\nv2.0.0\nv1.2.0\nv1.1.0\nv1.0.0\ndoc-builder-html\n\n\nAR\nDE\nEN\nES\nFR\nHI\nIT\nJA\nKO\nPT\nTE\nTR\nZH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet started\n\n\n\n\n\n\nTransformers\n\n\nInstallation\n\n\nQuickstart\n\n\n\n\n\n\nBase classes\n\n\n\n\n\n\nInference\n\n\n\n\n\n\nTraining\n\n\n\n\n\n\nQuantization\n\n\n\n\n\n\nExport to production\n\n\n\n\n\n\nResources\n\n\n\n\n\n\nContribute\n\n\n\n\n\n\nAPI\n\n\n\n\n\n\n\n\n\n\n\n\nJoin the Hugging Face community\n\n\nand get access to the augmented documentation experience\n\t\t\n\n\n\n\nCollaborate on models, datasets and Spaces\n\t\t\t\t\n\n\n\n\nFaster examples with accelerated inference\n\t\t\t\t\n\n\n\n\nSwitch between documentation themes\n\t\t\t\t\n\n\nSign Up\n\n\nto get started\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \nInstallation\n \nTransformers works with \nPyTorch\n, \nTensorFlow 2.0\n, and \nFlax\n. It has been tested on Python 3.9+, PyTorch 2.1+, TensorFlow 2.6+, and Flax 0.4.1+.\n \n \nVirtual environment\n \nA virtual environment helps manage different projects and avoids compatibility issues between dependencies. Take a look at the \nInstall packages in a virtual environment using pip and venv\n guide if you\u2019re unfamiliar with Python virtual environments.\n \nvenv \nuv \n \nCreate and activate a virtual environment in your project directory with \nvenv\n.\n \n \n Copied\n \npython -m venv .\nenv\n\n\nsource\n .\nenv\n/bin/activate\n \n \n \nPython\n \nYou can install Transformers with pip or uv.\n \npip \nuv \n \npip\n is a package installer for Python. Install Transformers with pip in your newly created virtual environment.\n \n \n Copied\n \npip install transformers\n \n \nFor GPU acceleration, install the appropriate CUDA drivers for \nPyTorch\n and \nTensorFlow\n.\n \nRun the command below to check if your system detects an NVIDIA GPU.\n \n \n Copied\n \nnvidia-smi\n \nTo install a CPU-only version of Transformers and a machine learning framework, run the following command.\n \nPyTorch \nTensorFlow \nFlax \n \n \n Copied\n \npip install \n'transformers[torch]'\n\nuv pip install \n'transformers[torch]'\n \n \nTest whether the install was successful with the following command. It should return a label and score for the provided text.\n \n \n Copied\n \npython -c \n\"from transformers import pipeline; print(pipeline('sentiment-analysis')('hugging face is the best'))\"\n\n[{\n'label'\n: \n'POSITIVE'\n, \n'score'\n: 0.9998704791069031}]\n \n \nSource install\n \nInstalling from source installs the \nlatest\n version rather than the \nstable\n version of the library. It ensures you have the most up-to-date changes in Transformers and it\u2019s useful for experimenting with the latest features or fixing a bug that hasn\u2019t been officially released in the stable version yet.\n \nThe downside is that the latest version may not always be stable. If you encounter any problems, please open a \nGitHub Issue\n so we can fix it as soon as possible.\n \nInstall from source with the following command.\n \n \n Copied\n \npip install git+https://github.com/huggingface/transformers\n \nCheck if the install was successful with the command below. It should return a label and score for the provided text.\n \n \n Copied\n \npython -c \n\"from transformers import pipeline; print(pipeline('sentiment-analysis')('hugging face is the best'))\"\n\n[{\n'label'\n: \n'POSITIVE'\n, \n'score'\n: 0.9998704791069031}]\n \n \nEditable install\n \nAn \neditable install\n is useful if you\u2019re developing locally with Transformers. It links your local copy of Transformers to the Transformers \nrepository\n instead of copying the files. The files are added to Python\u2019s import path.\n \n \n Copied\n \ngit \nclone\n https://github.com/huggingface/transformers.git\n\ncd\n transformers\npip install -e .\n \nYou must keep the local Transformers folder to keep using it.\n \nUpdate your local version of Transformers with the latest changes in the main repository with the following command.\n \n \n Copied\n \ncd\n ~/transformers/\ngit pull\n \n \nconda\n \nconda\n is a language-agnostic package manager. Install Transformers from the \nconda-forge\n channel in your newly created virtual environment.\n \n \n Copied\n \nconda install conda-forge::transformers\n \n \nSet up\n \nAfter installation, you can configure the Transformers cache location or set up the library for offline usage.\n \n \nCache directory\n \nWhen you load a pretrained model with \nfrom_pretrained()\n, the model is downloaded from the Hub and locally cached.\n \nEvery time you load a model, it checks whether the cached model is up-to-date. If it\u2019s the same, then the local model is loaded. If it\u2019s not the same, the newer model is downloaded and cached.\n \nThe default directory given by the shell environment variable \nTRANSFORMERS_CACHE\n is \n~/.cache/huggingface/hub\n. On Windows, the default directory is \nC:\\Users\\username\\.cache\\huggingface\\hub\n.\n \nCache a model in a different directory by changing the path in the following shell environment variables (listed by priority).\n \nHF_HUB_CACHE\n or \nTRANSFORMERS_CACHE\n (default)\n \nHF_HOME\n \nXDG_CACHE_HOME\n + \n/huggingface\n (only if \nHF_HOME\n is not set)\n \nOlder versions of Transformers uses the shell environment variables \nPYTORCH_TRANSFORMERS_CACHE\n or \nPYTORCH_PRETRAINED_BERT_CACHE\n. You should keep these unless you specify the newer shell environment variable \nTRANSFORMERS_CACHE\n.\n \n \nOffline mode\n \nTo use Transformers in an offline or firewalled environment requires the downloaded and cached files ahead of time. Download a model repository from the Hub with the \nsnapshot_download\n method.\n \nRefer to the \nDownload files from the Hub\n guide for more options for downloading files from the Hub. You can download files from specific revisions, download from the CLI, and even filter which files to download from a repository.\n \n \n Copied\n \nfrom\n huggingface_hub \nimport\n snapshot_download\n\nsnapshot_download(repo_id=\n\"meta-llama/Llama-2-7b-hf\"\n, repo_type=\n\"model\"\n)\n \nSet the environment variable \nHF_HUB_OFFLINE=1\n to prevent HTTP calls to the Hub when loading a model.\n \n \n Copied\n \nHF_HUB_OFFLINE=1 \\\npython examples/pytorch/language-modeling/run_clm.py --model_name_or_path meta-llama/Llama-2-7b-hf --dataset_name wikitext ...\n \nAnother option for only loading cached files is to set \nlocal_files_only=True\n in \nfrom_pretrained()\n.\n \n \n Copied\n \nfrom\n transformers \nimport\n LlamaForCausalLM\n\nmodel = LlamaForCausalLM.from_pretrained(\n\"./path/to/local/directory\"\n, local_files_only=\nTrue\n)\n \n<\n \n>\n \nUpdate\n on GitHub\n \n\n\n\n\n\n\n\u2190\nTransformers\n\n\nQuickstart\n\u2192\n\n\n\n\n\n\nInstallation\n\n\nVirtual environment\n\n\nPython\n\n\nSource install\n\n\nEditable install\n\n\nconda\n\n\nSet up\n\n\nCache directory\n\n\nOffline mode\n\n\n\n",
            "question": "How do I initialize the BERT tokenizer?",
            "answers": [
                {
                    "text": "BertTokenizer.from_pretrained('bert-base-uncased')",
                    "answer_start": -1
                }
            ]
        },
        {
            "context": "\n\n\n\nTransformers documentation\n\t\t\t\n\n\nPipelines\n\n\n\n\n\n\n\n\nTransformers\n\n\n\n\n\ud83c\udfe1 View all docs\nAWS Trainium & Inferentia\nAccelerate\nArgilla\nAutoTrain\nBitsandbytes\nChat UI\nDataset viewer\nDatasets\nDeploying on AWS\nDiffusers\nDistilabel\nEvaluate\nGradio\nHub\nHub Python Library\nHuggingface.js\nInference Endpoints (dedicated)\nInference Providers\nLeRobot\nLeaderboards\nLighteval\nMicrosoft Azure\nOptimum\nPEFT\nSafetensors\nSentence Transformers\nTRL\nTasks\nText Embeddings Inference\nText Generation Inference\nTokenizers\nTransformers\nTransformers.js\nsmolagents\ntimm\n\n\n\n\nSearch documentation\n\n\n\n\n\n\nmain\nv4.53.2\nv4.52.3\nv4.51.3\nv4.50.0\nv4.49.0\nv4.48.2\nv4.47.1\nv4.46.3\nv4.45.2\nv4.44.2\nv4.43.4\nv4.42.4\nv4.41.2\nv4.40.2\nv4.39.3\nv4.38.2\nv4.37.2\nv4.36.1\nv4.35.2\nv4.34.1\nv4.33.3\nv4.32.1\nv4.31.0\nv4.30.0\nv4.29.1\nv4.28.1\nv4.27.2\nv4.26.1\nv4.25.1\nv4.24.0\nv4.23.1\nv4.22.2\nv4.21.3\nv4.20.1\nv4.19.4\nv4.18.0\nv4.17.0\nv4.16.2\nv4.15.0\nv4.14.1\nv4.13.0\nv4.12.5\nv4.11.3\nv4.10.1\nv4.9.2\nv4.8.2\nv4.7.0\nv4.6.0\nv4.5.1\nv4.4.2\nv4.3.3\nv4.2.2\nv4.1.1\nv4.0.1\nv3.5.1\nv3.4.0\nv3.3.1\nv3.2.0\nv3.1.0\nv3.0.2\nv2.11.0\nv2.10.0\nv2.9.1\nv2.8.0\nv2.7.0\nv2.6.0\nv2.5.1\nv2.4.1\nv2.3.0\nv2.2.2\nv2.1.1\nv2.0.0\nv1.2.0\nv1.1.0\nv1.0.0\ndoc-builder-html\n\n\nAR\nDE\nEN\nES\nFR\nHI\nIT\nJA\nKO\nPT\nTE\nTR\nZH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet started\n\n\n\n\n\n\nTransformers\n\n\nInstallation\n\n\nQuickstart\n\n\n\n\n\n\nBase classes\n\n\n\n\n\n\nInference\n\n\n\n\n\n\nTraining\n\n\n\n\n\n\nQuantization\n\n\n\n\n\n\nExport to production\n\n\n\n\n\n\nResources\n\n\n\n\n\n\nContribute\n\n\n\n\n\n\nAPI\n\n\n\n\n\n\n\n\nMain Classes\n\n\n\n\n\n\nAuto Classes\n\n\nBackbones\n\n\nCallbacks\n\n\nConfiguration\n\n\nData Collator\n\n\nKeras callbacks\n\n\nLogging\n\n\nModels\n\n\nText Generation\n\n\nONNX\n\n\nOptimization\n\n\nModel outputs\n\n\nPEFT\n\n\nPipelines\n\n\nProcessors\n\n\nQuantization\n\n\nTokenizer\n\n\nTrainer\n\n\nDeepSpeed\n\n\nExecuTorch\n\n\nFeature Extractor\n\n\nImage Processor\n\n\nVideo Processor\n\n\n\n\n\n\nModels\n\n\n\n\n\n\nInternal helpers\n\n\n\n\n\n\nReference\n\n\n\n\n\n\n\n\n\n\n\n\nJoin the Hugging Face community\n\n\nand get access to the augmented documentation experience\n\t\t\n\n\n\n\nCollaborate on models, datasets and Spaces\n\t\t\t\t\n\n\n\n\nFaster examples with accelerated inference\n\t\t\t\t\n\n\n\n\nSwitch between documentation themes\n\t\t\t\t\n\n\nSign Up\n\n\nto get started\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \nPipelines\n \nThe pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of\nthe complex code from the library, offering a simple API dedicated to several tasks, including Named Entity\nRecognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering. See the\n\ntask summary\n for examples of use.\n \nThere are two categories of pipeline abstractions to be aware about:\n \nThe \npipeline()\n which is the most powerful object encapsulating all other pipelines.\n \nTask-specific pipelines are available for \naudio\n, \ncomputer vision\n, \nnatural language processing\n, and \nmultimodal\n tasks.\n \n \nThe pipeline abstraction\n \nThe \npipeline\n abstraction is a wrapper around all the other available pipelines. It is instantiated as any other\npipeline but can provide additional quality of life.\n \nSimple call on one item:\n \n \n Copied\n \n>>> \npipe = pipeline(\n\"text-classification\"\n)\n\n>>> \npipe(\n\"This restaurant is awesome\"\n)\n[{\n'label'\n: \n'POSITIVE'\n, \n'score'\n: \n0.9998743534088135\n}]\n \nIf you want to use a specific model from the \nhub\n you can ignore the task if the model on\nthe hub already defines it:\n \n \n Copied\n \n>>> \npipe = pipeline(model=\n\"FacebookAI/roberta-large-mnli\"\n)\n\n>>> \npipe(\n\"This restaurant is awesome\"\n)\n[{\n'label'\n: \n'NEUTRAL'\n, \n'score'\n: \n0.7313136458396912\n}]\n \nTo call a pipeline on many items, you can call it with a \nlist\n.\n \n \n Copied\n \n>>> \npipe = pipeline(\n\"text-classification\"\n)\n\n>>> \npipe([\n\"This restaurant is awesome\"\n, \n\"This restaurant is awful\"\n])\n[{\n'label'\n: \n'POSITIVE'\n, \n'score'\n: \n0.9998743534088135\n},\n {\n'label'\n: \n'NEGATIVE'\n, \n'score'\n: \n0.9996669292449951\n}]\n \nTo iterate over full datasets it is recommended to use a \ndataset\n directly. This means you don\u2019t need to allocate\nthe whole dataset at once, nor do you need to do batching yourself. This should work just as fast as custom loops on\nGPU. If it doesn\u2019t don\u2019t hesitate to create an issue.\n \n \n Copied\n \nimport\n datasets\n\nfrom\n transformers \nimport\n pipeline\n\nfrom\n transformers.pipelines.pt_utils \nimport\n KeyDataset\n\nfrom\n tqdm.auto \nimport\n tqdm\n\npipe = pipeline(\n\"automatic-speech-recognition\"\n, model=\n\"facebook/wav2vec2-base-960h\"\n, device=\n0\n)\ndataset = datasets.load_dataset(\n\"superb\"\n, name=\n\"asr\"\n, split=\n\"test\"\n)\n\n\n# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\n\n\n# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset\n\n\nfor\n out \nin\n tqdm(pipe(KeyDataset(dataset, \n\"file\"\n))):\n    \nprint\n(out)\n    \n# {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\n\n    \n# {\"text\": ....}\n\n    \n# ....\n \nFor ease of use, a generator is also possible:\n \n \n Copied\n \nfrom\n transformers \nimport\n pipeline\n\npipe = pipeline(\n\"text-classification\"\n)\n\n\n\ndef\n \ndata\n():\n    \nwhile\n \nTrue\n:\n        \n# This could come from a dataset, a database, a queue or HTTP request\n\n        \n# in a server\n\n        \n# Caveat: because this is iterative, you cannot use `num_workers > 1` variable\n\n        \n# to use multiple threads to preprocess data. You can still have 1 thread that\n\n        \n# does the preprocessing while the main runs the big inference\n\n        \nyield\n \n\"This is a test\"\n\n\n\n\nfor\n out \nin\n pipe(data()):\n    \nprint\n(out)\n    \n# {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\n\n    \n# {\"text\": ....}\n\n    \n# ....\n \n \ntransformers.pipeline\n \n \n<\n \nsource\n \n>\n \n(\n \ntask\n: typing.Optional[str] = None\n \nmodel\n: typing.Union[str, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel'), NoneType] = None\n \nconfig\n: typing.Union[str, transformers.configuration_utils.PretrainedConfig, NoneType] = None\n \ntokenizer\n: typing.Union[str, transformers.tokenization_utils.PreTrainedTokenizer, ForwardRef('PreTrainedTokenizerFast'), NoneType] = None\n \nfeature_extractor\n: typing.Union[str, ForwardRef('SequenceFeatureExtractor'), NoneType] = None\n \nimage_processor\n: typing.Union[str, transformers.image_processing_utils.BaseImageProcessor, NoneType] = None\n \nprocessor\n: typing.Union[str, transformers.processing_utils.ProcessorMixin, NoneType] = None\n \nframework\n: typing.Optional[str] = None\n \nrevision\n: typing.Optional[str] = None\n \nuse_fast\n: bool = True\n \ntoken\n: typing.Union[str, bool, NoneType] = None\n \ndevice\n: typing.Union[int, str, ForwardRef('torch.device'), NoneType] = None\n \ndevice_map\n: typing.Union[str, dict[str, typing.Union[int, str]], NoneType] = None\n \ntorch_dtype\n: typing.Union[str, ForwardRef('torch.dtype'), NoneType] = 'auto'\n \ntrust_remote_code\n: typing.Optional[bool] = None\n \nmodel_kwargs\n: typing.Optional[dict[str, typing.Any]] = None\n \npipeline_class\n: typing.Optional[typing.Any] = None\n \n**kwargs\n: typing.Any\n \n \n)\n \n\u2192\n \nPipeline\n \n \nParameters \n \n \ntask\n (\nstr\n) \u2014\nThe task defining which pipeline will be returned. Currently accepted tasks are:\n\n\n\n\n\"audio-classification\"\n: will return a \nAudioClassificationPipeline\n.\n\n\n\"automatic-speech-recognition\"\n: will return a \nAutomaticSpeechRecognitionPipeline\n.\n\n\n\"depth-estimation\"\n: will return a \nDepthEstimationPipeline\n.\n\n\n\"document-question-answering\"\n: will return a \nDocumentQuestionAnsweringPipeline\n.\n\n\n\"feature-extraction\"\n: will return a \nFeatureExtractionPipeline\n.\n\n\n\"fill-mask\"\n: will return a \nFillMaskPipeline\n:.\n\n\n\"image-classification\"\n: will return a \nImageClassificationPipeline\n.\n\n\n\"image-feature-extraction\"\n: will return an \nImageFeatureExtractionPipeline\n.\n\n\n\"image-segmentation\"\n: will return a \nImageSegmentationPipeline\n.\n\n\n\"image-text-to-text\"\n: will return a \nImageTextToTextPipeline\n.\n\n\n\"image-to-image\"\n: will return a \nImageToImagePipeline\n.\n\n\n\"image-to-text\"\n: will return a \nImageToTextPipeline\n.\n\n\n\"mask-generation\"\n: will return a \nMaskGenerationPipeline\n.\n\n\n\"object-detection\"\n: will return a \nObjectDetectionPipeline\n.\n\n\n\"question-answering\"\n: will return a \nQuestionAnsweringPipeline\n.\n\n\n\"summarization\"\n: will return a \nSummarizationPipeline\n.\n\n\n\"table-question-answering\"\n: will return a \nTableQuestionAnsweringPipeline\n.\n\n\n\"text2text-generation\"\n: will return a \nText2TextGenerationPipeline\n.\n\n\n\"text-classification\"\n (alias \n\"sentiment-analysis\"\n available): will return a\n\nTextClassificationPipeline\n.\n\n\n\"text-generation\"\n: will return a \nTextGenerationPipeline\n:.\n\n\n\"text-to-audio\"\n (alias \n\"text-to-speech\"\n available): will return a \nTextToAudioPipeline\n:.\n\n\n\"token-classification\"\n (alias \n\"ner\"\n available): will return a \nTokenClassificationPipeline\n.\n\n\n\"translation\"\n: will return a \nTranslationPipeline\n.\n\n\n\"translation_xx_to_yy\"\n: will return a \nTranslationPipeline\n.\n\n\n\"video-classification\"\n: will return a \nVideoClassificationPipeline\n.\n\n\n\"visual-question-answering\"\n: will return a \nVisualQuestionAnsweringPipeline\n.\n\n\n\"zero-shot-classification\"\n: will return a \nZeroShotClassificationPipeline\n.\n\n\n\"zero-shot-image-classification\"\n: will return a \nZeroShotImageClassificationPipeline\n.\n\n\n\"zero-shot-audio-classification\"\n: will return a \nZeroShotAudioClassificationPipeline\n.\n\n\n\"zero-shot-object-detection\"\n: will return a \nZeroShotObjectDetectionPipeline\n.\n\n\n \n \n \nmodel\n (\nstr\n or \nPreTrainedModel\n or \nTFPreTrainedModel\n, \noptional\n) \u2014\nThe model that will be used by the pipeline to make predictions. This can be a model identifier or an\nactual instance of a pretrained model inheriting from \nPreTrainedModel\n (for PyTorch) or\n\nTFPreTrainedModel\n (for TensorFlow).\n\n\nIf not provided, the default for the \ntask\n will be loaded.\n \n \n \nconfig\n (\nstr\n or \nPretrainedConfig\n, \noptional\n) \u2014\nThe configuration that will be used by the pipeline to instantiate the model. This can be a model\nidentifier or an actual pretrained model configuration inheriting from \nPretrainedConfig\n.\n\n\nIf not provided, the default configuration file for the requested model will be used. That means that if\n\nmodel\n is given, its default configuration will be used. However, if \nmodel\n is not supplied, this\n\ntask\n\u2019s default model\u2019s config is used instead.\n \n \n \ntokenizer\n (\nstr\n or \nPreTrainedTokenizer\n, \noptional\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This can be a model\nidentifier or an actual pretrained tokenizer inheriting from \nPreTrainedTokenizer\n.\n\n\nIf not provided, the default tokenizer for the given \nmodel\n will be loaded (if it is a string). If \nmodel\n\nis not specified or not a string, then the default tokenizer for \nconfig\n is loaded (if it is a string).\nHowever, if \nconfig\n is also not given or not a string, then the default tokenizer for the given \ntask\n\nwill be loaded.\n \n \n \nfeature_extractor\n (\nstr\n or \nPreTrainedFeatureExtractor\n, \noptional\n) \u2014\nThe feature extractor that will be used by the pipeline to encode data for the model. This can be a model\nidentifier or an actual pretrained feature extractor inheriting from \nPreTrainedFeatureExtractor\n.\n\n\nFeature extractors are used for non-NLP models, such as Speech or Vision models as well as multi-modal\nmodels. Multi-modal models will also require a tokenizer to be passed.\n\n\nIf not provided, the default feature extractor for the given \nmodel\n will be loaded (if it is a string). If\n\nmodel\n is not specified or not a string, then the default feature extractor for \nconfig\n is loaded (if it\nis a string). However, if \nconfig\n is also not given or not a string, then the default feature extractor\nfor the given \ntask\n will be loaded.\n \n \n \nimage_processor\n (\nstr\n or \nBaseImageProcessor\n, \noptional\n) \u2014\nThe image processor that will be used by the pipeline to preprocess images for the model. This can be a\nmodel identifier or an actual image processor inheriting from \nBaseImageProcessor\n.\n\n\nImage processors are used for Vision models and multi-modal models that require image inputs. Multi-modal\nmodels will also require a tokenizer to be passed.\n\n\nIf not provided, the default image processor for the given \nmodel\n will be loaded (if it is a string). If\n\nmodel\n is not specified or not a string, then the default image processor for \nconfig\n is loaded (if it is\na string).\n \n \n \nprocessor\n (\nstr\n or \nProcessorMixin\n, \noptional\n) \u2014\nThe processor that will be used by the pipeline to preprocess data for the model. This can be a model\nidentifier or an actual processor inheriting from \nProcessorMixin\n.\n\n\nProcessors are used for multi-modal models that require multi-modal inputs, for example, a model that\nrequires both text and image inputs.\n\n\nIf not provided, the default processor for the given \nmodel\n will be loaded (if it is a string). If \nmodel\n\nis not specified or not a string, then the default processor for \nconfig\n is loaded (if it is a string).\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \nrevision\n (\nstr\n, \noptional\n, defaults to \n\"main\"\n) \u2014\nWhen passing a task name or a string model identifier: The specific model version to use. It can be a\nbranch name, a tag name, or a commit id, since we use a git-based system for storing models and other\nartifacts on huggingface.co, so \nrevision\n can be any identifier allowed by git.\n \n \n \nuse_fast\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether or not to use a Fast tokenizer if possible (a \nPreTrainedTokenizerFast\n).\n \n \n \nuse_auth_token\n (\nstr\n or \nbool\n, \noptional\n) \u2014\nThe token to use as HTTP bearer authorization for remote files. If \nTrue\n, will use the token generated\nwhen running \nhuggingface-cli login\n (stored in \n~/.huggingface\n).\n \n \n \ndevice\n (\nint\n or \nstr\n or \ntorch.device\n) \u2014\nDefines the device (\ne.g.\n, \n\"cpu\"\n, \n\"cuda:1\"\n, \n\"mps\"\n, or a GPU ordinal rank like \n1\n) on which this\npipeline will be allocated.\n \n \n \ndevice_map\n (\nstr\n or \ndict[str, Union[int, str, torch.device]\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut). When \naccelerate\n library is present, set\n\ndevice_map=\"auto\"\n to compute the most optimized \ndevice_map\n automatically (see\n\nhere\n\nfor more information).\n\n\n\n\nDo not use \ndevice_map\n AND \ndevice\n at the same time as they will conflict\n\n\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n).\n \n \n \ntrust_remote_code\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to allow for custom code defined on the Hub in their own modeling, configuration,\ntokenization or even pipeline files. This option should only be set to \nTrue\n for repositories you trust\nand in which you have read the code, as it will execute code present on the Hub on your local machine.\n \n \n \nmodel_kwargs\n (\ndict[str, Any]\n, \noptional\n) \u2014\nAdditional dictionary of keyword arguments passed along to the model\u2019s \nfrom_pretrained(..., **model_kwargs)\n function.\n \n \n \nkwargs\n (\ndict[str, Any]\n, \noptional\n) \u2014\nAdditional keyword arguments passed along to the specific pipeline init (see the documentation for the\ncorresponding pipeline class for possible values).\n \n \n \nReturns\n \n\n\nPipeline\n\n\n \n \n\n\nA suitable pipeline for the task.\n\n\n \n \nUtility factory method to build a \nPipeline\n.\n \nA pipeline consists of:\n \nOne or more components for pre-processing model inputs, such as a \ntokenizer\n,\n\nimage_processor\n, \nfeature_extractor\n, or \nprocessor\n.\n \nA \nmodel\n that generates predictions from the inputs.\n \nOptional post-processing steps to refine the model\u2019s output, which can also be handled by processors.\n \nWhile there are such optional arguments as `tokenizer`, `feature_extractor`, `image_processor`, and `processor`,\nthey shouldn't be specified all at once. If these components are not provided, `pipeline` will try to load\nrequired ones automatically. In case you want to provide these components explicitly, please refer to a\nspecific pipeline in order to get more details regarding what components are required.\n \n \nExamples:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline, AutoModelForTokenClassification, AutoTokenizer\n\n\n>>> \n# Sentiment analysis pipeline\n\n\n>>> \nanalyzer = pipeline(\n\"sentiment-analysis\"\n)\n\n\n>>> \n# Question answering pipeline, specifying the checkpoint identifier\n\n\n>>> \noracle = pipeline(\n\n... \n    \n\"question-answering\"\n, model=\n\"distilbert/distilbert-base-cased-distilled-squad\"\n, tokenizer=\n\"google-bert/bert-base-cased\"\n\n\n... \n)\n\n\n>>> \n# Named entity recognition pipeline, passing in a specific model and tokenizer\n\n\n>>> \nmodel = AutoModelForTokenClassification.from_pretrained(\n\"dbmdz/bert-large-cased-finetuned-conll03-english\"\n)\n\n>>> \ntokenizer = AutoTokenizer.from_pretrained(\n\"google-bert/bert-base-cased\"\n)\n\n>>> \nrecognizer = pipeline(\n\"ner\"\n, model=model, tokenizer=tokenizer)\n \n \nPipeline batching\n \nAll pipelines can use batching. This will work\nwhenever the pipeline uses its streaming ability (so when passing lists or \nDataset\n or \ngenerator\n).\n \n \n Copied\n \nfrom\n transformers \nimport\n pipeline\n\nfrom\n transformers.pipelines.pt_utils \nimport\n KeyDataset\n\nimport\n datasets\n\ndataset = datasets.load_dataset(\n\"imdb\"\n, name=\n\"plain_text\"\n, split=\n\"unsupervised\"\n)\npipe = pipeline(\n\"text-classification\"\n, device=\n0\n)\n\nfor\n out \nin\n pipe(KeyDataset(dataset, \n\"text\"\n), batch_size=\n8\n, truncation=\n\"only_first\"\n):\n    \nprint\n(out)\n    \n# [{'label': 'POSITIVE', 'score': 0.9998743534088135}]\n\n    \n# Exactly the same output as before, but the content are passed\n\n    \n# as batches to the model\n \nHowever, this is not automatically a win for performance. It can be either a 10x speedup or 5x slowdown depending\non hardware, data and the actual model being used.\n \nExample where it\u2019s mostly a speedup:\n \n \n Copied\n \nfrom\n transformers \nimport\n pipeline\n\nfrom\n torch.utils.data \nimport\n Dataset\n\nfrom\n tqdm.auto \nimport\n tqdm\n\npipe = pipeline(\n\"text-classification\"\n, device=\n0\n)\n\n\n\nclass\n \nMyDataset\n(\nDataset\n):\n    \ndef\n \n__len__\n(\nself\n):\n        \nreturn\n \n5000\n\n\n    \ndef\n \n__getitem__\n(\nself, i\n):\n        \nreturn\n \n\"This is a test\"\n\n\n\ndataset = MyDataset()\n\n\nfor\n batch_size \nin\n [\n1\n, \n8\n, \n64\n, \n256\n]:\n    \nprint\n(\n\"-\"\n * \n30\n)\n    \nprint\n(\nf\"Streaming batch_size=\n{batch_size}\n\"\n)\n    \nfor\n out \nin\n tqdm(pipe(dataset, batch_size=batch_size), total=\nlen\n(dataset)):\n        \npass\n \n \n Copied\n \n# On GTX 970\n------------------------------\n\nStreaming no batching\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:26<00:00, 187.52it/s]\n\n------------------------------\nStreaming batch_size=8\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:04<00:00, 1205.95it/s]\n------------------------------\n\nStreaming batch\n_size=64\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:02<00:00, 2478.24it/s]\n------------------------------\nStreaming batch_\nsize=256\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:01<00:00, 2554.43it/s]\n(diminishing returns, saturated the GPU)\n \nExample where it\u2019s most a slowdown:\n \n \n Copied\n \nclass\n \nMyDataset\n(\nDataset\n):\n    \ndef\n \n__len__\n(\nself\n):\n        \nreturn\n \n5000\n\n\n    \ndef\n \n__getitem__\n(\nself, i\n):\n        \nif\n i % \n64\n == \n0\n:\n            n = \n100\n\n        \nelse\n:\n            n = \n1\n\n        \nreturn\n \n\"This is a test\"\n * n\n \nThis is a occasional very long sentence compared to the other. In that case, the \nwhole\n batch will need to be 400\ntokens long, so the whole batch will be [64, 400] instead of [64, 4], leading to the high slowdown. Even worse, on\nbigger batches, the program simply crashes.\n \n \n Copied\n \n------------------------------\n\nStreaming no batching\n\n100\n%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| \n1000\n/\n1000\n [\n00\n:\n05\n<\n00\n:\n00\n, \n183.69\nit\n/s]\n\n------------------------------\n\nStreaming batch_size=\n8\n\n\n100\n%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| \n1000\n/\n1000\n [\n00\n:\n03\n<\n00\n:\n00\n, \n265.74\nit\n/s]\n\n------------------------------\n\nStreaming batch_size=\n64\n\n\n100\n%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| \n1000\n/\n1000\n [\n00\n:\n26\n<\n00\n:\n00\n, \n37.80\nit\n/s]\n\n------------------------------\n\nStreaming batch_size=\n256\n\n  \n0\n%|                                                                                 | \n0\n/\n1000\n [\n00\n:\n00\n<?\n, ?\nit\n/s]\nTraceback (most recent call \nlast\n):\n  File \n\"/home/nicolas/src/transformers/test.py\"\n, \nline\n \n42\n, \nin\n <module>\n    \nfor\n out \nin\n tqdm(pipe(dataset, batch_size=\n256\n), total=\nlen\n(dataset)):\n....\n    q = q / math.\nsqrt\n(dim_per_head)  \n# (bs, n_heads, q_length, dim_per_head)\n\nRuntimeError: CUDA out \nof\n memory. Tried \nto\n allocate \n376.00\n MiB (GPU \n0\n; \n3.95\n GiB total capacity; \n1.72\n GiB already allocated; \n354.88\n MiB free; \n2.46\n GiB reserved \nin\n total \nby\n PyTorch)\n \nThere are no good (general) solutions for this problem, and your mileage may vary depending on your use cases. Rule of\nthumb:\n \nFor users, a rule of thumb is:\n \nMeasure performance on your load, with your hardware. Measure, measure, and keep measuring. Real numbers are the\nonly way to go.\n \nIf you are latency constrained (live product doing inference), don\u2019t batch.\n \nIf you are using CPU, don\u2019t batch.\n \nIf you are using throughput (you want to run your model on a bunch of static data), on GPU, then:\n \nIf you have no clue about the size of the sequence_length (\u201cnatural\u201d data), by default don\u2019t batch, measure and\ntry tentatively to add it, add OOM checks to recover when it will fail (and it will at some point if you don\u2019t\ncontrol the sequence_length.)\n \nIf your sequence_length is super regular, then batching is more likely to be VERY interesting, measure and push\nit until you get OOMs.\n \nThe larger the GPU the more likely batching is going to be more interesting\n \nAs soon as you enable batching, make sure you can handle OOMs nicely.\n \n \nPipeline chunk batching\n \nzero-shot-classification\n and \nquestion-answering\n are slightly specific in the sense, that a single input might yield\nmultiple forward pass of a model. Under normal circumstances, this would yield issues with \nbatch_size\n argument.\n \nIn order to circumvent this issue, both of these pipelines are a bit specific, they are \nChunkPipeline\n instead of\nregular \nPipeline\n. In short:\n \n \n Copied\n \npreprocessed = pipe.preprocess(inputs)\nmodel_outputs = pipe.forward(preprocessed)\noutputs = pipe.postprocess(model_outputs)\n \nNow becomes:\n \n \n Copied\n \nall_model_outputs = []\n\nfor\n preprocessed \nin\n pipe.preprocess(inputs):\n    model_outputs = pipe.forward(preprocessed)\n    all_model_outputs.append(model_outputs)\noutputs = pipe.postprocess(all_model_outputs)\n \nThis should be very transparent to your code because the pipelines are used in\nthe same way.\n \nThis is a simplified view, since the pipeline can handle automatically the batch to ! Meaning you don\u2019t have to care\nabout how many forward passes you inputs are actually going to trigger, you can optimize the \nbatch_size\n\nindependently of the inputs. The caveats from the previous section still apply.\n \n \nPipeline FP16 inference\n \nModels can be run in FP16 which can be significantly faster on GPU while saving memory. Most models will not suffer noticeable performance loss from this. The larger the model, the less likely that it will.\n \nTo enable FP16 inference, you can simply pass \ntorch_dtype=torch.float16\n or \ntorch_dtype='float16'\n to the pipeline constructor. Note that this only works for models with a PyTorch backend. Your inputs will be converted to FP16 internally.\n \n \nPipeline custom code\n \nIf you want to override a specific pipeline.\n \nDon\u2019t hesitate to create an issue for your task at hand, the goal of the pipeline is to be easy to use and support most\ncases, so \ntransformers\n could maybe support your use case.\n \nIf you want to try simply you can:\n \nSubclass your pipeline of choice\n \n \n Copied\n \nclass\n \nMyPipeline\n(\nTextClassificationPipeline\n):\n    \ndef\n \npostprocess\n():\n        \n# Your code goes here\n\n        scores = scores * \n100\n\n        \n# And here\n\n\n\nmy_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)\n\n# or if you use *pipeline* function, then:\n\nmy_pipeline = pipeline(model=\n\"xxxx\"\n, pipeline_class=MyPipeline)\n \nThat should enable you to do all the custom code you want.\n \n \nImplementing a pipeline\n \nImplementing a new pipeline\n \n \nAudio\n \nPipelines available for audio tasks include the following.\n \n \nAudioClassificationPipeline\n \n \nclass\n \ntransformers.\nAudioClassificationPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \nfeature_extractor\n (\nSequenceFeatureExtractor\n) \u2014\nThe feature extractor that will be used by the pipeline to encode data for the model. This object inherits from\n\nSequenceFeatureExtractor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nAudio classification pipeline using any \nAutoModelForAudioClassification\n. This pipeline predicts the class of a\nraw waveform or an audio file. In case of an audio file, ffmpeg should be installed to support multiple audio\nformats.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \nclassifier = pipeline(model=\n\"superb/wav2vec2-base-superb-ks\"\n)\n\n>>> \nclassifier(\n\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\"\n)\n[{\n'score'\n: \n0.997\n, \n'label'\n: \n'_unknown_'\n}, {\n'score'\n: \n0.002\n, \n'label'\n: \n'left'\n}, {\n'score'\n: \n0.0\n, \n'label'\n: \n'yes'\n}, {\n'score'\n: \n0.0\n, \n'label'\n: \n'down'\n}, {\n'score'\n: \n0.0\n, \n'label'\n: \n'stop'\n}]\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"audio-classification\"\n.\n \nSee the list of available models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \ninputs\n: typing.Union[numpy.ndarray, bytes, str, dict]\n \n**kwargs\n: typing.Any\n \n \n)\n \n\u2192\n \nA list of \ndict\n with the following keys\n \n \nParameters \n \n \ninputs\n (\nnp.ndarray\n or \nbytes\n or \nstr\n or \ndict\n) \u2014\nThe inputs is either :\n\n\nstr\n that is the filename of the audio file, the file will be read at the correct sampling rate\nto get the waveform using \nffmpeg\n. This requires \nffmpeg\n to be installed on the system.\n\n\nbytes\n it is supposed to be the content of an audio file and is interpreted by \nffmpeg\n in the\nsame way.\n\n\n(\nnp.ndarray\n of shape (n, ) of type \nnp.float32\n or \nnp.float64\n)\nRaw audio at the correct sampling rate (no further check will be done)\n\n\ndict\n form can be used to pass raw audio sampled at arbitrary \nsampling_rate\n and let this\npipeline do the resampling. The dict must be either be in the format \n{\"sampling_rate\": int, \"raw\": np.array}\n, or \n{\"sampling_rate\": int, \"array\": np.array}\n, where the key \n\"raw\"\n or\n\n\"array\"\n is used to denote the raw audio waveform.\n\n\n \n \n \ntop_k\n (\nint\n, \noptional\n, defaults to None) \u2014\nThe number of top labels that will be returned by the pipeline. If the provided number is \nNone\n or\nhigher than the number of labels available in the model configuration, it will default to the number of\nlabels.\n \n \n \nfunction_to_apply(\nstr\n,\n \noptional\n, defaults to \u201csoftmax\u201d) \u2014\nThe function to apply to the model output. By default, the pipeline will apply the softmax function to\nthe output of the model. Valid options: [\u201csoftmax\u201d, \u201csigmoid\u201d, \u201cnone\u201d]. Note that passing Python\u2019s\nbuilt-in \nNone\n will default to \u201csoftmax\u201d, so you need to pass the string \u201cnone\u201d to disable any\npost-processing.\n \n \n \nReturns\n \n\n\nA list of \ndict\n with the following keys\n\n\n \n \n\n\n\n\nlabel\n (\nstr\n) \u2014 The label predicted.\n\n\nscore\n (\nfloat\n) \u2014 The corresponding probability.\n\n\n\n\n \n \nClassify the sequence(s) given as inputs. See the \nAutomaticSpeechRecognitionPipeline\n documentation for more\ninformation.\n \n \nAutomaticSpeechRecognitionPipeline\n \n \nclass\n \ntransformers.\nAutomaticSpeechRecognitionPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \nmodel\n: PreTrainedModel\n \nfeature_extractor\n: typing.Union[ForwardRef('SequenceFeatureExtractor'), str] = None\n \ntokenizer\n: typing.Optional[transformers.tokenization_utils.PreTrainedTokenizer] = None\n \ndecoder\n: typing.Union[ForwardRef('BeamSearchDecoderCTC'), str, NoneType] = None\n \ndevice\n: typing.Union[int, ForwardRef('torch.device')] = None\n \ntorch_dtype\n: typing.Union[str, ForwardRef('torch.dtype'), NoneType] = None\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \nfeature_extractor\n (\nSequenceFeatureExtractor\n) \u2014\nThe feature extractor that will be used by the pipeline to encode waveform for the model.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \ndecoder\n (\npyctcdecode.BeamSearchDecoderCTC\n, \noptional\n) \u2014\n\nPyCTCDecode\u2019s\nBeamSearchDecoderCTC\n\ncan be passed for language model boosted decoding. See \nWav2Vec2ProcessorWithLM\n for more information.\n \n \n \nchunk_length_s\n (\nfloat\n, \noptional\n, defaults to 0) \u2014\nThe input length for in each chunk. If \nchunk_length_s = 0\n then chunking is disabled (default).\n\n\n\n\nFor more information on how to effectively use \nchunk_length_s\n, please have a look at the \nASR chunking\nblog post\n.\n\n\n \n \n \nstride_length_s\n (\nfloat\n, \noptional\n, defaults to \nchunk_length_s / 6\n) \u2014\nThe length of stride on the left and right of each chunk. Used only with \nchunk_length_s > 0\n. This enables\nthe model to \nsee\n more context and infer letters better than without this context but the pipeline\ndiscards the stride bits at the end to make the final reconstitution as perfect as possible.\n\n\n\n\nFor more information on how to effectively use \nstride_length_s\n, please have a look at the \nASR chunking\nblog post\n.\n\n\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled. If no framework is specified, will default to the one currently installed. If no framework is\nspecified and both frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if\nno model is provided.\n \n \n \ndevice\n (Union[\nint\n, \ntorch.device\n], \noptional\n) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to \nNone\n will leverage CPU, a positive will run the\nmodel on the associated CUDA device id.\n \n \n \ntorch_dtype\n (Union[\nint\n, \ntorch.dtype\n], \noptional\n) \u2014\nThe data-type (dtype) of the computation. Setting this to \nNone\n will use float32 precision. Set to\n\ntorch.float16\n or \ntorch.bfloat16\n to use half-precision in the respective dtypes.\n \n \n \n \nPipeline that aims at extracting spoken text contained within some audio.\n \nThe input can be either a raw waveform or a audio file. In case of the audio file, ffmpeg should be installed for\nto support multiple audio formats\n \nUnless the model you\u2019re using explicitly sets these generation parameters in its configuration files\n(\ngeneration_config.json\n), the following default values will be used:\n \nmax_new_tokens: 256\n \nnum_beams: 5\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \ntranscriber = pipeline(model=\n\"openai/whisper-base\"\n)\n\n>>> \ntranscriber(\n\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\"\n)\n{\n'text'\n: \n' He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick, peppered flour-fatten sauce.'\n}\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \ninputs\n: typing.Union[numpy.ndarray, bytes, str, dict]\n \n**kwargs\n: typing.Any\n \n \n)\n \n\u2192\n \nDict\n \n \nParameters \n \n \ninputs\n (\nnp.ndarray\n or \nbytes\n or \nstr\n or \ndict\n) \u2014\nThe inputs is either :\n\n\n\n\nstr\n that is either the filename of a local audio file, or a public URL address to download the\naudio file. The file will be read at the correct sampling rate to get the waveform using\n\nffmpeg\n. This requires \nffmpeg\n to be installed on the system.\n\n\nbytes\n it is supposed to be the content of an audio file and is interpreted by \nffmpeg\n in the\nsame way.\n\n\n(\nnp.ndarray\n of shape (n, ) of type \nnp.float32\n or \nnp.float64\n)\nRaw audio at the correct sampling rate (no further check will be done)\n\n\ndict\n form can be used to pass raw audio sampled at arbitrary \nsampling_rate\n and let this\npipeline do the resampling. The dict must be in the format \n{\"sampling_rate\": int, \"raw\": np.array}\n with optionally a \n\"stride\": (left: int, right: int)\n than can ask the pipeline to\ntreat the first \nleft\n samples and last \nright\n samples to be ignored in decoding (but used at\ninference to provide more context to the model). Only use \nstride\n with CTC models.\n\n\n \n \n \nreturn_timestamps\n (\noptional\n, \nstr\n or \nbool\n) \u2014\nOnly available for pure CTC models (Wav2Vec2, HuBERT, etc) and the Whisper model. Not available for\nother sequence-to-sequence models.\n\n\nFor CTC models, timestamps can take one of two formats:\n\n\n\n\n\"char\"\n: the pipeline will return timestamps along the text for every character in the text. For\ninstance, if you get \n[{\"text\": \"h\", \"timestamp\": (0.5, 0.6)}, {\"text\": \"i\", \"timestamp\": (0.7, 0.9)}]\n, then it means the model predicts that the letter \u201ch\u201d was spoken after \n0.5\n and before\n\n0.6\n seconds.\n\n\n\"word\"\n: the pipeline will return timestamps along the text for every word in the text. For\ninstance, if you get \n[{\"text\": \"hi \", \"timestamp\": (0.5, 0.9)}, {\"text\": \"there\", \"timestamp\": (1.0, 1.5)}]\n, then it means the model predicts that the word \u201chi\u201d was spoken after \n0.5\n and\nbefore \n0.9\n seconds.\n\n\n\n\nFor the Whisper model, timestamps can take one of two formats:\n\n\n\n\n\"word\"\n: same as above for word-level CTC timestamps. Word-level timestamps are predicted\nthrough the \ndynamic-time warping (DTW)\n algorithm, an approximation to word-level timestamps\nby inspecting the cross-attention weights.\n\n\nTrue\n: the pipeline will return timestamps along the text for \nsegments\n of words in the text.\nFor instance, if you get \n[{\"text\": \" Hi there!\", \"timestamp\": (0.5, 1.5)}]\n, then it means the\nmodel predicts that the segment \u201cHi there!\u201d was spoken after \n0.5\n and before \n1.5\n seconds.\nNote that a segment of text refers to a sequence of one or more words, rather than individual\nwords as with word-level timestamps.\n\n\n \n \n \ngenerate_kwargs\n (\ndict\n, \noptional\n) \u2014\nThe dictionary of ad-hoc parametrization of \ngenerate_config\n to be used for the generation call. For a\ncomplete overview of generate, check the \nfollowing\nguide\n.\n \n \n \nReturns\n \n\n\nDict\n\n\n \n \n\n\nA dictionary with the following keys:\n\n\n\n\ntext\n (\nstr\n): The recognized text.\n\n\nchunks\n (\noptional(, \nlist[Dict]\n)\nWhen using \nreturn_timestamps\n, the \nchunks\n will become a list containing all the various text\nchunks identified by the model, \ne.g.* \n[{\"text\": \"hi \", \"timestamp\": (0.5, 0.9)}, {\"text\": \"there\", \"timestamp\": (1.0, 1.5)}]\n. The original full text can roughly be recovered by doing\n\n\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])\n.\n\n\n\n\n \n \nTranscribe the audio sequence(s) given as inputs to text. See the \nAutomaticSpeechRecognitionPipeline\n\ndocumentation for more information.\n \n \nTextToAudioPipeline\n \n \nclass\n \ntransformers.\nTextToAudioPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \nvocoder\n = None\n \nsampling_rate\n = None\n \nno_processor\n = True\n \n**kwargs\n \n \n)\n \n \n \n \nText-to-audio generation pipeline using any \nAutoModelForTextToWaveform\n or \nAutoModelForTextToSpectrogram\n. This\npipeline generates an audio file from an input text and optional other conditional inputs.\n \nUnless the model you\u2019re using explicitly sets these generation parameters in its configuration files\n(\ngeneration_config.json\n), the following default values will be used:\n \nmax_new_tokens: 256\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \npipe = pipeline(model=\n\"suno/bark-small\"\n)\n\n>>> \noutput = pipe(\n\"Hey it's HuggingFace on the phone!\"\n)\n\n\n>>> \naudio = output[\n\"audio\"\n]\n\n>>> \nsampling_rate = output[\n\"sampling_rate\"\n]\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nYou can specify parameters passed to the model by using \nTextToAudioPipeline.__call__.forward_params\n or\n\nTextToAudioPipeline.__call__.generate_kwargs\n.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \nmusic_generator = pipeline(task=\n\"text-to-audio\"\n, model=\n\"facebook/musicgen-small\"\n, framework=\n\"pt\"\n)\n\n\n>>> \n# diversify the music generation by adding randomness with a high temperature and set a maximum music length\n\n\n>>> \ngenerate_kwargs = {\n\n... \n    \n\"do_sample\"\n: \nTrue\n,\n\n... \n    \n\"temperature\"\n: \n0.7\n,\n\n... \n    \n\"max_new_tokens\"\n: \n35\n,\n\n... \n}\n\n\n>>> \noutputs = music_generator(\n\"Techno music with high melodic riffs\"\n, generate_kwargs=generate_kwargs)\n \nThis pipeline can currently be loaded from \npipeline()\n using the following task identifiers: \n\"text-to-speech\"\n or\n\n\"text-to-audio\"\n.\n \nSee the list of available models on \nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \ntext_inputs\n: typing.Union[str, list[str]]\n \n**forward_params\n \n \n)\n \n\u2192\n \nA \ndict\n or a list of \ndict\n \n \nParameters \n \n \ntext_inputs\n (\nstr\n or \nlist[str]\n) \u2014\nThe text(s) to generate.\n \n \n \nforward_params\n (\ndict\n, \noptional\n) \u2014\nParameters passed to the model generation/forward method. \nforward_params\n are always passed to the\nunderlying model.\n \n \n \ngenerate_kwargs\n (\ndict\n, \noptional\n) \u2014\nThe dictionary of ad-hoc parametrization of \ngenerate_config\n to be used for the generation call. For a\ncomplete overview of generate, check the \nfollowing\nguide\n. \ngenerate_kwargs\n are\nonly passed to the underlying model if the latter is a generative model.\n \n \n \nReturns\n \n\n\nA \ndict\n or a list of \ndict\n\n\n \n \n\n\nThe dictionaries have two keys:\n\n\n\n\naudio\n (\nnp.ndarray\n of shape \n(nb_channels, audio_length)\n) \u2014 The generated audio waveform.\n\n\nsampling_rate\n (\nint\n) \u2014 The sampling rate of the generated audio waveform.\n\n\n\n\n \n \nGenerates speech/audio from the inputs. See the \nTextToAudioPipeline\n documentation for more information.\n \n \nZeroShotAudioClassificationPipeline\n \n \nclass\n \ntransformers.\nZeroShotAudioClassificationPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nfeature_extractor\n (\nSequenceFeatureExtractor\n) \u2014\nThe feature extractor that will be used by the pipeline to encode data for the model. This object inherits from\n\nSequenceFeatureExtractor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nZero shot audio classification pipeline using \nClapModel\n. This pipeline predicts the class of an audio when you\nprovide an audio and a set of \ncandidate_labels\n.\n \nThe default \nhypothesis_template\n is : \n\"This is a sound of {}.\"\n. Make sure you update it for your usage.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n>>> \nfrom\n datasets \nimport\n load_dataset\n\n\n>>> \ndataset = load_dataset(\n\"ashraq/esc50\"\n)\n\n>>> \naudio = \nnext\n(\niter\n(dataset[\n\"train\"\n][\n\"audio\"\n]))[\n\"array\"\n]\n\n>>> \nclassifier = pipeline(task=\n\"zero-shot-audio-classification\"\n, model=\n\"laion/clap-htsat-unfused\"\n)\n\n>>> \nclassifier(audio, candidate_labels=[\n\"Sound of a dog\"\n, \n\"Sound of vaccum cleaner\"\n])\n[{\n'score'\n: \n0.9996\n, \n'label'\n: \n'Sound of a dog'\n}, {\n'score'\n: \n0.0004\n, \n'label'\n: \n'Sound of vaccum cleaner'\n}]\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n This audio\nclassification pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"zero-shot-audio-classification\"\n. See the list of available models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \naudios\n: typing.Union[numpy.ndarray, bytes, str, dict]\n \n**kwargs\n: typing.Any\n \n \n)\n \n \n \nParameters \n \n \naudios\n (\nstr\n, \nlist[str]\n, \nnp.array\n or \nlist[np.array]\n) \u2014\nThe pipeline handles three types of inputs:\n\n\nA string containing a http link pointing to an audio\n\n\nA string containing a local path to an audio\n\n\nAn audio loaded in numpy\n\n\n \n \n \ncandidate_labels\n (\nlist[str]\n) \u2014\nThe candidate labels for this audio. They will be formatted using \nhypothesis_template\n.\n \n \n \nhypothesis_template\n (\nstr\n, \noptional\n, defaults to \n\"This is a sound of {}\"\n) \u2014\nThe format used in conjunction with \ncandidate_labels\n to attempt the audio classification by\nreplacing the placeholder with the candidate_labels. Pass \u201d{}\u201d if \ncandidate_labels\n are\nalready formatted.\n \n \n \n \nAssign labels to the audio(s) passed as inputs.\n \n \nComputer vision\n \nPipelines available for computer vision tasks include the following.\n \n \nDepthEstimationPipeline\n \n \nclass\n \ntransformers.\nDepthEstimationPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \nimage_processor\n (\nBaseImageProcessor\n) \u2014\nThe image processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nBaseImageProcessor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nDepth estimation pipeline using any \nAutoModelForDepthEstimation\n. This pipeline predicts the depth of an image.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \ndepth_estimator = pipeline(task=\n\"depth-estimation\"\n, model=\n\"LiheYoung/depth-anything-base-hf\"\n)\n\n>>> \noutput = depth_estimator(\n\"http://images.cocodataset.org/val2017/000000039769.jpg\"\n)\n\n>>> \n# This is a tensor with the values being the depth expressed in meters for each pixel\n\n\n>>> \noutput[\n\"predicted_depth\"\n].shape\ntorch.Size([\n1\n, \n384\n, \n384\n])\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis depth estimation pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"depth-estimation\"\n.\n \nSee the list of available models on \nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \ninputs\n: typing.Union[str, list[str], ForwardRef('Image.Image'), list['Image.Image']]\n \n**kwargs\n: typing.Any\n \n \n)\n \n \n \nParameters \n \n \ninputs\n (\nstr\n, \nlist[str]\n, \nPIL.Image\n or \nlist[PIL.Image]\n) \u2014\nThe pipeline handles three types of images:\n\n\n\n\nA string containing a http link pointing to an image\n\n\nA string containing a local path to an image\n\n\nAn image loaded in PIL directly\n\n\n\n\nThe pipeline accepts either a single image or a batch of images, which must then be passed as a string.\nImages in a batch must all be in the same format: all as http links, all as local paths, or all as PIL\nimages.\n \n \n \nparameters\n (\nDict\n, \noptional\n) \u2014\nA dictionary of argument names to parameter values, to control pipeline behaviour.\nThe only parameter available right now is \ntimeout\n, which is the length of time, in seconds,\nthat the pipeline should wait before giving up on trying to download an image.\n \n \n \ntimeout\n (\nfloat\n, \noptional\n, defaults to None) \u2014\nThe maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\nthe call may block forever.\n \n \n \n \nPredict the depth(s) of the image(s) passed as inputs.\n \n \nImageClassificationPipeline\n \n \nclass\n \ntransformers.\nImageClassificationPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \nimage_processor\n (\nBaseImageProcessor\n) \u2014\nThe image processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nBaseImageProcessor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \nfunction_to_apply\n (\nstr\n, \noptional\n, defaults to \n\"default\"\n) \u2014\nThe function to apply to the model outputs in order to retrieve the scores. Accepts four different values:\n\n\n\n\n\"default\"\n: if the model has a single label, will apply the sigmoid function on the output. If the model\nhas several labels, will apply the softmax function on the output.\n\n\n\"sigmoid\"\n: Applies the sigmoid function on the output.\n\n\n\"softmax\"\n: Applies the softmax function on the output.\n\n\n\"none\"\n: Does not apply any function on the output.\n\n\n \n \n \n \nImage classification pipeline using any \nAutoModelForImageClassification\n. This pipeline predicts the class of an\nimage.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \nclassifier = pipeline(model=\n\"microsoft/beit-base-patch16-224-pt22k-ft22k\"\n)\n\n>>> \nclassifier(\n\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n)\n[{\n'score'\n: \n0.442\n, \n'label'\n: \n'macaw'\n}, {\n'score'\n: \n0.088\n, \n'label'\n: \n'popinjay'\n}, {\n'score'\n: \n0.075\n, \n'label'\n: \n'parrot'\n}, {\n'score'\n: \n0.073\n, \n'label'\n: \n'parodist, lampooner'\n}, {\n'score'\n: \n0.046\n, \n'label'\n: \n'poll, poll_parrot'\n}]\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis image classification pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"image-classification\"\n.\n \nSee the list of available models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \ninputs\n: typing.Union[str, list[str], ForwardRef('Image.Image'), list['Image.Image']]\n \n**kwargs\n: typing.Any\n \n \n)\n \n \n \nParameters \n \n \ninputs\n (\nstr\n, \nlist[str]\n, \nPIL.Image\n or \nlist[PIL.Image]\n) \u2014\nThe pipeline handles three types of images:\n\n\n\n\nA string containing a http link pointing to an image\n\n\nA string containing a local path to an image\n\n\nAn image loaded in PIL directly\n\n\n\n\nThe pipeline accepts either a single image or a batch of images, which must then be passed as a string.\nImages in a batch must all be in the same format: all as http links, all as local paths, or all as PIL\nimages.\n \n \n \nfunction_to_apply\n (\nstr\n, \noptional\n, defaults to \n\"default\"\n) \u2014\nThe function to apply to the model outputs in order to retrieve the scores. Accepts four different\nvalues:\n\n\nIf this argument is not specified, then it will apply the following functions according to the number\nof labels:\n\n\n\n\nIf the model has a single label, will apply the sigmoid function on the output.\n\n\nIf the model has several labels, will apply the softmax function on the output.\n\n\n\n\nPossible values are:\n\n\n\n\n\"sigmoid\"\n: Applies the sigmoid function on the output.\n\n\n\"softmax\"\n: Applies the softmax function on the output.\n\n\n\"none\"\n: Does not apply any function on the output.\n\n\n \n \n \ntop_k\n (\nint\n, \noptional\n, defaults to 5) \u2014\nThe number of top labels that will be returned by the pipeline. If the provided number is higher than\nthe number of labels available in the model configuration, it will default to the number of labels.\n \n \n \ntimeout\n (\nfloat\n, \noptional\n, defaults to None) \u2014\nThe maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\nthe call may block forever.\n \n \n \n \nAssign labels to the image(s) passed as inputs.\n \n \nImageSegmentationPipeline\n \n \nclass\n \ntransformers.\nImageSegmentationPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \nimage_processor\n (\nBaseImageProcessor\n) \u2014\nThe image processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nBaseImageProcessor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nImage segmentation pipeline using any \nAutoModelForXXXSegmentation\n. This pipeline predicts masks of objects and\ntheir classes.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \nsegmenter = pipeline(model=\n\"facebook/detr-resnet-50-panoptic\"\n)\n\n>>> \nsegments = segmenter(\n\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n)\n\n>>> \nlen\n(segments)\n\n2\n\n\n\n>>> \nsegments[\n0\n][\n\"label\"\n]\n\n'bird'\n\n\n\n>>> \nsegments[\n1\n][\n\"label\"\n]\n\n'bird'\n\n\n\n>>> \ntype\n(segments[\n0\n][\n\"mask\"\n])  \n# This is a black and white mask showing where is the bird on the original image.\n\n<\nclass\n \n'PIL.Image.Image'\n>\n\n\n>>> \nsegments[\n0\n][\n\"mask\"\n].size\n(\n768\n, \n512\n)\n \nThis image segmentation pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"image-segmentation\"\n.\n \nSee the list of available models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \ninputs\n: typing.Union[str, ForwardRef('Image.Image'), list[str], list['Image.Image']]\n \n**kwargs\n: typing.Any\n \n \n)\n \n \n \nParameters \n \n \ninputs\n (\nstr\n, \nlist[str]\n, \nPIL.Image\n or \nlist[PIL.Image]\n) \u2014\nThe pipeline handles three types of images:\n\n\n\n\nA string containing an HTTP(S) link pointing to an image\n\n\nA string containing a local path to an image\n\n\nAn image loaded in PIL directly\n\n\n\n\nThe pipeline accepts either a single image or a batch of images. Images in a batch must all be in the\nsame format: all as HTTP(S) links, all as local paths, or all as PIL images.\n \n \n \nsubtask\n (\nstr\n, \noptional\n) \u2014\nSegmentation task to be performed, choose [\nsemantic\n, \ninstance\n and \npanoptic\n] depending on model\ncapabilities. If not set, the pipeline will attempt tp resolve in the following order:\n\npanoptic\n, \ninstance\n, \nsemantic\n.\n \n \n \nthreshold\n (\nfloat\n, \noptional\n, defaults to 0.9) \u2014\nProbability threshold to filter out predicted masks.\n \n \n \nmask_threshold\n (\nfloat\n, \noptional\n, defaults to 0.5) \u2014\nThreshold to use when turning the predicted masks into binary values.\n \n \n \noverlap_mask_area_threshold\n (\nfloat\n, \noptional\n, defaults to 0.5) \u2014\nMask overlap threshold to eliminate small, disconnected segments.\n \n \n \ntimeout\n (\nfloat\n, \noptional\n, defaults to None) \u2014\nThe maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\nthe call may block forever.\n \n \n \n \nPerform segmentation (detect masks & classes) in the image(s) passed as inputs.\n \n \nImageToImagePipeline\n \n \nclass\n \ntransformers.\nImageToImagePipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \nimage_processor\n (\nBaseImageProcessor\n) \u2014\nThe image processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nBaseImageProcessor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nImage to Image pipeline using any \nAutoModelForImageToImage\n. This pipeline generates an image based on a previous\nimage input.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n PIL \nimport\n Image\n\n>>> \nimport\n requests\n\n\n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \nupscaler = pipeline(\n\"image-to-image\"\n, model=\n\"caidas/swin2SR-classical-sr-x2-64\"\n)\n\n>>> \nimg = Image.\nopen\n(requests.get(\n\"http://images.cocodataset.org/val2017/000000039769.jpg\"\n, stream=\nTrue\n).raw)\n\n>>> \nimg = img.resize((\n64\n, \n64\n))\n\n>>> \nupscaled_img = upscaler(img)\n\n>>> \nimg.size\n(\n64\n, \n64\n)\n\n\n>>> \nupscaled_img.size\n(\n144\n, \n144\n)\n \nThis image to image pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"image-to-image\"\n.\n \nSee the list of available models on \nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \nimages\n: typing.Union[str, list[str], ForwardRef('Image.Image'), list['Image.Image']]\n \n**kwargs\n: typing.Any\n \n \n)\n \n \n \nParameters \n \n \nimages\n (\nstr\n, \nlist[str]\n, \nPIL.Image\n or \nlist[PIL.Image]\n) \u2014\nThe pipeline handles three types of images:\n\n\n\n\nA string containing a http link pointing to an image\n\n\nA string containing a local path to an image\n\n\nAn image loaded in PIL directly\n\n\n\n\nThe pipeline accepts either a single image or a batch of images, which must then be passed as a string.\nImages in a batch must all be in the same format: all as http links, all as local paths, or all as PIL\nimages.\n \n \n \ntimeout\n (\nfloat\n, \noptional\n, defaults to None) \u2014\nThe maximum time in seconds to wait for fetching images from the web. If None, no timeout is used and\nthe call may block forever.\n \n \n \n \nTransform the image(s) passed as inputs.\n \n \nObjectDetectionPipeline\n \n \nclass\n \ntransformers.\nObjectDetectionPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \nimage_processor\n (\nBaseImageProcessor\n) \u2014\nThe image processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nBaseImageProcessor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nObject detection pipeline using any \nAutoModelForObjectDetection\n. This pipeline predicts bounding boxes of objects\nand their classes.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \ndetector = pipeline(model=\n\"facebook/detr-resnet-50\"\n)\n\n>>> \ndetector(\n\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n)\n[{\n'score'\n: \n0.997\n, \n'label'\n: \n'bird'\n, \n'box'\n: {\n'xmin'\n: \n69\n, \n'ymin'\n: \n171\n, \n'xmax'\n: \n396\n, \n'ymax'\n: \n507\n}}, {\n'score'\n: \n0.999\n, \n'label'\n: \n'bird'\n, \n'box'\n: {\n'xmin'\n: \n398\n, \n'ymin'\n: \n105\n, \n'xmax'\n: \n767\n, \n'ymax'\n: \n507\n}}]\n\n\n>>> \n# x, y  are expressed relative to the top left hand corner.\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis object detection pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"object-detection\"\n.\n \nSee the list of available models on \nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \ninputs\n (\nstr\n, \nlist[str]\n, \nPIL.Image\n or \nlist[PIL.Image]\n) \u2014\nThe pipeline handles three types of images:\n\n\n\n\nA string containing an HTTP(S) link pointing to an image\n\n\nA string containing a local path to an image\n\n\nAn image loaded in PIL directly\n\n\n\n\nThe pipeline accepts either a single image or a batch of images. Images in a batch must all be in the\nsame format: all as HTTP(S) links, all as local paths, or all as PIL images.\n \n \n \nthreshold\n (\nfloat\n, \noptional\n, defaults to 0.5) \u2014\nThe probability necessary to make a prediction.\n \n \n \ntimeout\n (\nfloat\n, \noptional\n, defaults to None) \u2014\nThe maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\nthe call may block forever.\n \n \n \n \nDetect objects (bounding boxes & classes) in the image(s) passed as inputs.\n \n \nVideoClassificationPipeline\n \n \nclass\n \ntransformers.\nVideoClassificationPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \nimage_processor\n (\nBaseImageProcessor\n) \u2014\nThe image processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nBaseImageProcessor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nVideo classification pipeline using any \nAutoModelForVideoClassification\n. This pipeline predicts the class of a\nvideo.\n \nThis video classification pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"video-classification\"\n.\n \nSee the list of available models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \ninputs\n: typing.Union[str, list[str], NoneType] = None\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \ninputs\n (\nstr\n, \nlist[str]\n) \u2014\nThe pipeline handles three types of videos:\n\n\n\n\nA string containing a http link pointing to a video\n\n\nA string containing a local path to a video\n\n\n\n\nThe pipeline accepts either a single video or a batch of videos, which must then be passed as a string.\nVideos in a batch must all be in the same format: all as http links or all as local paths.\n \n \n \ntop_k\n (\nint\n, \noptional\n, defaults to 5) \u2014\nThe number of top labels that will be returned by the pipeline. If the provided number is higher than\nthe number of labels available in the model configuration, it will default to the number of labels.\n \n \n \nnum_frames\n (\nint\n, \noptional\n, defaults to \nself.model.config.num_frames\n) \u2014\nThe number of frames sampled from the video to run the classification on. If not provided, will default\nto the number of frames specified in the model configuration.\n \n \n \nframe_sampling_rate\n (\nint\n, \noptional\n, defaults to 1) \u2014\nThe sampling rate used to select frames from the video. If not provided, will default to 1, i.e. every\nframe will be used.\n \n \n \nfunction_to_apply(\nstr\n,\n \noptional\n, defaults to \u201csoftmax\u201d) \u2014\nThe function to apply to the model output. By default, the pipeline will apply the softmax function to\nthe output of the model. Valid options: [\u201csoftmax\u201d, \u201csigmoid\u201d, \u201cnone\u201d]. Note that passing Python\u2019s\nbuilt-in \nNone\n will default to \u201csoftmax\u201d, so you need to pass the string \u201cnone\u201d to disable any\npost-processing.\n \n \n \n \nAssign labels to the video(s) passed as inputs.\n \n \nZeroShotImageClassificationPipeline\n \n \nclass\n \ntransformers.\nZeroShotImageClassificationPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \nimage_processor\n (\nBaseImageProcessor\n) \u2014\nThe image processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nBaseImageProcessor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nZero shot image classification pipeline using \nCLIPModel\n. This pipeline predicts the class of an image when you\nprovide an image and a set of \ncandidate_labels\n.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \nclassifier = pipeline(model=\n\"google/siglip-so400m-patch14-384\"\n)\n\n>>> \nclassifier(\n\n... \n    \n\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n,\n\n... \n    candidate_labels=[\n\"animals\"\n, \n\"humans\"\n, \n\"landscape\"\n],\n\n... \n)\n[{\n'score'\n: \n0.965\n, \n'label'\n: \n'animals'\n}, {\n'score'\n: \n0.03\n, \n'label'\n: \n'humans'\n}, {\n'score'\n: \n0.005\n, \n'label'\n: \n'landscape'\n}]\n\n\n>>> \nclassifier(\n\n... \n    \n\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n,\n\n... \n    candidate_labels=[\n\"black and white\"\n, \n\"photorealist\"\n, \n\"painting\"\n],\n\n... \n)\n[{\n'score'\n: \n0.996\n, \n'label'\n: \n'black and white'\n}, {\n'score'\n: \n0.003\n, \n'label'\n: \n'photorealist'\n}, {\n'score'\n: \n0.0\n, \n'label'\n: \n'painting'\n}]\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis image classification pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"zero-shot-image-classification\"\n.\n \nSee the list of available models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \nimage\n: typing.Union[str, list[str], ForwardRef('Image.Image'), list['Image.Image']]\n \ncandidate_labels\n: list\n \n**kwargs\n: typing.Any\n \n \n)\n \n \n \nParameters \n \n \nimage\n (\nstr\n, \nlist[str]\n, \nPIL.Image\n or \nlist[PIL.Image]\n) \u2014\nThe pipeline handles three types of images:\n\n\n\n\nA string containing a http link pointing to an image\n\n\nA string containing a local path to an image\n\n\nAn image loaded in PIL directly\n\n\n \n \n \ncandidate_labels\n (\nlist[str]\n) \u2014\nThe candidate labels for this image. They will be formatted using \nhypothesis_template\n.\n \n \n \nhypothesis_template\n (\nstr\n, \noptional\n, defaults to \n\"This is a photo of {}\"\n) \u2014\nThe format used in conjunction with \ncandidate_labels\n to attempt the image classification by\nreplacing the placeholder with the candidate_labels. Pass \u201d{}\u201d if \ncandidate_labels\n are\nalready formatted.\n \n \n \ntimeout\n (\nfloat\n, \noptional\n, defaults to None) \u2014\nThe maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\nthe call may block forever.\n \n \n \n \nAssign labels to the image(s) passed as inputs.\n \n \nZeroShotObjectDetectionPipeline\n \n \nclass\n \ntransformers.\nZeroShotObjectDetectionPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \nimage_processor\n (\nBaseImageProcessor\n) \u2014\nThe image processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nBaseImageProcessor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nZero shot object detection pipeline using \nOwlViTForObjectDetection\n. This pipeline predicts bounding boxes of\nobjects when you provide an image and a set of \ncandidate_labels\n.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \ndetector = pipeline(model=\n\"google/owlvit-base-patch32\"\n, task=\n\"zero-shot-object-detection\"\n)\n\n>>> \ndetector(\n\n... \n    \n\"http://images.cocodataset.org/val2017/000000039769.jpg\"\n,\n\n... \n    candidate_labels=[\n\"cat\"\n, \n\"couch\"\n],\n\n... \n)\n[{\n'score'\n: \n0.287\n, \n'label'\n: \n'cat'\n, \n'box'\n: {\n'xmin'\n: \n324\n, \n'ymin'\n: \n20\n, \n'xmax'\n: \n640\n, \n'ymax'\n: \n373\n}}, {\n'score'\n: \n0.254\n, \n'label'\n: \n'cat'\n, \n'box'\n: {\n'xmin'\n: \n1\n, \n'ymin'\n: \n55\n, \n'xmax'\n: \n315\n, \n'ymax'\n: \n472\n}}, {\n'score'\n: \n0.121\n, \n'label'\n: \n'couch'\n, \n'box'\n: {\n'xmin'\n: \n4\n, \n'ymin'\n: \n0\n, \n'xmax'\n: \n642\n, \n'ymax'\n: \n476\n}}]\n\n\n>>> \ndetector(\n\n... \n    \n\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n,\n\n... \n    candidate_labels=[\n\"head\"\n, \n\"bird\"\n],\n\n... \n)\n[{\n'score'\n: \n0.119\n, \n'label'\n: \n'bird'\n, \n'box'\n: {\n'xmin'\n: \n71\n, \n'ymin'\n: \n170\n, \n'xmax'\n: \n410\n, \n'ymax'\n: \n508\n}}]\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis object detection pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"zero-shot-object-detection\"\n.\n \nSee the list of available models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \nimage\n: typing.Union[str, ForwardRef('Image.Image'), list[dict[str, typing.Any]]]\n \ncandidate_labels\n: typing.Union[str, list[str], NoneType] = None\n \n**kwargs\n: typing.Any\n \n \n)\n \n \n \nParameters \n \n \nimage\n (\nstr\n, \nPIL.Image\n or \nlist[dict[str, Any]]\n) \u2014\nThe pipeline handles three types of images:\n\n\n\n\nA string containing an http url pointing to an image\n\n\nA string containing a local path to an image\n\n\nAn image loaded in PIL directly\n\n\n\n\nYou can use this parameter to send directly a list of images, or a dataset or a generator like so:\n \n \n \n \nDetect objects (bounding boxes & classes) in the image(s) passed as inputs.\n \n \nNatural Language Processing\n \nPipelines available for natural language processing tasks include the following.\n \n \nFillMaskPipeline\n \n \nclass\n \ntransformers.\nFillMaskPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \nmodel\n: typing.Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]\n \ntokenizer\n: typing.Optional[transformers.tokenization_utils.PreTrainedTokenizer] = None\n \nfeature_extractor\n: typing.Optional[ForwardRef('SequenceFeatureExtractor')] = None\n \nimage_processor\n: typing.Optional[transformers.image_processing_utils.BaseImageProcessor] = None\n \nprocessor\n: typing.Optional[transformers.processing_utils.ProcessorMixin] = None\n \nmodelcard\n: typing.Optional[transformers.modelcard.ModelCard] = None\n \nframework\n: typing.Optional[str] = None\n \ntask\n: str = ''\n \nargs_parser\n: ArgumentHandler = None\n \ndevice\n: typing.Union[int, ForwardRef('torch.device')] = None\n \ntorch_dtype\n: typing.Union[str, ForwardRef('torch.dtype'), NoneType] = None\n \nbinary_output\n: bool = False\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \ntop_k\n (\nint\n, \noptional\n, defaults to 5) \u2014\nThe number of predictions to return.\n \n \n \ntargets\n (\nstr\n or \nlist[str]\n, \noptional\n) \u2014\nWhen passed, the model will limit the scores to the passed targets instead of looking up in the whole\nvocab. If the provided targets are not in the model vocab, they will be tokenized and the first resulting\ntoken will be used (with a warning, and that might be slower).\n \n \n \ntokenizer_kwargs\n (\ndict\n, \noptional\n) \u2014\nAdditional dictionary of keyword arguments passed along to the tokenizer.\n \n \n \n \nMasked language modeling prediction pipeline using any \nModelWithLMHead\n. See the \nmasked language modeling\nexamples\n for more information.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \nfill_masker = pipeline(model=\n\"google-bert/bert-base-uncased\"\n)\n\n>>> \nfill_masker(\n\"This is a simple [MASK].\"\n)\n[{\n'score'\n: \n0.042\n, \n'token'\n: \n3291\n, \n'token_str'\n: \n'problem'\n, \n'sequence'\n: \n'this is a simple problem.'\n}, {\n'score'\n: \n0.031\n, \n'token'\n: \n3160\n, \n'token_str'\n: \n'question'\n, \n'sequence'\n: \n'this is a simple question.'\n}, {\n'score'\n: \n0.03\n, \n'token'\n: \n8522\n, \n'token_str'\n: \n'equation'\n, \n'sequence'\n: \n'this is a simple equation.'\n}, {\n'score'\n: \n0.027\n, \n'token'\n: \n2028\n, \n'token_str'\n: \n'one'\n, \n'sequence'\n: \n'this is a simple one.'\n}, {\n'score'\n: \n0.024\n, \n'token'\n: \n3627\n, \n'token_str'\n: \n'rule'\n, \n'sequence'\n: \n'this is a simple rule.'\n}]\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis mask filling pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"fill-mask\"\n.\n \nThe models that this pipeline can use are models that have been trained with a masked language modeling objective,\nwhich includes the bi-directional models in the library. See the up-to-date list of available models on\n\nhuggingface.co/models\n.\n \nThis pipeline only works for inputs with exactly one token masked. Experimental: We added support for multiple\nmasks. The returned values are raw model output, and correspond to disjoint probabilities where one might expect\njoint probabilities (See \ndiscussion\n).\n \n \n \nThis pipeline now supports tokenizer_kwargs. For example try:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \nfill_masker = pipeline(model=\n\"google-bert/bert-base-uncased\"\n)\n\n>>> \ntokenizer_kwargs = {\n\"truncation\"\n: \nTrue\n}\n\n>>> \nfill_masker(\n\n... \n    \n\"This is a simple [MASK]. \"\n + \n\"...with a large amount of repeated text appended. \"\n * \n100\n,\n\n... \n    tokenizer_kwargs=tokenizer_kwargs,\n\n... \n)\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \ninputs\n: typing.Union[str, list[str]]\n \n**kwargs\n: typing.Any\n \n \n)\n \n\u2192\n \nA list or a list of list of \ndict\n \n \nParameters \n \n \ninputs\n (\nstr\n or \nlist[str]\n) \u2014\nOne or several texts (or one list of prompts) with masked tokens.\n \n \n \ntargets\n (\nstr\n or \nlist[str]\n, \noptional\n) \u2014\nWhen passed, the model will limit the scores to the passed targets instead of looking up in the whole\nvocab. If the provided targets are not in the model vocab, they will be tokenized and the first\nresulting token will be used (with a warning, and that might be slower).\n \n \n \ntop_k\n (\nint\n, \noptional\n) \u2014\nWhen passed, overrides the number of predictions to return.\n \n \n \nReturns\n \n\n\nA list or a list of list of \ndict\n\n\n \n \n\n\nEach result comes as list of dictionaries with the following keys:\n\n\n\n\nsequence\n (\nstr\n) \u2014 The corresponding input with the mask token prediction.\n\n\nscore\n (\nfloat\n) \u2014 The corresponding probability.\n\n\ntoken\n (\nint\n) \u2014 The predicted token id (to replace the masked one).\n\n\ntoken_str\n (\nstr\n) \u2014 The predicted token (to replace the masked one).\n\n\n\n\n \n \nFill the masked token in the text(s) given as inputs.\n \n \nQuestionAnsweringPipeline\n \n \nclass\n \ntransformers.\nQuestionAnsweringPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \nmodel\n: typing.Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]\n \ntokenizer\n: PreTrainedTokenizer\n \nmodelcard\n: typing.Optional[transformers.modelcard.ModelCard] = None\n \nframework\n: typing.Optional[str] = None\n \ntask\n: str = ''\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nQuestion Answering pipeline using any \nModelForQuestionAnswering\n. See the \nquestion answering\nexamples\n for more information.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \noracle = pipeline(model=\n\"deepset/roberta-base-squad2\"\n)\n\n>>> \noracle(question=\n\"Where do I live?\"\n, context=\n\"My name is Wolfgang and I live in Berlin\"\n)\n{\n'score'\n: \n0.9191\n, \n'start'\n: \n34\n, \n'end'\n: \n40\n, \n'answer'\n: \n'Berlin'\n}\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis question answering pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"question-answering\"\n.\n \nThe models that this pipeline can use are models that have been fine-tuned on a question answering task. See the\nup-to-date list of available models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n\u2192\n \nA \ndict\n or a list of \ndict\n \n \nParameters \n \n \nquestion\n (\nstr\n or \nlist[str]\n) \u2014\nOne or several question(s) (must be used in conjunction with the \ncontext\n argument).\n \n \n \ncontext\n (\nstr\n or \nlist[str]\n) \u2014\nOne or several context(s) associated with the question(s) (must be used in conjunction with the\n\nquestion\n argument).\n \n \n \ntop_k\n (\nint\n, \noptional\n, defaults to 1) \u2014\nThe number of answers to return (will be chosen by order of likelihood). Note that we return less than\ntop_k answers if there are not enough options available within the context.\n \n \n \ndoc_stride\n (\nint\n, \noptional\n, defaults to 128) \u2014\nIf the context is too long to fit with the question for the model, it will be split in several chunks\nwith some overlap. This argument controls the size of that overlap.\n \n \n \nmax_answer_len\n (\nint\n, \noptional\n, defaults to 15) \u2014\nThe maximum length of predicted answers (e.g., only answers with a shorter length are considered).\n \n \n \nmax_seq_len\n (\nint\n, \noptional\n, defaults to 384) \u2014\nThe maximum length of the total sentence (context + question) in tokens of each chunk passed to the\nmodel. The context will be split in several chunks (using \ndoc_stride\n as overlap) if needed.\n \n \n \nmax_question_len\n (\nint\n, \noptional\n, defaults to 64) \u2014\nThe maximum length of the question after tokenization. It will be truncated if needed.\n \n \n \nhandle_impossible_answer\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not we accept impossible as an answer.\n \n \n \nalign_to_words\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nAttempts to align the answer to real words. Improves quality on space separated languages. Might hurt on\nnon-space-separated languages (like Japanese or Chinese)\n \n \n \nReturns\n \n\n\nA \ndict\n or a list of \ndict\n\n\n \n \n\n\nEach result comes as a dictionary with the following keys:\n\n\n\n\nscore\n (\nfloat\n) \u2014 The probability associated to the answer.\n\n\nstart\n (\nint\n) \u2014 The character start index of the answer (in the tokenized version of the input).\n\n\nend\n (\nint\n) \u2014 The character end index of the answer (in the tokenized version of the input).\n\n\nanswer\n (\nstr\n) \u2014 The answer to the question.\n\n\n\n\n \n \nAnswer the question(s) given as inputs by using the context(s).\n \n \ncreate_sample\n \n \n<\n \nsource\n \n>\n \n(\n \nquestion\n: typing.Union[str, list[str]]\n \ncontext\n: typing.Union[str, list[str]]\n \n \n)\n \n\u2192\n \nOne or a list of \nSquadExample\n \n \nParameters \n \n \nquestion\n (\nstr\n or \nlist[str]\n) \u2014 The question(s) asked.\n \n \n \ncontext\n (\nstr\n or \nlist[str]\n) \u2014 The context(s) in which we will look for the answer.\n \n \n \nReturns\n \n\n\nOne or a list of \nSquadExample\n\n\n \n \n\n\nThe corresponding \nSquadExample\n grouping question and context.\n\n\n \n \nQuestionAnsweringPipeline leverages the \nSquadExample\n internally. This helper method encapsulate all the\nlogic for converting question(s) and context(s) to \nSquadExample\n.\n \nWe currently support extractive question answering.\n \n \nspan_to_answer\n \n \n<\n \nsource\n \n>\n \n(\n \ntext\n: str\n \nstart\n: int\n \nend\n: int\n \n \n)\n \n\u2192\n \nDictionary like `{\u2018answer\u2019\n \n \nParameters \n \n \ntext\n (\nstr\n) \u2014 The actual context to extract the answer from.\n \n \n \nstart\n (\nint\n) \u2014 The answer starting token index.\n \n \n \nend\n (\nint\n) \u2014 The answer end token index.\n \n \n \nReturns\n \n\n\nDictionary like `{\u2018answer\u2019\n\n\n \n \n\n\nstr, \u2018start\u2019: int, \u2018end\u2019: int}`\n\n\n \n \nWhen decoding from token probabilities, this method maps token indexes to actual word in the initial context.\n \n \nSummarizationPipeline\n \n \nclass\n \ntransformers.\nSummarizationPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nSummarize news articles and other documents.\n \nThis summarizing pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"summarization\"\n.\n \nThe models that this pipeline can use are models that have been fine-tuned on a summarization task, which is\ncurrently, \u2019\nbart-large-cnn\n\u2019, \u2019\ngoogle-t5/t5-small\n\u2019, \u2019\ngoogle-t5/t5-base\n\u2019, \u2019\ngoogle-t5/t5-large\n\u2019, \u2019\ngoogle-t5/t5-3b\n\u2019, \u2019\ngoogle-t5/t5-11b\n\u2019. See the up-to-date\nlist of available models on \nhuggingface.co/models\n. For a list\nof available parameters, see the \nfollowing\ndocumentation\n \nUnless the model you\u2019re using explicitly sets these generation parameters in its configuration files\n(\ngeneration_config.json\n), the following default values will be used:\n \nmax_new_tokens: 256\n \nnum_beams: 4\n \n \nUsage:\n \n \n Copied\n \n# use bart in pytorch\n\nsummarizer = pipeline(\n\"summarization\"\n)\nsummarizer(\n\"An apple a day, keeps the doctor away\"\n, min_length=\n5\n, max_length=\n20\n)\n\n\n# use t5 in tf\n\nsummarizer = pipeline(\n\"summarization\"\n, model=\n\"google-t5/t5-base\"\n, tokenizer=\n\"google-t5/t5-base\"\n, framework=\n\"tf\"\n)\nsummarizer(\n\"An apple a day, keeps the doctor away\"\n, min_length=\n5\n, max_length=\n20\n)\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n\u2192\n \nA list or a list of list of \ndict\n \n \nParameters \n \n \ndocuments\n (\nstr\n or \nlist[str]\n) \u2014\nOne or several articles (or one list of articles) to summarize.\n \n \n \nreturn_text\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether or not to include the decoded texts in the outputs\n \n \n \nreturn_tensors\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to include the tensors of predictions (as token indices) in the outputs.\n \n \n \nclean_up_tokenization_spaces\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to clean up the potential extra spaces in the text output.\n \n \n \ngenerate_kwargs\n \u2014\nAdditional keyword arguments to pass along to the generate method of the model (see the generate method\ncorresponding to your framework \nhere\n).\n \n \n \nReturns\n \n\n\nA list or a list of list of \ndict\n\n\n \n \n\n\nEach result comes as a dictionary with the following keys:\n\n\n\n\nsummary_text\n (\nstr\n, present when \nreturn_text=True\n) \u2014 The summary of the corresponding input.\n\n\nsummary_token_ids\n (\ntorch.Tensor\n or \ntf.Tensor\n, present when \nreturn_tensors=True\n) \u2014 The token\nids of the summary.\n\n\n\n\n \n \nSummarize the text(s) given as inputs.\n \n \nTableQuestionAnsweringPipeline\n \n \nclass\n \ntransformers.\nTableQuestionAnsweringPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \nargs_parser\n = <transformers.pipelines.table_question_answering.TableQuestionAnsweringArgumentHandler object at 0x7f813a290a00>\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nTable Question Answering pipeline using a \nModelForTableQuestionAnswering\n. This pipeline is only available in\nPyTorch.\n \nUnless the model you\u2019re using explicitly sets these generation parameters in its configuration files\n(\ngeneration_config.json\n), the following default values will be used:\n \nmax_new_tokens: 256\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \noracle = pipeline(model=\n\"google/tapas-base-finetuned-wtq\"\n)\n\n>>> \ntable = {\n\n... \n    \n\"Repository\"\n: [\n\"Transformers\"\n, \n\"Datasets\"\n, \n\"Tokenizers\"\n],\n\n... \n    \n\"Stars\"\n: [\n\"36542\"\n, \n\"4512\"\n, \n\"3934\"\n],\n\n... \n    \n\"Contributors\"\n: [\n\"651\"\n, \n\"77\"\n, \n\"34\"\n],\n\n... \n    \n\"Programming language\"\n: [\n\"Python\"\n, \n\"Python\"\n, \n\"Rust, Python and NodeJS\"\n],\n\n... \n}\n\n>>> \noracle(query=\n\"How many stars does the transformers repository have?\"\n, table=table)\n{\n'answer'\n: \n'AVERAGE > 36542'\n, \n'coordinates'\n: [(\n0\n, \n1\n)], \n'cells'\n: [\n'36542'\n], \n'aggregator'\n: \n'AVERAGE'\n}\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis tabular question answering pipeline can currently be loaded from \npipeline()\n using the following task\nidentifier: \n\"table-question-answering\"\n.\n \nThe models that this pipeline can use are models that have been fine-tuned on a tabular question answering task.\nSee the up-to-date list of available models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n\u2192\n \nA dictionary or a list of dictionaries containing results\n \n \nParameters \n \n \ntable\n (\npd.DataFrame\n or \nDict\n) \u2014\nPandas DataFrame or dictionary that will be converted to a DataFrame containing all the table values.\nSee above for an example of dictionary.\n \n \n \nquery\n (\nstr\n or \nlist[str]\n) \u2014\nQuery or list of queries that will be sent to the model alongside the table.\n \n \n \nsequential\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to do inference sequentially or as a batch. Batching is faster, but models like SQA require the\ninference to be done sequentially to extract relations within sequences, given their conversational\nnature.\n \n \n \npadding\n (\nbool\n, \nstr\n or \nPaddingStrategy\n, \noptional\n, defaults to \nFalse\n) \u2014\nActivates and controls padding. Accepts the following values:\n\n\n\n\nTrue\n or \n'longest'\n: Pad to the longest sequence in the batch (or no padding if only a single\nsequence if provided).\n\n\n'max_length'\n: Pad to a maximum length specified with the argument \nmax_length\n or to the maximum\nacceptable input length for the model if that argument is not provided.\n\n\nFalse\n or \n'do_not_pad'\n (default): No padding (i.e., can output a batch with sequences of different\nlengths).\n\n\n \n \n \ntruncation\n (\nbool\n, \nstr\n or \nTapasTruncationStrategy\n, \noptional\n, defaults to \nFalse\n) \u2014\nActivates and controls truncation. Accepts the following values:\n\n\n\n\nTrue\n or \n'drop_rows_to_fit'\n: Truncate to a maximum length specified with the argument \nmax_length\n\nor to the maximum acceptable input length for the model if that argument is not provided. This will\ntruncate row by row, removing rows from the table.\n\n\nFalse\n or \n'do_not_truncate'\n (default): No truncation (i.e., can output batch with sequence lengths\ngreater than the model maximum admissible input size).\n\n\n \n \n \nReturns\n \n\n\nA dictionary or a list of dictionaries containing results\n\n\n \n \n\n\nEach result is a dictionary with the following\nkeys:\n\n\n\n\nanswer\n (\nstr\n) \u2014 The answer of the query given the table. If there is an aggregator, the answer will\nbe preceded by \nAGGREGATOR >\n.\n\n\ncoordinates\n (\nlist[tuple[int, int]]\n) \u2014 Coordinates of the cells of the answers.\n\n\ncells\n (\nlist[str]\n) \u2014 List of strings made up of the answer cell values.\n\n\naggregator\n (\nstr\n) \u2014 If the model has an aggregator, this returns the aggregator.\n\n\n\n\n \n \nAnswers queries according to a table. The pipeline accepts several types of inputs which are detailed below:\n \npipeline(table, query)\n \npipeline(table, [query])\n \npipeline(table=table, query=query)\n \npipeline(table=table, query=[query])\n \npipeline({\"table\": table, \"query\": query})\n \npipeline({\"table\": table, \"query\": [query]})\n \npipeline([{\"table\": table, \"query\": query}, {\"table\": table, \"query\": query}])\n \nThe \ntable\n argument should be a dict or a DataFrame built from that dict, containing the whole table:\n \n \nExample:\n \n \n Copied\n \ndata = {\n    \n\"actors\"\n: [\n\"brad pitt\"\n, \n\"leonardo di caprio\"\n, \n\"george clooney\"\n],\n    \n\"age\"\n: [\n\"56\"\n, \n\"45\"\n, \n\"59\"\n],\n    \n\"number of movies\"\n: [\n\"87\"\n, \n\"53\"\n, \n\"69\"\n],\n    \n\"date of birth\"\n: [\n\"7 february 1967\"\n, \n\"10 june 1996\"\n, \n\"28 november 1967\"\n],\n}\n \nThis dictionary can be passed in as such, or can be converted to a pandas DataFrame:\n \n \nExample:\n \n \n Copied\n \nimport\n pandas \nas\n pd\n\ntable = pd.DataFrame.from_dict(data)\n \n \nTextClassificationPipeline\n \n \nclass\n \ntransformers.\nTextClassificationPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \nreturn_all_scores\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to return all prediction scores or just the one of the predicted class.\n \n \n \nfunction_to_apply\n (\nstr\n, \noptional\n, defaults to \n\"default\"\n) \u2014\nThe function to apply to the model outputs in order to retrieve the scores. Accepts four different values:\n\n\n\n\n\"default\"\n: if the model has a single label, will apply the sigmoid function on the output. If the model\nhas several labels, will apply the softmax function on the output. In case of regression tasks, will not\napply any function on the output.\n\n\n\"sigmoid\"\n: Applies the sigmoid function on the output.\n\n\n\"softmax\"\n: Applies the softmax function on the output.\n\n\n\"none\"\n: Does not apply any function on the output.\n\n\n \n \n \n \nText classification pipeline using any \nModelForSequenceClassification\n. See the \nsequence classification\nexamples\n for more information.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \nclassifier = pipeline(model=\n\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n)\n\n>>> \nclassifier(\n\"This movie is disgustingly good !\"\n)\n[{\n'label'\n: \n'POSITIVE'\n, \n'score'\n: \n1.0\n}]\n\n\n>>> \nclassifier(\n\"Director tried too much.\"\n)\n[{\n'label'\n: \n'NEGATIVE'\n, \n'score'\n: \n0.996\n}]\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis text classification pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"sentiment-analysis\"\n (for classifying sequences according to positive or negative sentiments).\n \nIf multiple classification labels are available (\nmodel.config.num_labels >= 2\n), the pipeline will run a softmax\nover the results. If there is a single label, the pipeline will run a sigmoid over the result. In case of regression\ntasks (\nmodel.config.problem_type == \"regression\"\n), will not apply any function on the output.\n \nThe models that this pipeline can use are models that have been fine-tuned on a sequence classification task. See\nthe up-to-date list of available models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \ninputs\n: typing.Union[str, list[str], dict[str, str], list[dict[str, str]]]\n \n**kwargs\n: typing.Any\n \n \n)\n \n\u2192\n \nA list of \ndict\n \n \nParameters \n \n \ninputs\n (\nstr\n or \nlist[str]\n or \ndict[str]\n, or \nlist[dict[str]]\n) \u2014\nOne or several texts to classify. In order to use text pairs for your classification, you can send a\ndictionary containing \n{\"text\", \"text_pair\"}\n keys, or a list of those.\n \n \n \ntop_k\n (\nint\n, \noptional\n, defaults to \n1\n) \u2014\nHow many results to return.\n \n \n \nfunction_to_apply\n (\nstr\n, \noptional\n, defaults to \n\"default\"\n) \u2014\nThe function to apply to the model outputs in order to retrieve the scores. Accepts four different\nvalues:\n\n\nIf this argument is not specified, then it will apply the following functions according to the number\nof labels:\n\n\n\n\nIf problem type is regression, will not apply any function on the output.\n\n\nIf the model has a single label, will apply the sigmoid function on the output.\n\n\nIf the model has several labels, will apply the softmax function on the output.\n\n\n\n\nPossible values are:\n\n\n\n\n\"sigmoid\"\n: Applies the sigmoid function on the output.\n\n\n\"softmax\"\n: Applies the softmax function on the output.\n\n\n\"none\"\n: Does not apply any function on the output.\n\n\n \n \n \nReturns\n \n\n\nA list of \ndict\n\n\n \n \n\n\nEach result comes as list of dictionaries with the following keys:\n\n\n\n\nlabel\n (\nstr\n) \u2014 The label predicted.\n\n\nscore\n (\nfloat\n) \u2014 The corresponding probability.\n\n\n\n\nIf \ntop_k\n is used, one such dictionary is returned per label.\n\n\n \n \nClassify the text(s) given as inputs.\n \n \nTextGenerationPipeline\n \n \nclass\n \ntransformers.\nTextGenerationPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nLanguage generation pipeline using any \nModelWithLMHead\n or \nModelForCausalLM\n. This pipeline predicts the words\nthat will follow a specified text prompt. When the underlying model is a conversational model, it can also accept\none or more chats, in which case the pipeline will operate in chat mode and will continue the chat(s) by adding\nits response(s). Each chat takes the form of a list of dicts, where each dict contains \u201crole\u201d and \u201ccontent\u201d keys.\n \nUnless the model you\u2019re using explicitly sets these generation parameters in its configuration files\n(\ngeneration_config.json\n), the following default values will be used:\n \nmax_new_tokens: 256\n \ndo_sample: True\n \ntemperature: 0.7\n \n \nExamples:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \ngenerator = pipeline(model=\n\"openai-community/gpt2\"\n)\n\n>>> \ngenerator(\n\"I can't believe you did such a \"\n, do_sample=\nFalse\n)\n[{\n'generated_text'\n: \n\"I can't believe you did such a icky thing to me. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I\"\n}]\n\n\n>>> \n# These parameters will return suggestions, and only the newly created text making it easier for prompting suggestions.\n\n\n>>> \noutputs = generator(\n\"My tart needs some\"\n, num_return_sequences=\n4\n, return_full_text=\nFalse\n)\n \n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \ngenerator = pipeline(model=\n\"HuggingFaceH4/zephyr-7b-beta\"\n)\n\n>>> \n# Zephyr-beta is a conversational model, so let's pass it a chat instead of a single string\n\n\n>>> \ngenerator([{\n\"role\"\n: \n\"user\"\n, \n\"content\"\n: \n\"What is the capital of France? Answer in one word.\"\n}], do_sample=\nFalse\n, max_new_tokens=\n2\n)\n[{\n'generated_text'\n: [{\n'role'\n: \n'user'\n, \n'content'\n: \n'What is the capital of France? Answer in one word.'\n}, {\n'role'\n: \n'assistant'\n, \n'content'\n: \n'Paris'\n}]}]\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n. You can pass text\ngeneration parameters to this pipeline to control stopping criteria, decoding strategy, and more. Learn more about\ntext generation parameters in \nText generation strategies\n and \nText\ngeneration\n.\n \nThis language generation pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"text-generation\"\n.\n \nThe models that this pipeline can use are models that have been trained with an autoregressive language modeling\nobjective. See the list of available \ntext completion models\n\nand the list of \nconversational models\n\non [huggingface.co/models].\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \ntext_inputs\n \n**kwargs\n \n \n)\n \n\u2192\n \nA list or a list of lists of \ndict\n \n \nParameters \n \n \ntext_inputs\n (\nstr\n, \nlist[str]\n, list[dict[str, str]], or \nlist[list[dict[str, str]]]\n) \u2014\nOne or several prompts (or one list of prompts) to complete. If strings or a list of string are\npassed, this pipeline will continue each prompt. Alternatively, a \u201cchat\u201d, in the form of a list\nof dicts with \u201crole\u201d and \u201ccontent\u201d keys, can be passed, or a list of such chats. When chats are passed,\nthe model\u2019s chat template will be used to format them before passing them to the model.\n \n \n \nreturn_tensors\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nReturns the tensors of predictions (as token indices) in the outputs. If set to\n\nTrue\n, the decoded text is not returned.\n \n \n \nreturn_text\n (\nbool\n, \noptional\n) \u2014\nReturns the decoded texts in the outputs.\n \n \n \nreturn_full_text\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nIf set to \nFalse\n only added text is returned, otherwise the full text is returned. Cannot be\nspecified at the same time as \nreturn_text\n.\n \n \n \nclean_up_tokenization_spaces\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether or not to clean up the potential extra spaces in the text output.\n \n \n \ncontinue_final_message(\n \nbool\n, \noptional\n) \u2014 This indicates that you want the model to continue the\nlast message in the input chat rather than starting a new one, allowing you to \u201cprefill\u201d its response.\nBy default this is \nTrue\n when the final message in the input chat has the \nassistant\n role and\n\nFalse\n otherwise, but you can manually override that behaviour by setting this flag.\n \n \n \nprefix\n (\nstr\n, \noptional\n) \u2014\nPrefix added to prompt.\n \n \n \nhandle_long_generation\n (\nstr\n, \noptional\n) \u2014\nBy default, this pipelines does not handle long generation (ones that exceed in one form or the other\nthe model maximum length). There is no perfect way to address this (more info\n:\nhttps://github.com/huggingface/transformers/issues/14033#issuecomment-948385227\n). This provides common\nstrategies to work around that problem depending on your use case.\n\n\n\n\nNone\n : default strategy where nothing in particular happens\n\n\n\"hole\"\n: Truncates left of input, and leaves a gap wide enough to let generation happen (might\ntruncate a lot of the prompt and not suitable when generation exceed the model capacity)\n\n\n \n \n \ngenerate_kwargs\n (\ndict\n, \noptional\n) \u2014\nAdditional keyword arguments to pass along to the generate method of the model (see the generate method\ncorresponding to your framework \nhere\n).\n \n \n \nReturns\n \n\n\nA list or a list of lists of \ndict\n\n\n \n \n\n\nReturns one of the following dictionaries (cannot return a combination\nof both \ngenerated_text\n and \ngenerated_token_ids\n):\n\n\n\n\ngenerated_text\n (\nstr\n, present when \nreturn_text=True\n) \u2014 The generated text.\n\n\ngenerated_token_ids\n (\ntorch.Tensor\n or \ntf.Tensor\n, present when \nreturn_tensors=True\n) \u2014 The token\nids of the generated text.\n\n\n\n\n \n \nComplete the prompt(s) given as inputs.\n \n \nText2TextGenerationPipeline\n \n \nclass\n \ntransformers.\nText2TextGenerationPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nPipeline for text to text generation using seq2seq models.\n \nUnless the model you\u2019re using explicitly sets these generation parameters in its configuration files\n(\ngeneration_config.json\n), the following default values will be used:\n \nmax_new_tokens: 256\n \nnum_beams: 4\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \ngenerator = pipeline(model=\n\"mrm8488/t5-base-finetuned-question-generation-ap\"\n)\n\n>>> \ngenerator(\n\n... \n    \n\"answer: Manuel context: Manuel has created RuPERTa-base with the support of HF-Transformers and Google\"\n\n\n... \n)\n[{\n'generated_text'\n: \n'question: Who created the RuPERTa-base?'\n}]\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n. You can pass text\ngeneration parameters to this pipeline to control stopping criteria, decoding strategy, and more. Learn more about\ntext generation parameters in \nText generation strategies\n and \nText\ngeneration\n.\n \nThis Text2TextGenerationPipeline pipeline can currently be loaded from \npipeline()\n using the following task\nidentifier: \n\"text2text-generation\"\n.\n \nThe models that this pipeline can use are models that have been fine-tuned on a translation task. See the\nup-to-date list of available models on\n\nhuggingface.co/models\n. For a list of available\nparameters, see the \nfollowing\ndocumentation\n \n \nUsage:\n \n \n Copied\n \ntext2text_generator = pipeline(\n\"text2text-generation\"\n)\ntext2text_generator(\n\"question: What is 42 ? context: 42 is the answer to life, the universe and everything\"\n)\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n: typing.Union[str, list[str]]\n \n**kwargs\n: typing.Any\n \n \n)\n \n\u2192\n \nA list or a list of list of \ndict\n \n \nParameters \n \n \nargs\n (\nstr\n or \nlist[str]\n) \u2014\nInput text for the encoder.\n \n \n \nreturn_tensors\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to include the tensors of predictions (as token indices) in the outputs.\n \n \n \nreturn_text\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether or not to include the decoded texts in the outputs.\n \n \n \nclean_up_tokenization_spaces\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to clean up the potential extra spaces in the text output.\n \n \n \ntruncation\n (\nTruncationStrategy\n, \noptional\n, defaults to \nTruncationStrategy.DO_NOT_TRUNCATE\n) \u2014\nThe truncation strategy for the tokenization within the pipeline. \nTruncationStrategy.DO_NOT_TRUNCATE\n\n(default) will never truncate, but it is sometimes desirable to truncate the input to fit the model\u2019s\nmax_length instead of throwing an error down the line.\n \n \n \ngenerate_kwargs\n \u2014\nAdditional keyword arguments to pass along to the generate method of the model (see the generate method\ncorresponding to your framework \nhere\n).\n \n \n \nReturns\n \n\n\nA list or a list of list of \ndict\n\n\n \n \n\n\nEach result comes as a dictionary with the following keys:\n\n\n\n\ngenerated_text\n (\nstr\n, present when \nreturn_text=True\n) \u2014 The generated text.\n\n\ngenerated_token_ids\n (\ntorch.Tensor\n or \ntf.Tensor\n, present when \nreturn_tensors=True\n) \u2014 The token\nids of the generated text.\n\n\n\n\n \n \nGenerate the output text(s) using text(s) given as inputs.\n \n \ncheck_inputs\n \n \n<\n \nsource\n \n>\n \n(\n \ninput_length\n: int\n \nmin_length\n: int\n \nmax_length\n: int\n \n \n)\n \n \n \n \nChecks whether there might be something wrong with given input with regard to the model.\n \n \nTokenClassificationPipeline\n \n \nclass\n \ntransformers.\nTokenClassificationPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \nargs_parser\n = <transformers.pipelines.token_classification.TokenClassificationArgumentHandler object at 0x7f813a292650>\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \nignore_labels\n (\nlist[str]\n, defaults to \n[\"O\"]\n) \u2014\nA list of labels to ignore.\n \n \n \ngrouped_entities\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nDEPRECATED, use \naggregation_strategy\n instead. Whether or not to group the tokens corresponding to the\nsame entity together in the predictions or not.\n \n \n \nstride\n (\nint\n, \noptional\n) \u2014\nIf stride is provided, the pipeline is applied on all the text. The text is split into chunks of size\nmodel_max_length. Works only with fast tokenizers and \naggregation_strategy\n different from \nNONE\n. The\nvalue of this argument defines the number of overlapping tokens between chunks. In other words, the model\nwill shift forward by \ntokenizer.model_max_length - stride\n tokens each step.\n \n \n \naggregation_strategy\n (\nstr\n, \noptional\n, defaults to \n\"none\"\n) \u2014\nThe strategy to fuse (or not) tokens based on the model prediction.\n\n\n\n\n\u201cnone\u201d : Will simply not do any aggregation and simply return raw results from the model\n\n\n\u201csimple\u201d : Will attempt to group entities following the default schema. (A, B-TAG), (B, I-TAG), (C,\nI-TAG), (D, B-TAG2) (E, B-TAG2) will end up being [{\u201cword\u201d: ABC, \u201centity\u201d: \u201cTAG\u201d}, {\u201cword\u201d: \u201cD\u201d,\n\u201centity\u201d: \u201cTAG2\u201d}, {\u201cword\u201d: \u201cE\u201d, \u201centity\u201d: \u201cTAG2\u201d}] Notice that two consecutive B tags will end up as\ndifferent entities. On word based languages, we might end up splitting words undesirably : Imagine\nMicrosoft being tagged as [{\u201cword\u201d: \u201cMicro\u201d, \u201centity\u201d: \u201cENTERPRISE\u201d}, {\u201cword\u201d: \u201csoft\u201d, \u201centity\u201d:\n\u201cNAME\u201d}]. Look for FIRST, MAX, AVERAGE for ways to mitigate that and disambiguate words (on languages\nthat support that meaning, which is basically tokens separated by a space). These mitigations will\nonly work on real words, \u201cNew york\u201d might still be tagged with two different entities.\n\n\n\u201cfirst\u201d : (works only on word based models) Will use the \nSIMPLE\n strategy except that words, cannot\nend up with different tags. Words will simply use the tag of the first token of the word when there\nis ambiguity.\n\n\n\u201caverage\u201d : (works only on word based models) Will use the \nSIMPLE\n strategy except that words,\ncannot end up with different tags. scores will be averaged first across tokens, and then the maximum\nlabel is applied.\n\n\n\u201cmax\u201d : (works only on word based models) Will use the \nSIMPLE\n strategy except that words, cannot\nend up with different tags. Word entity will simply be the token with the maximum score.\n\n\n \n \n \n \nNamed Entity Recognition pipeline using any \nModelForTokenClassification\n. See the \nnamed entity recognition\nexamples\n for more information.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \ntoken_classifier = pipeline(model=\n\"Jean-Baptiste/camembert-ner\"\n, aggregation_strategy=\n\"simple\"\n)\n\n>>> \nsentence = \n\"Je m'appelle jean-baptiste et je vis \u00e0 montr\u00e9al\"\n\n\n>>> \ntokens = token_classifier(sentence)\n\n>>> \ntokens\n[{\n'entity_group'\n: \n'PER'\n, \n'score'\n: \n0.9931\n, \n'word'\n: \n'jean-baptiste'\n, \n'start'\n: \n12\n, \n'end'\n: \n26\n}, {\n'entity_group'\n: \n'LOC'\n, \n'score'\n: \n0.998\n, \n'word'\n: \n'montr\u00e9al'\n, \n'start'\n: \n38\n, \n'end'\n: \n47\n}]\n\n\n>>> \ntoken = tokens[\n0\n]\n\n>>> \n# Start and end provide an easy way to highlight words in the original text.\n\n\n>>> \nsentence[token[\n\"start\"\n] : token[\n\"end\"\n]]\n\n' jean-baptiste'\n\n\n\n>>> \n# Some models use the same idea to do part of speech.\n\n\n>>> \nsyntaxer = pipeline(model=\n\"vblagoje/bert-english-uncased-finetuned-pos\"\n, aggregation_strategy=\n\"simple\"\n)\n\n>>> \nsyntaxer(\n\"My name is Sarah and I live in London\"\n)\n[{\n'entity_group'\n: \n'PRON'\n, \n'score'\n: \n0.999\n, \n'word'\n: \n'my'\n, \n'start'\n: \n0\n, \n'end'\n: \n2\n}, {\n'entity_group'\n: \n'NOUN'\n, \n'score'\n: \n0.997\n, \n'word'\n: \n'name'\n, \n'start'\n: \n3\n, \n'end'\n: \n7\n}, {\n'entity_group'\n: \n'AUX'\n, \n'score'\n: \n0.994\n, \n'word'\n: \n'is'\n, \n'start'\n: \n8\n, \n'end'\n: \n10\n}, {\n'entity_group'\n: \n'PROPN'\n, \n'score'\n: \n0.999\n, \n'word'\n: \n'sarah'\n, \n'start'\n: \n11\n, \n'end'\n: \n16\n}, {\n'entity_group'\n: \n'CCONJ'\n, \n'score'\n: \n0.999\n, \n'word'\n: \n'and'\n, \n'start'\n: \n17\n, \n'end'\n: \n20\n}, {\n'entity_group'\n: \n'PRON'\n, \n'score'\n: \n0.999\n, \n'word'\n: \n'i'\n, \n'start'\n: \n21\n, \n'end'\n: \n22\n}, {\n'entity_group'\n: \n'VERB'\n, \n'score'\n: \n0.998\n, \n'word'\n: \n'live'\n, \n'start'\n: \n23\n, \n'end'\n: \n27\n}, {\n'entity_group'\n: \n'ADP'\n, \n'score'\n: \n0.999\n, \n'word'\n: \n'in'\n, \n'start'\n: \n28\n, \n'end'\n: \n30\n}, {\n'entity_group'\n: \n'PROPN'\n, \n'score'\n: \n0.999\n, \n'word'\n: \n'london'\n, \n'start'\n: \n31\n, \n'end'\n: \n37\n}]\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis token recognition pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"ner\"\n (for predicting the classes of tokens in a sequence: person, organisation, location or miscellaneous).\n \nThe models that this pipeline can use are models that have been fine-tuned on a token classification task. See the\nup-to-date list of available models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \ninputs\n: typing.Union[str, list[str]]\n \n**kwargs\n: typing.Any\n \n \n)\n \n\u2192\n \nA list or a list of list of \ndict\n \n \nParameters \n \n \ninputs\n (\nstr\n or \nList[str]\n) \u2014\nOne or several texts (or one list of texts) for token classification. Can be pre-tokenized when\n\nis_split_into_words=True\n.\n \n \n \nReturns\n \n\n\nA list or a list of list of \ndict\n\n\n \n \n\n\nEach result comes as a list of dictionaries (one for each token in the\ncorresponding input, or each entity if this pipeline was instantiated with an aggregation_strategy) with\nthe following keys:\n\n\n\n\nword\n (\nstr\n) \u2014 The token/word classified. This is obtained by decoding the selected tokens. If you\nwant to have the exact string in the original sentence, use \nstart\n and \nend\n.\n\n\nscore\n (\nfloat\n) \u2014 The corresponding probability for \nentity\n.\n\n\nentity\n (\nstr\n) \u2014 The entity predicted for that token/word (it is named \nentity_group\n when\n\naggregation_strategy\n is not \n\"none\"\n.\n\n\nindex\n (\nint\n, only present when \naggregation_strategy=\"none\"\n) \u2014 The index of the corresponding\ntoken in the sentence.\n\n\nstart\n (\nint\n, \noptional\n) \u2014 The index of the start of the corresponding entity in the sentence. Only\nexists if the offsets are available within the tokenizer\n\n\nend\n (\nint\n, \noptional\n) \u2014 The index of the end of the corresponding entity in the sentence. Only\nexists if the offsets are available within the tokenizer\n\n\n\n\n \n \nClassify each token of the text(s) given as inputs.\n \n \naggregate_words\n \n \n<\n \nsource\n \n>\n \n(\n \nentities\n: list\n \naggregation_strategy\n: AggregationStrategy\n \n \n)\n \n \n \n \nOverride tokens from a given word that disagree to force agreement on word boundaries.\n \nExample: micro|soft| com|pany| B-ENT I-NAME I-ENT I-ENT will be rewritten with first strategy as microsoft|\ncompany| B-ENT I-ENT\n \n \ngather_pre_entities\n \n \n<\n \nsource\n \n>\n \n(\n \nsentence\n: str\n \ninput_ids\n: ndarray\n \nscores\n: ndarray\n \noffset_mapping\n: typing.Optional[list[tuple[int, int]]]\n \nspecial_tokens_mask\n: ndarray\n \naggregation_strategy\n: AggregationStrategy\n \nword_ids\n: typing.Optional[list[typing.Optional[int]]] = None\n \nword_to_chars_map\n: typing.Optional[list[tuple[int, int]]] = None\n \n \n)\n \n \n \n \nFuse various numpy arrays into dicts with all the information needed for aggregation\n \n \ngroup_entities\n \n \n<\n \nsource\n \n>\n \n(\n \nentities\n: list\n \n \n)\n \n \n \nParameters \n \n \nentities\n (\ndict\n) \u2014 The entities predicted by the pipeline.\n \n \n \n \nFind and group together the adjacent tokens with the same entity predicted.\n \n \ngroup_sub_entities\n \n \n<\n \nsource\n \n>\n \n(\n \nentities\n: list\n \n \n)\n \n \n \nParameters \n \n \nentities\n (\ndict\n) \u2014 The entities predicted by the pipeline.\n \n \n \n \nGroup together the adjacent tokens with the same entity predicted.\n \n \nTranslationPipeline\n \n \nclass\n \ntransformers.\nTranslationPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nTranslates from one language to another.\n \nThis translation pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"translation_xx_to_yy\"\n.\n \nThe models that this pipeline can use are models that have been fine-tuned on a translation task. See the\nup-to-date list of available models on \nhuggingface.co/models\n.\nFor a list of available parameters, see the \nfollowing\ndocumentation\n \nUnless the model you\u2019re using explicitly sets these generation parameters in its configuration files\n(\ngeneration_config.json\n), the following default values will be used:\n \nmax_new_tokens: 256\n \nnum_beams: 4\n \n \nUsage:\n \n \n Copied\n \nen_fr_translator = pipeline(\n\"translation_en_to_fr\"\n)\nen_fr_translator(\n\"How old are you?\"\n)\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n\u2192\n \nA list or a list of list of \ndict\n \n \nParameters \n \n \nargs\n (\nstr\n or \nlist[str]\n) \u2014\nTexts to be translated.\n \n \n \nreturn_tensors\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to include the tensors of predictions (as token indices) in the outputs.\n \n \n \nreturn_text\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether or not to include the decoded texts in the outputs.\n \n \n \nclean_up_tokenization_spaces\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to clean up the potential extra spaces in the text output.\n \n \n \nsrc_lang\n (\nstr\n, \noptional\n) \u2014\nThe language of the input. Might be required for multilingual models. Will not have any effect for\nsingle pair translation models\n \n \n \ntgt_lang\n (\nstr\n, \noptional\n) \u2014\nThe language of the desired output. Might be required for multilingual models. Will not have any effect\nfor single pair translation models\n \n \n \ngenerate_kwargs\n \u2014\nAdditional keyword arguments to pass along to the generate method of the model (see the generate method\ncorresponding to your framework \nhere\n).\n \n \n \nReturns\n \n\n\nA list or a list of list of \ndict\n\n\n \n \n\n\nEach result comes as a dictionary with the following keys:\n\n\n\n\ntranslation_text\n (\nstr\n, present when \nreturn_text=True\n) \u2014 The translation.\n\n\ntranslation_token_ids\n (\ntorch.Tensor\n or \ntf.Tensor\n, present when \nreturn_tensors=True\n) \u2014 The\ntoken ids of the translation.\n\n\n\n\n \n \nTranslate the text(s) given as inputs.\n \n \nZeroShotClassificationPipeline\n \n \nclass\n \ntransformers.\nZeroShotClassificationPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \nargs_parser\n = <transformers.pipelines.zero_shot_classification.ZeroShotClassificationArgumentHandler object at 0x7f813a2681f0>\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nNLI-based zero-shot classification pipeline using a \nModelForSequenceClassification\n trained on NLI (natural\nlanguage inference) tasks. Equivalent of \ntext-classification\n pipelines, but these models don\u2019t require a\nhardcoded number of potential classes, they can be chosen at runtime. It usually means it\u2019s slower but it is\n\nmuch\n more flexible.\n \nAny combination of sequences and labels can be passed and each combination will be posed as a premise/hypothesis\npair and passed to the pretrained model. Then, the logit for \nentailment\n is taken as the logit for the candidate\nlabel being valid. Any NLI model can be used, but the id of the \nentailment\n label must be included in the model\nconfig\u2019s :attr:\n~transformers.PretrainedConfig.label2id\n.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \noracle = pipeline(model=\n\"facebook/bart-large-mnli\"\n)\n\n>>> \noracle(\n\n... \n    \n\"I have a problem with my iphone that needs to be resolved asap!!\"\n,\n\n... \n    candidate_labels=[\n\"urgent\"\n, \n\"not urgent\"\n, \n\"phone\"\n, \n\"tablet\"\n, \n\"computer\"\n],\n\n... \n)\n{\n'sequence'\n: \n'I have a problem with my iphone that needs to be resolved asap!!'\n, \n'labels'\n: [\n'urgent'\n, \n'phone'\n, \n'computer'\n, \n'not urgent'\n, \n'tablet'\n], \n'scores'\n: [\n0.504\n, \n0.479\n, \n0.013\n, \n0.003\n, \n0.002\n]}\n\n\n>>> \noracle(\n\n... \n    \n\"I have a problem with my iphone that needs to be resolved asap!!\"\n,\n\n... \n    candidate_labels=[\n\"english\"\n, \n\"german\"\n],\n\n... \n)\n{\n'sequence'\n: \n'I have a problem with my iphone that needs to be resolved asap!!'\n, \n'labels'\n: [\n'english'\n, \n'german'\n], \n'scores'\n: [\n0.814\n, \n0.186\n]}\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis NLI pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"zero-shot-classification\"\n.\n \nThe models that this pipeline can use are models that have been fine-tuned on an NLI task. See the up-to-date list\nof available models on \nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \nsequences\n: typing.Union[str, list[str]]\n \n*args\n \n**kwargs\n \n \n)\n \n\u2192\n \nA \ndict\n or a list of \ndict\n \n \nParameters \n \n \nsequences\n (\nstr\n or \nlist[str]\n) \u2014\nThe sequence(s) to classify, will be truncated if the model input is too large.\n \n \n \ncandidate_labels\n (\nstr\n or \nlist[str]\n) \u2014\nThe set of possible class labels to classify each sequence into. Can be a single label, a string of\ncomma-separated labels, or a list of labels.\n \n \n \nhypothesis_template\n (\nstr\n, \noptional\n, defaults to \n\"This example is {}.\"\n) \u2014\nThe template used to turn each label into an NLI-style hypothesis. This template must include a {} or\nsimilar syntax for the candidate label to be inserted into the template. For example, the default\ntemplate is \n\"This example is {}.\"\n With the candidate label \n\"sports\"\n, this would be fed into the\nmodel like \n\"<cls> sequence to classify <sep> This example is sports . <sep>\"\n. The default template\nworks well in many cases, but it may be worthwhile to experiment with different templates depending on\nthe task setting.\n \n \n \nmulti_label\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not multiple candidate labels can be true. If \nFalse\n, the scores are normalized such that\nthe sum of the label likelihoods for each sequence is 1. If \nTrue\n, the labels are considered\nindependent and probabilities are normalized for each candidate by doing a softmax of the entailment\nscore vs. the contradiction score.\n \n \n \nReturns\n \n\n\nA \ndict\n or a list of \ndict\n\n\n \n \n\n\nEach result comes as a dictionary with the following keys:\n\n\n\n\nsequence\n (\nstr\n) \u2014 The sequence for which this is the output.\n\n\nlabels\n (\nlist[str]\n) \u2014 The labels sorted by order of likelihood.\n\n\nscores\n (\nlist[float]\n) \u2014 The probabilities for each of the labels.\n\n\n\n\n \n \nClassify the sequence(s) given as inputs. See the \nZeroShotClassificationPipeline\n documentation for more\ninformation.\n \n \nMultimodal\n \nPipelines available for multimodal tasks include the following.\n \n \nDocumentQuestionAnsweringPipeline\n \n \nclass\n \ntransformers.\nDocumentQuestionAnsweringPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nimage_processor\n (\nBaseImageProcessor\n) \u2014\nThe image processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nBaseImageProcessor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nDocument Question Answering pipeline using any \nAutoModelForDocumentQuestionAnswering\n. The inputs/outputs are\nsimilar to the (extractive) question answering pipeline; however, the pipeline takes an image (and optional OCR\u2019d\nwords/boxes) as input instead of text context.\n \nUnless the model you\u2019re using explicitly sets these generation parameters in its configuration files\n(\ngeneration_config.json\n), the following default values will be used:\n \nmax_new_tokens: 256\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \ndocument_qa = pipeline(model=\n\"impira/layoutlm-document-qa\"\n)\n\n>>> \ndocument_qa(\n\n... \n    image=\n\"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\"\n,\n\n... \n    question=\n\"What is the invoice number?\"\n,\n\n... \n)\n[{\n'score'\n: \n0.425\n, \n'answer'\n: \n'us-001'\n, \n'start'\n: \n16\n, \n'end'\n: \n16\n}]\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis document question answering pipeline can currently be loaded from \npipeline()\n using the following task\nidentifier: \n\"document-question-answering\"\n.\n \nThe models that this pipeline can use are models that have been fine-tuned on a document question answering task.\nSee the up-to-date list of available models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \nimage\n: typing.Union[ForwardRef('Image.Image'), str, list[dict[str, typing.Any]]]\n \nquestion\n: typing.Optional[str] = None\n \nword_boxes\n: typing.Optional[tuple[str, list[float]]] = None\n \n**kwargs\n: typing.Any\n \n \n)\n \n\u2192\n \nA \ndict\n or a list of \ndict\n \n \nParameters \n \n \nimage\n (\nstr\n or \nPIL.Image\n) \u2014\nThe pipeline handles three types of images:\n\n\n\n\nA string containing a http link pointing to an image\n\n\nA string containing a local path to an image\n\n\nAn image loaded in PIL directly\n\n\n\n\nThe pipeline accepts either a single image or a batch of images. If given a single image, it can be\nbroadcasted to multiple questions.\n \n \n \nquestion\n (\nstr\n) \u2014\nA question to ask of the document.\n \n \n \nword_boxes\n (\nlist[str, tuple[float, float, float, float]]\n, \noptional\n) \u2014\nA list of words and bounding boxes (normalized 0->1000). If you provide this optional input, then the\npipeline will use these words and boxes instead of running OCR on the image to derive them for models\nthat need them (e.g. LayoutLM). This allows you to reuse OCR\u2019d results across many invocations of the\npipeline without having to re-run it each time.\n \n \n \ntop_k\n (\nint\n, \noptional\n, defaults to 1) \u2014\nThe number of answers to return (will be chosen by order of likelihood). Note that we return less than\ntop_k answers if there are not enough options available within the context.\n \n \n \ndoc_stride\n (\nint\n, \noptional\n, defaults to 128) \u2014\nIf the words in the document are too long to fit with the question for the model, it will be split in\nseveral chunks with some overlap. This argument controls the size of that overlap.\n \n \n \nmax_answer_len\n (\nint\n, \noptional\n, defaults to 15) \u2014\nThe maximum length of predicted answers (e.g., only answers with a shorter length are considered).\n \n \n \nmax_seq_len\n (\nint\n, \noptional\n, defaults to 384) \u2014\nThe maximum length of the total sentence (context + question) in tokens of each chunk passed to the\nmodel. The context will be split in several chunks (using \ndoc_stride\n as overlap) if needed.\n \n \n \nmax_question_len\n (\nint\n, \noptional\n, defaults to 64) \u2014\nThe maximum length of the question after tokenization. It will be truncated if needed.\n \n \n \nhandle_impossible_answer\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not we accept impossible as an answer.\n \n \n \nlang\n (\nstr\n, \noptional\n) \u2014\nLanguage to use while running OCR. Defaults to english.\n \n \n \ntesseract_config\n (\nstr\n, \noptional\n) \u2014\nAdditional flags to pass to tesseract while running OCR.\n \n \n \ntimeout\n (\nfloat\n, \noptional\n, defaults to None) \u2014\nThe maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\nthe call may block forever.\n \n \n \nReturns\n \n\n\nA \ndict\n or a list of \ndict\n\n\n \n \n\n\nEach result comes as a dictionary with the following keys:\n\n\n\n\nscore\n (\nfloat\n) \u2014 The probability associated to the answer.\n\n\nstart\n (\nint\n) \u2014 The start word index of the answer (in the OCR\u2019d version of the input or provided\n\nword_boxes\n).\n\n\nend\n (\nint\n) \u2014 The end word index of the answer (in the OCR\u2019d version of the input or provided\n\nword_boxes\n).\n\n\nanswer\n (\nstr\n) \u2014 The answer to the question.\n\n\nwords\n (\nlist[int]\n) \u2014 The index of each word/box pair that is in the answer\n\n\n\n\n \n \nAnswer the question(s) given as inputs by using the document(s). A document is defined as an image and an\noptional list of (word, box) tuples which represent the text in the document. If the \nword_boxes\n are not\nprovided, it will use the Tesseract OCR engine (if available) to extract the words and boxes automatically for\nLayoutLM-like models which require them as input. For Donut, no OCR is run.\n \nYou can invoke the pipeline several ways:\n \npipeline(image=image, question=question)\n \npipeline(image=image, question=question, word_boxes=word_boxes)\n \npipeline([{\"image\": image, \"question\": question}])\n \npipeline([{\"image\": image, \"question\": question, \"word_boxes\": word_boxes}])\n \n \nFeatureExtractionPipeline\n \n \nclass\n \ntransformers.\nFeatureExtractionPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \nmodel\n: typing.Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]\n \ntokenizer\n: typing.Optional[transformers.tokenization_utils.PreTrainedTokenizer] = None\n \nfeature_extractor\n: typing.Optional[ForwardRef('SequenceFeatureExtractor')] = None\n \nimage_processor\n: typing.Optional[transformers.image_processing_utils.BaseImageProcessor] = None\n \nprocessor\n: typing.Optional[transformers.processing_utils.ProcessorMixin] = None\n \nmodelcard\n: typing.Optional[transformers.modelcard.ModelCard] = None\n \nframework\n: typing.Optional[str] = None\n \ntask\n: str = ''\n \nargs_parser\n: ArgumentHandler = None\n \ndevice\n: typing.Union[int, ForwardRef('torch.device')] = None\n \ntorch_dtype\n: typing.Union[str, ForwardRef('torch.dtype'), NoneType] = None\n \nbinary_output\n: bool = False\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \ntokenize_kwargs\n (\ndict\n, \noptional\n) \u2014\nAdditional dictionary of keyword arguments passed along to the tokenizer.\n \n \n \nreturn_tensors\n (\nbool\n, \noptional\n) \u2014\nIf \nTrue\n, returns a tensor according to the specified framework, otherwise returns a list.\n \n \n \n \nFeature extraction pipeline uses no model head. This pipeline extracts the hidden states from the base\ntransformer, which can be used as features in downstream tasks.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \nextractor = pipeline(model=\n\"google-bert/bert-base-uncased\"\n, task=\n\"feature-extraction\"\n)\n\n>>> \nresult = extractor(\n\"This is a simple test.\"\n, return_tensors=\nTrue\n)\n\n>>> \nresult.shape  \n# This is a tensor of shape [1, sequence_length, hidden_dimension] representing the input string.\n\ntorch.Size([\n1\n, \n8\n, \n768\n])\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis feature extraction pipeline can currently be loaded from \npipeline()\n using the task identifier:\n\n\"feature-extraction\"\n.\n \nAll models may be used for this pipeline. See a list of all models, including community-contributed models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n: typing.Union[str, list[str]]\n \n**kwargs\n: typing.Any\n \n \n)\n \n\u2192\n \nA nested list of \nfloat\n \n \nParameters \n \n \nargs\n (\nstr\n or \nlist[str]\n) \u2014 One or several texts (or one list of texts) to get the features of.\n \n \n \nReturns\n \n\n\nA nested list of \nfloat\n\n\n \n \n\n\nThe features computed by the model.\n\n\n \n \nExtract the features of the input(s) text.\n \n \nImageFeatureExtractionPipeline\n \n \nclass\n \ntransformers.\nImageFeatureExtractionPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \nmodel\n: typing.Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]\n \ntokenizer\n: typing.Optional[transformers.tokenization_utils.PreTrainedTokenizer] = None\n \nfeature_extractor\n: typing.Optional[ForwardRef('SequenceFeatureExtractor')] = None\n \nimage_processor\n: typing.Optional[transformers.image_processing_utils.BaseImageProcessor] = None\n \nprocessor\n: typing.Optional[transformers.processing_utils.ProcessorMixin] = None\n \nmodelcard\n: typing.Optional[transformers.modelcard.ModelCard] = None\n \nframework\n: typing.Optional[str] = None\n \ntask\n: str = ''\n \nargs_parser\n: ArgumentHandler = None\n \ndevice\n: typing.Union[int, ForwardRef('torch.device')] = None\n \ntorch_dtype\n: typing.Union[str, ForwardRef('torch.dtype'), NoneType] = None\n \nbinary_output\n: bool = False\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \nimage_processor\n (\nBaseImageProcessor\n) \u2014\nThe image processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nBaseImageProcessor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \nimage_processor_kwargs\n (\ndict\n, \noptional\n) \u2014\nAdditional dictionary of keyword arguments passed along to the image processor e.g.\n{\u201csize\u201d: {\u201cheight\u201d: 100, \u201cwidth\u201d: 100}\u200c}\n \n \n \npool\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to return the pooled output. If \nFalse\n, the model will return the raw hidden states.\n \n \n \n \nImage feature extraction pipeline uses no model head. This pipeline extracts the hidden states from the base\ntransformer, which can be used as features in downstream tasks.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \nextractor = pipeline(model=\n\"google/vit-base-patch16-224\"\n, task=\n\"image-feature-extraction\"\n)\n\n>>> \nresult = extractor(\n\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n, return_tensors=\nTrue\n)\n\n>>> \nresult.shape  \n# This is a tensor of shape [1, sequence_lenth, hidden_dimension] representing the input image.\n\ntorch.Size([\n1\n, \n197\n, \n768\n])\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis image feature extraction pipeline can currently be loaded from \npipeline()\n using the task identifier:\n\n\"image-feature-extraction\"\n.\n \nAll vision models may be used for this pipeline. See a list of all models, including community-contributed models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n: typing.Union[str, ForwardRef('Image.Image'), list['Image.Image'], list[str]]\n \n**kwargs\n: typing.Any\n \n \n)\n \n\u2192\n \nA nested list of \nfloat\n \n \nParameters \n \n \nimages\n (\nstr\n, \nlist[str]\n, \nPIL.Image\n or \nlist[PIL.Image]\n) \u2014\nThe pipeline handles three types of images:\n\n\n\n\nA string containing a http link pointing to an image\n\n\nA string containing a local path to an image\n\n\nAn image loaded in PIL directly\n\n\n\n\nThe pipeline accepts either a single image or a batch of images, which must then be passed as a string.\nImages in a batch must all be in the same format: all as http links, all as local paths, or all as PIL\nimages.\n \n \n \ntimeout\n (\nfloat\n, \noptional\n, defaults to None) \u2014\nThe maximum time in seconds to wait for fetching images from the web. If None, no timeout is used and\nthe call may block forever.\n \n \n \nReturns\n \n\n\nA nested list of \nfloat\n\n\n \n \n\n\nThe features computed by the model.\n\n\n \n \nExtract the features of the input(s).\n \n \nImageToTextPipeline\n \n \nclass\n \ntransformers.\nImageToTextPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nimage_processor\n (\nBaseImageProcessor\n) \u2014\nThe image processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nBaseImageProcessor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nImage To Text pipeline using a \nAutoModelForVision2Seq\n. This pipeline predicts a caption for a given image.\n \nUnless the model you\u2019re using explicitly sets these generation parameters in its configuration files\n(\ngeneration_config.json\n), the following default values will be used:\n \nmax_new_tokens: 256\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \ncaptioner = pipeline(model=\n\"ydshieh/vit-gpt2-coco-en\"\n)\n\n>>> \ncaptioner(\n\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n)\n[{\n'generated_text'\n: \n'two birds are standing next to each other '\n}]\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis image to text pipeline can currently be loaded from pipeline() using the following task identifier:\n\u201cimage-to-text\u201d.\n \nSee the list of available models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \ninputs\n: typing.Union[str, list[str], ForwardRef('Image.Image'), list['Image.Image']]\n \n**kwargs\n \n \n)\n \n\u2192\n \nA list or a list of list of \ndict\n \n \nParameters \n \n \ninputs\n (\nstr\n, \nlist[str]\n, \nPIL.Image\n or \nlist[PIL.Image]\n) \u2014\nThe pipeline handles three types of images:\n\n\n\n\nA string containing a HTTP(s) link pointing to an image\n\n\nA string containing a local path to an image\n\n\nAn image loaded in PIL directly\n\n\n\n\nThe pipeline accepts either a single image or a batch of images.\n \n \n \nmax_new_tokens\n (\nint\n, \noptional\n) \u2014\nThe amount of maximum tokens to generate. By default it will use \ngenerate\n default.\n \n \n \ngenerate_kwargs\n (\nDict\n, \noptional\n) \u2014\nPass it to send all of these arguments directly to \ngenerate\n allowing full control of this function.\n \n \n \ntimeout\n (\nfloat\n, \noptional\n, defaults to None) \u2014\nThe maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\nthe call may block forever.\n \n \n \nReturns\n \n\n\nA list or a list of list of \ndict\n\n\n \n \n\n\nEach result comes as a dictionary with the following key:\n\n\n\n\ngenerated_text\n (\nstr\n) \u2014 The generated text.\n\n\n\n\n \n \nAssign labels to the image(s) passed as inputs.\n \n \nImageTextToTextPipeline\n \n \nclass\n \ntransformers.\nImageTextToTextPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \nprocessor\n (\nProcessorMixin\n) \u2014\nThe processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nProcessorMixin\n. Processor is a composite object that might contain \ntokenizer\n, \nfeature_extractor\n, and\n\nimage_processor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nImage-text-to-text pipeline using an \nAutoModelForImageTextToText\n. This pipeline generates text given an image and text.\nWhen the underlying model is a conversational model, it can also accept one or more chats,\nin which case the pipeline will operate in chat mode and will continue the chat(s) by adding its response(s).\nEach chat takes the form of a list of dicts, where each dict contains \u201crole\u201d and \u201ccontent\u201d keys.\n \nUnless the model you\u2019re using explicitly sets these generation parameters in its configuration files\n(\ngeneration_config.json\n), the following default values will be used:\n \nmax_new_tokens: 256\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \npipe = pipeline(task=\n\"image-text-to-text\"\n, model=\n\"Salesforce/blip-image-captioning-base\"\n)\n\n>>> \npipe(\n\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n, text=\n\"A photo of\"\n)\n[{\n'generated_text'\n: \n'a photo of two birds'\n}]\n \n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \npipe = pipeline(\n\"image-text-to-text\"\n, model=\n\"llava-hf/llava-interleave-qwen-0.5b-hf\"\n)\n\n>>> \nmessages = [\n\n>>> \n    {\n\n>>> \n        \n\"role\"\n: \n\"user\"\n,\n\n>>> \n        \n\"content\"\n: [\n\n>>> \n            {\n\n>>> \n                \n\"type\"\n: \n\"image\"\n,\n\n>>> \n                \n\"url\"\n: \n\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n,\n\n>>> \n            },\n\n>>> \n            {\n\"type\"\n: \n\"text\"\n, \n\"text\"\n: \n\"Describe this image.\"\n},\n\n>>> \n        ],\n\n>>> \n    },\n\n>>> \n    {\n\n>>> \n        \n\"role\"\n: \n\"assistant\"\n,\n\n>>> \n        \n\"content\"\n: [\n\n>>> \n            {\n\"type\"\n: \n\"text\"\n, \n\"text\"\n: \n\"There is a dog and\"\n},\n\n>>> \n        ],\n\n>>> \n    },\n\n>>> \n]\n\n>>> \npipe(text=messages, max_new_tokens=\n20\n, return_full_text=\nFalse\n)\n[{\n'input_text'\n: [{\n'role'\n: \n'user'\n,\n    \n'content'\n: [{\n'type'\n: \n'image'\n,\n    \n'url'\n: \n'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\n},\n    {\n'type'\n: \n'text'\n, \n'text'\n: \n'Describe this image.'\n}]},\n{\n'role'\n: \n'assistant'\n,\n    \n'content'\n: [{\n'type'\n: \n'text'\n, \n'text'\n: \n'There is a dog and'\n}]}],\n\n'generated_text'\n: \n' a person in the image. The dog is sitting on the sand, and the person is sitting on'\n}]\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis image-text to text pipeline can currently be loaded from pipeline() using the following task identifier:\n\u201cimage-text-to-text\u201d.\n \nSee the list of available models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \nimages\n: typing.Union[str, list[str], list[list[str]], ForwardRef('Image.Image'), list['Image.Image'], list[list['Image.Image']], list[dict], NoneType] = None\n \ntext\n: typing.Union[str, list[str], list[dict], NoneType] = None\n \n**kwargs\n \n \n)\n \n\u2192\n \nA list or a list of list of \ndict\n \n \nParameters \n \n \nimages\n (\nstr\n, \nlist[str]\n, \nPIL.Image, \nlist[PIL.Image]\n, \nlist[dict[str, Union[str, PIL.Image]]]`) \u2014\nThe pipeline handles three types of images:\n\n\n\n\nA string containing a HTTP(s) link pointing to an image\n\n\nA string containing a local path to an image\n\n\nAn image loaded in PIL directly\n\n\n\n\nThe pipeline accepts either a single image or a batch of images. Finally, this pipeline also supports\nthe chat format (see \ntext\n) containing images and text in this argument.\n \n \n \ntext\n (str, list[str], \nlist[dict[str, Union[str, PIL.Image]]]\n) \u2014\nThe text to be used for generation. If a list of strings is passed, the length of the list should be\nthe same as the number of images. Text can also follow the chat format: a list of dictionaries where\neach dictionary represents a message in a conversation. Each dictionary should have two keys: \u2018role\u2019\nand \u2018content\u2019. \u2018role\u2019 should be one of \u2018user\u2019, \u2018system\u2019 or \u2018assistant\u2019. \u2018content\u2019 should be a list of\ndictionary containing the text of the message and the type of the message. The type of the message\ncan be either \u2018text\u2019 or \u2018image\u2019. If the type is \u2018image\u2019, no text is needed.\n \n \n \nreturn_tensors\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nReturns the tensors of predictions (as token indices) in the outputs. If set to\n\nTrue\n, the decoded text is not returned.\n \n \n \nreturn_text\n (\nbool\n, \noptional\n) \u2014\nReturns the decoded texts in the outputs.\n \n \n \nreturn_full_text\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nIf set to \nFalse\n only added text is returned, otherwise the full text is returned. Cannot be\nspecified at the same time as \nreturn_text\n.\n \n \n \nclean_up_tokenization_spaces\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether or not to clean up the potential extra spaces in the text output.\n \n \n \ncontinue_final_message(\n \nbool\n, \noptional\n) \u2014 This indicates that you want the model to continue the\nlast message in the input chat rather than starting a new one, allowing you to \u201cprefill\u201d its response.\nBy default this is \nTrue\n when the final message in the input chat has the \nassistant\n role and\n\nFalse\n otherwise, but you can manually override that behaviour by setting this flag.\n \n \n \nReturns\n \n\n\nA list or a list of list of \ndict\n\n\n \n \n\n\nEach result comes as a dictionary with the following key (cannot\nreturn a combination of both \ngenerated_text\n and \ngenerated_token_ids\n):\n\n\n\n\ngenerated_text\n (\nstr\n, present when \nreturn_text=True\n) \u2014 The generated text.\n\n\ngenerated_token_ids\n (\ntorch.Tensor\n, present when \nreturn_tensors=True\n) \u2014 The token\nids of the generated text.\n\n\ninput_text\n (\nstr\n) \u2014 The input text.\n\n\n\n\n \n \nGenerate a text given text and the image(s) passed as inputs.\n \n \nMaskGenerationPipeline\n \n \nclass\n \ntransformers.\nMaskGenerationPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \nimage_processor\n (\nBaseImageProcessor\n) \u2014\nThe image processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nBaseImageProcessor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \npoints_per_batch\n (\noptional\n, int, default to 64) \u2014\nSets the number of points run simultaneously by the model. Higher numbers may be faster but use more GPU\nmemory.\n \n \n \noutput_bboxes_mask\n (\nbool\n, \noptional\n, default to \nFalse\n) \u2014\nWhether or not to output the bounding box predictions.\n \n \n \noutput_rle_masks\n (\nbool\n, \noptional\n, default to \nFalse\n) \u2014\nWhether or not to output the masks in \nRLE\n format\n \n \n \n \nAutomatic mask generation for images using \nSamForMaskGeneration\n. This pipeline predicts binary masks for an\nimage, given an image. It is a \nChunkPipeline\n because you can separate the points in a mini-batch in order to\navoid OOM issues. Use the \npoints_per_batch\n argument to control the number of points that will be processed at the\nsame time. Default is \n64\n.\n \nThe pipeline works in 3 steps:\n \npreprocess\n: A grid of 1024 points evenly separated is generated along with bounding boxes and point\nlabels.\nFor more details on how the points and bounding boxes are created, check the \n_generate_crop_boxes\n\nfunction. The image is also preprocessed using the \nimage_processor\n. This function \nyields\n a minibatch of\n\npoints_per_batch\n.\n \nforward\n: feeds the outputs of \npreprocess\n to the model. The image embedding is computed only once.\nCalls both \nself.model.get_image_embeddings\n and makes sure that the gradients are not computed, and the\ntensors and models are on the same device.\n \npostprocess\n: The most important part of the automatic mask generation happens here. Three steps\nare induced:\n \nimage_processor.postprocess_masks (run on each minibatch loop): takes in the raw output masks,\nresizes them according\nto the image size, and transforms there to binary masks.\n \nimage_processor.filter_masks (on each minibatch loop): uses both \npred_iou_thresh\n and\n\nstability_scores\n. Also\napplies a variety of filters based on non maximum suppression to remove bad masks.\n \nimage_processor.postprocess_masks_for_amg applies the NSM on the mask to only keep relevant ones.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \ngenerator = pipeline(model=\n\"facebook/sam-vit-base\"\n, task=\n\"mask-generation\"\n)\n\n>>> \noutputs = generator(\n\n... \n    \n\"http://images.cocodataset.org/val2017/000000039769.jpg\"\n,\n\n... \n)\n\n\n>>> \noutputs = generator(\n\n... \n    \n\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n, points_per_batch=\n128\n\n\n... \n)\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis segmentation pipeline can currently be loaded from \npipeline()\n using the following task identifier:\n\n\"mask-generation\"\n.\n \nSee the list of available models on \nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \nimage\n: typing.Union[str, ForwardRef('Image.Image'), list[str], list['Image.Image']]\n \n*args\n: typing.Any\n \n**kwargs\n: typing.Any\n \n \n)\n \n\u2192\n \nDict\n \n \nParameters \n \n \nimage\n (\nstr\n, \nList[str]\n, \nPIL.Image\n or \nList[PIL.Image]\n) \u2014\nImage or list of images.\n \n \n \nmask_threshold\n (\nfloat\n, \noptional\n, defaults to 0.0) \u2014\nThreshold to use when turning the predicted masks into binary values.\n \n \n \npred_iou_thresh\n (\nfloat\n, \noptional\n, defaults to 0.88) \u2014\nA filtering threshold in \n[0,1]\n applied on the model\u2019s predicted mask quality.\n \n \n \nstability_score_thresh\n (\nfloat\n, \noptional\n, defaults to 0.95) \u2014\nA filtering threshold in \n[0,1]\n, using the stability of the mask under changes to the cutoff used to\nbinarize the model\u2019s mask predictions.\n \n \n \nstability_score_offset\n (\nint\n, \noptional\n, defaults to 1) \u2014\nThe amount to shift the cutoff when calculated the stability score.\n \n \n \ncrops_nms_thresh\n (\nfloat\n, \noptional\n, defaults to 0.7) \u2014\nThe box IoU cutoff used by non-maximal suppression to filter duplicate masks.\n \n \n \ncrops_n_layers\n (\nint\n, \noptional\n, defaults to 0) \u2014\nIf \ncrops_n_layers>0\n, mask prediction will be run again on crops of the image. Sets the number of\nlayers to run, where each layer has 2**i_layer number of image crops.\n \n \n \ncrop_overlap_ratio\n (\nfloat\n, \noptional\n, defaults to \n512 / 1500\n) \u2014\nSets the degree to which crops overlap. In the first crop layer, crops will overlap by this fraction of\nthe image length. Later layers with more crops scale down this overlap.\n \n \n \ncrop_n_points_downscale_factor\n (\nint\n, \noptional\n, defaults to \n1\n) \u2014\nThe number of points-per-side sampled in layer n is scaled down by crop_n_points_downscale_factor**n.\n \n \n \ntimeout\n (\nfloat\n, \noptional\n, defaults to None) \u2014\nThe maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\nthe call may block forever.\n \n \n \nReturns\n \n\n\nDict\n\n\n \n \n\n\nA dictionary with the following keys:\n\n\n\n\nmask\n (\nPIL.Image\n) \u2014 A binary mask of the detected object as a PIL Image of shape \n(width, height)\n of the original image. Returns a mask filled with zeros if no object is found.\n\n\nscore\n (\noptional\n \nfloat\n) \u2014 Optionally, when the model is capable of estimating a confidence of\nthe \u201cobject\u201d described by the label and the mask.\n\n\n\n\n \n \nGenerates binary segmentation masks\n \n \nVisualQuestionAnsweringPipeline\n \n \nclass\n \ntransformers.\nVisualQuestionAnsweringPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \n*args\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nimage_processor\n (\nBaseImageProcessor\n) \u2014\nThe image processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nBaseImageProcessor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nVisual Question Answering pipeline using a \nAutoModelForVisualQuestionAnswering\n. This pipeline is currently only\navailable in PyTorch.\n \nUnless the model you\u2019re using explicitly sets these generation parameters in its configuration files\n(\ngeneration_config.json\n), the following default values will be used:\n \nmax_new_tokens: 256\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n pipeline\n\n\n>>> \noracle = pipeline(model=\n\"dandelin/vilt-b32-finetuned-vqa\"\n)\n\n>>> \nimage_url = \n\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/lena.png\"\n\n\n>>> \noracle(question=\n\"What is she wearing ?\"\n, image=image_url)\n[{\n'score'\n: \n0.948\n, \n'answer'\n: \n'hat'\n}, {\n'score'\n: \n0.009\n, \n'answer'\n: \n'fedora'\n}, {\n'score'\n: \n0.003\n, \n'answer'\n: \n'clothes'\n}, {\n'score'\n: \n0.003\n, \n'answer'\n: \n'sun hat'\n}, {\n'score'\n: \n0.002\n, \n'answer'\n: \n'nothing'\n}]\n\n\n>>> \noracle(question=\n\"What is she wearing ?\"\n, image=image_url, top_k=\n1\n)\n[{\n'score'\n: \n0.948\n, \n'answer'\n: \n'hat'\n}]\n\n\n>>> \noracle(question=\n\"Is this a person ?\"\n, image=image_url, top_k=\n1\n)\n[{\n'score'\n: \n0.993\n, \n'answer'\n: \n'yes'\n}]\n\n\n>>> \noracle(question=\n\"Is this a man ?\"\n, image=image_url, top_k=\n1\n)\n[{\n'score'\n: \n0.996\n, \n'answer'\n: \n'no'\n}]\n \nLearn more about the basics of using a pipeline in the \npipeline tutorial\n \nThis visual question answering pipeline can currently be loaded from \npipeline()\n using the following task\nidentifiers: \n\"visual-question-answering\", \"vqa\"\n.\n \nThe models that this pipeline can use are models that have been fine-tuned on a visual question answering task. See\nthe up-to-date list of available models on\n\nhuggingface.co/models\n.\n \n \n__call__\n \n \n<\n \nsource\n \n>\n \n(\n \nimage\n: typing.Union[ForwardRef('Image.Image'), str, list['Image.Image'], list[str], ForwardRef('KeyDataset')]\n \nquestion\n: typing.Union[str, list[str], NoneType] = None\n \n**kwargs\n \n \n)\n \n\u2192\n \nA dictionary or a list of dictionaries containing the result. The dictionaries contain the following keys\n \n \nParameters \n \n \nimage\n (\nstr\n, \nlist[str]\n, \nPIL.Image\n, \nlist[PIL.Image]\n or \nKeyDataset\n) \u2014\nThe pipeline handles three types of images:\n\n\n\n\nA string containing a http link pointing to an image\n\n\nA string containing a local path to an image\n\n\nAn image loaded in PIL directly\n\n\n\n\nThe pipeline accepts either a single image or a batch of images. If given a single image, it can be\nbroadcasted to multiple questions.\nFor dataset: the passed in dataset must be of type \ntransformers.pipelines.pt_utils.KeyDataset\n\nExample:\n \n \n \nReturns\n \n\n\nA dictionary or a list of dictionaries containing the result. The dictionaries contain the following keys\n\n\n \n \n\n\n\n\nlabel\n (\nstr\n) \u2014 The label identified by the model.\n\n\nscore\n (\nint\n) \u2014 The score attributed by the model for that label.\n\n\n\n\n \n \nAnswers open-ended questions about images. The pipeline accepts several types of inputs which are detailed\nbelow:\n \npipeline(image=image, question=question)\n \npipeline({\"image\": image, \"question\": question})\n \npipeline([{\"image\": image, \"question\": question}])\n \npipeline([{\"image\": image, \"question\": question}, {\"image\": image, \"question\": question}])\n \n \nParent class: Pipeline\n \n \nclass\n \ntransformers.\nPipeline\n \n \n<\n \nsource\n \n>\n \n(\n \nmodel\n: typing.Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]\n \ntokenizer\n: typing.Optional[transformers.tokenization_utils.PreTrainedTokenizer] = None\n \nfeature_extractor\n: typing.Optional[ForwardRef('SequenceFeatureExtractor')] = None\n \nimage_processor\n: typing.Optional[transformers.image_processing_utils.BaseImageProcessor] = None\n \nprocessor\n: typing.Optional[transformers.processing_utils.ProcessorMixin] = None\n \nmodelcard\n: typing.Optional[transformers.modelcard.ModelCard] = None\n \nframework\n: typing.Optional[str] = None\n \ntask\n: str = ''\n \nargs_parser\n: ArgumentHandler = None\n \ndevice\n: typing.Union[int, ForwardRef('torch.device')] = None\n \ntorch_dtype\n: typing.Union[str, ForwardRef('torch.dtype'), NoneType] = None\n \nbinary_output\n: bool = False\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \nTFPreTrainedModel\n) \u2014\nThe model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n\nPreTrainedModel\n for PyTorch and \nTFPreTrainedModel\n for TensorFlow.\n \n \n \ntokenizer\n (\nPreTrainedTokenizer\n) \u2014\nThe tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n\nPreTrainedTokenizer\n.\n \n \n \nfeature_extractor\n (\nSequenceFeatureExtractor\n) \u2014\nThe feature extractor that will be used by the pipeline to encode data for the model. This object inherits from\n\nSequenceFeatureExtractor\n.\n \n \n \nimage_processor\n (\nBaseImageProcessor\n) \u2014\nThe image processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nBaseImageProcessor\n.\n \n \n \nprocessor\n (\nProcessorMixin\n) \u2014\nThe processor that will be used by the pipeline to encode data for the model. This object inherits from\n\nProcessorMixin\n. Processor is a composite object that might contain \ntokenizer\n, \nfeature_extractor\n, and\n\nimage_processor\n.\n \n \n \nmodelcard\n (\nstr\n or \nModelCard\n, \noptional\n) \u2014\nModel card attributed to the model for this pipeline.\n \n \n \nframework\n (\nstr\n, \noptional\n) \u2014\nThe framework to use, either \n\"pt\"\n for PyTorch or \n\"tf\"\n for TensorFlow. The specified framework must be\ninstalled.\n\n\nIf no framework is specified, will default to the one currently installed. If no framework is specified and\nboth frameworks are installed, will default to the framework of the \nmodel\n, or to PyTorch if no model is\nprovided.\n \n \n \ntask\n (\nstr\n, defaults to \n\"\"\n) \u2014\nA task-identifier for the pipeline.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 8) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the number of\nworkers to be used.\n \n \n \nbatch_size\n (\nint\n, \noptional\n, defaults to 1) \u2014\nWhen the pipeline will use \nDataLoader\n (when passing a dataset, on GPU for a Pytorch model), the size of\nthe batch to use, for inference this is not always beneficial, please read \nBatching with\npipelines\n .\n \n \n \nargs_parser\n (\nArgumentHandler\n, \noptional\n) \u2014\nReference to the object in charge of parsing supplied pipeline parameters.\n \n \n \ndevice\n (\nint\n, \noptional\n, defaults to -1) \u2014\nDevice ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\nthe associated CUDA device id. You can pass native \ntorch.device\n or a \nstr\n too\n \n \n \ntorch_dtype\n (\nstr\n or \ntorch.dtype\n, \noptional\n) \u2014\nSent directly as \nmodel_kwargs\n (just a simpler shortcut) to use the available precision for this model\n(\ntorch.float16\n, \ntorch.bfloat16\n, \u2026 or \n\"auto\"\n)\n \n \n \nbinary_output\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nFlag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\nthe raw output data e.g. text.\n \n \n \n \nThe Pipeline class is the class from which all pipelines inherit. Refer to this class for methods shared across\ndifferent pipelines.\n \nBase class implementing pipelined operations. Pipeline workflow is defined as a sequence of the following\noperations:\n \nInput -> Tokenization -> Model Inference -> Post-Processing (task dependent) -> Output\n \nPipeline supports running on CPU or GPU through the device argument (see below).\n \nSome pipeline, like for instance \nFeatureExtractionPipeline\n (\n'feature-extraction'\n) output large tensor object\nas nested-lists. In order to avoid dumping such large structure as textual data we provide the \nbinary_output\n\nconstructor argument. If set to \nTrue\n, the output will be stored in the pickle format.\n \n \ncheck_model_type\n \n \n<\n \nsource\n \n>\n \n(\n \nsupported_models\n: typing.Union[list[str], dict]\n \n \n)\n \n \n \nParameters \n \n \nsupported_models\n (\nlist[str]\n or \ndict\n) \u2014\nThe list of models supported by the pipeline, or a dictionary with model class values.\n \n \n \n \nCheck if the model class is in supported by the pipeline.\n \n \ndevice_placement\n \n \n<\n \nsource\n \n>\n \n(\n \n)\n \n \n \n \nContext Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n \n \nExamples:\n \n \n Copied\n \n# Explicitly ask for tensor allocation on CUDA device :0\n\npipe = pipeline(..., device=\n0\n)\n\nwith\n pipe.device_placement():\n    \n# Every framework specific tensor allocation will be done on the request device\n\n    output = pipe(...)\n \n \nensure_tensor_on_device\n \n \n<\n \nsource\n \n>\n \n(\n \n**inputs\n \n \n)\n \n\u2192\n \ndict[str, torch.Tensor]\n \n \nParameters \n \n \ninputs\n (keyword arguments that should be \ntorch.Tensor\n, the rest is ignored) \u2014\nThe tensors to place on \nself.device\n.\n \n \n \nRecursive\n on lists \nonly\n. \u2014\n \n \n \nReturns\n \n\n\ndict[str, torch.Tensor]\n\n\n \n \n\n\nThe same as \ninputs\n but on the proper device.\n\n\n \n \nEnsure PyTorch tensors are on the specified device.\n \n \npostprocess\n \n \n<\n \nsource\n \n>\n \n(\n \nmodel_outputs\n: ModelOutput\n \n**postprocess_parameters\n: dict\n \n \n)\n \n \n \n \nPostprocess will receive the raw outputs of the \n_forward\n method, generally tensors, and reformat them into\nsomething more friendly. Generally it will output a list or a dict or results (containing just strings and\nnumbers).\n \n \npredict\n \n \n<\n \nsource\n \n>\n \n(\n \nX\n \n \n)\n \n \n \n \nScikit / Keras interface to transformers\u2019 pipelines. This method will forward to \ncall\n().\n \n \npreprocess\n \n \n<\n \nsource\n \n>\n \n(\n \ninput_\n: typing.Any\n \n**preprocess_parameters\n: dict\n \n \n)\n \n \n \n \nPreprocess will take the \ninput_\n of a specific pipeline and return a dictionary of everything necessary for\n\n_forward\n to run properly. It should contain at least one tensor, but might have arbitrary other items.\n \n \npush_to_hub\n \n \n<\n \nsource\n \n>\n \n(\n \nrepo_id\n: str\n \nuse_temp_dir\n: typing.Optional[bool] = None\n \ncommit_message\n: typing.Optional[str] = None\n \nprivate\n: typing.Optional[bool] = None\n \ntoken\n: typing.Union[bool, str, NoneType] = None\n \nmax_shard_size\n: typing.Union[str, int, NoneType] = '5GB'\n \ncreate_pr\n: bool = False\n \nsafe_serialization\n: bool = True\n \nrevision\n: typing.Optional[str] = None\n \ncommit_description\n: typing.Optional[str] = None\n \ntags\n: typing.Optional[list[str]] = None\n \n**deprecated_kwargs\n \n \n)\n \n \n \nParameters \n \n \nrepo_id\n (\nstr\n) \u2014\nThe name of the repository you want to push your pipe to. It should contain your organization name\nwhen pushing to a given organization.\n \n \n \nuse_temp_dir\n (\nbool\n, \noptional\n) \u2014\nWhether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\nWill default to \nTrue\n if there is no directory named like \nrepo_id\n, \nFalse\n otherwise.\n \n \n \ncommit_message\n (\nstr\n, \noptional\n) \u2014\nMessage to commit while pushing. Will default to \n\"Upload pipe\"\n.\n \n \n \nprivate\n (\nbool\n, \noptional\n) \u2014\nWhether to make the repo private. If \nNone\n (default), the repo will be public unless the organization\u2019s default is private. This value is ignored if the repo already exists.\n \n \n \ntoken\n (\nbool\n or \nstr\n, \noptional\n) \u2014\nThe token to use as HTTP bearer authorization for remote files. If \nTrue\n, will use the token generated\nwhen running \nhuggingface-cli login\n (stored in \n~/.huggingface\n). Will default to \nTrue\n if \nrepo_url\n\nis not specified.\n \n \n \nmax_shard_size\n (\nint\n or \nstr\n, \noptional\n, defaults to \n\"5GB\"\n) \u2014\nOnly applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\nwill then be each of size lower than this size. If expressed as a string, needs to be digits followed\nby a unit (like \n\"5MB\"\n). We default it to \n\"5GB\"\n so that users can easily load models on free-tier\nGoogle Colab instances without any CPU OOM issues.\n \n \n \ncreate_pr\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to create a PR with the uploaded files or directly commit.\n \n \n \nsafe_serialization\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether or not to convert the model weights in safetensors format for safer serialization.\n \n \n \nrevision\n (\nstr\n, \noptional\n) \u2014\nBranch to push the uploaded files to.\n \n \n \ncommit_description\n (\nstr\n, \noptional\n) \u2014\nThe description of the commit that will be created\n \n \n \ntags\n (\nlist[str]\n, \noptional\n) \u2014\nList of tags to push on the Hub.\n \n \n \n \nUpload the pipeline file to the \ud83e\udd17 Model Hub.\n \n \nExamples:\n \n \n Copied\n \nfrom\n transformers \nimport\n pipeline\n\npipe = pipeline(\n\"google-bert/bert-base-cased\"\n)\n\n\n# Push the pipe to your namespace with the name \"my-finetuned-bert\".\n\npipe.push_to_hub(\n\"my-finetuned-bert\"\n)\n\n\n# Push the pipe to an organization with the name \"my-finetuned-bert\".\n\npipe.push_to_hub(\n\"huggingface/my-finetuned-bert\"\n)\n \n \nsave_pretrained\n \n \n<\n \nsource\n \n>\n \n(\n \nsave_directory\n: typing.Union[str, os.PathLike]\n \nsafe_serialization\n: bool = True\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nsave_directory\n (\nstr\n or \nos.PathLike\n) \u2014\nA path to the directory where to saved. It will be created if it doesn\u2019t exist.\n \n \n \nsafe_serialization\n (\nstr\n) \u2014\nWhether to save the model using \nsafetensors\n or the traditional way for PyTorch or Tensorflow.\n \n \n \nkwargs\n (\ndict[str, Any]\n, \noptional\n) \u2014\nAdditional key word arguments passed along to the \npush_to_hub()\n method.\n \n \n \n \nSave the pipeline\u2019s model and tokenizer.\n \n \ntransform\n \n \n<\n \nsource\n \n>\n \n(\n \nX\n \n \n)\n \n \n \n \nScikit / Keras interface to transformers\u2019 pipelines. This method will forward to \ncall\n().\n \n<\n \n>\n \nUpdate\n on GitHub\n \n\n\n\n\n\n\n\u2190\nPEFT\n\n\nProcessors\n\u2192\n\n\n\n\n\n\nPipelines\n\n\nThe pipeline abstraction\n\n\nPipeline batching\n\n\nPipeline chunk batching\n\n\nPipeline F\nP16 inference\n\n\nPipeline custom code\n\n\nImplementing a pipeline\n\n\nAudio\n\n\nAudio\nClassification\nPipeline\n\n\nAutomatic\nSpeech\nRecognition\nPipeline\n\n\nText\nTo\nAudio\nPipeline\n\n\nZero\nShot\nAudio\nClassification\nPipeline\n\n\nComputer vision\n\n\nDepth\nEstimation\nPipeline\n\n\nImage\nClassification\nPipeline\n\n\nImage\nSegmentation\nPipeline\n\n\nImage\nTo\nImage\nPipeline\n\n\nObject\nDetection\nPipeline\n\n\nVideo\nClassification\nPipeline\n\n\nZero\nShot\nImage\nClassification\nPipeline\n\n\nZero\nShot\nObject\nDetection\nPipeline\n\n\nNatural \nLanguage \nProcessing\n\n\nFill\nMask\nPipeline\n\n\nQuestion\nAnswering\nPipeline\n\n\nSummarization\nPipeline\n\n\nTable\nQuestion\nAnswering\nPipeline\n\n\nText\nClassification\nPipeline\n\n\nText\nGeneration\nPipeline\n\n\nText2\nText\nGeneration\nPipeline\n\n\nToken\nClassification\nPipeline\n\n\nTranslation\nPipeline\n\n\nZero\nShot\nClassification\nPipeline\n\n\nMultimodal\n\n\nDocument\nQuestion\nAnswering\nPipeline\n\n\nFeature\nExtraction\nPipeline\n\n\nImage\nFeature\nExtraction\nPipeline\n\n\nImage\nTo\nText\nPipeline\n\n\nImage\nText\nTo\nText\nPipeline\n\n\nMask\nGeneration\nPipeline\n\n\nVisual\nQuestion\nAnswering\nPipeline\n\n\nParent class: \nPipeline\n\n\n\n",
            "question": "How do I initialize the BERT tokenizer?",
            "answers": [
                {
                    "text": "BertTokenizer.from_pretrained('bert-base-uncased')",
                    "answer_start": -1
                }
            ]
        },
        {
            "context": "\n\n\n\nTransformers documentation\n\t\t\t\n\n\nQuickstart\n\n\n\n\n\n\n\n\nTransformers\n\n\n\n\n\ud83c\udfe1 View all docs\nAWS Trainium & Inferentia\nAccelerate\nArgilla\nAutoTrain\nBitsandbytes\nChat UI\nDataset viewer\nDatasets\nDeploying on AWS\nDiffusers\nDistilabel\nEvaluate\nGradio\nHub\nHub Python Library\nHuggingface.js\nInference Endpoints (dedicated)\nInference Providers\nLeRobot\nLeaderboards\nLighteval\nMicrosoft Azure\nOptimum\nPEFT\nSafetensors\nSentence Transformers\nTRL\nTasks\nText Embeddings Inference\nText Generation Inference\nTokenizers\nTransformers\nTransformers.js\nsmolagents\ntimm\n\n\n\n\nSearch documentation\n\n\n\n\n\n\nmain\nv4.53.2\nv4.52.3\nv4.51.3\nv4.50.0\nv4.49.0\nv4.48.2\nv4.47.1\nv4.46.3\nv4.45.2\nv4.44.2\nv4.43.4\nv4.42.4\nv4.41.2\nv4.40.2\nv4.39.3\nv4.38.2\nv4.37.2\nv4.36.1\nv4.35.2\nv4.34.1\nv4.33.3\nv4.32.1\nv4.31.0\nv4.30.0\nv4.29.1\nv4.28.1\nv4.27.2\nv4.26.1\nv4.25.1\nv4.24.0\nv4.23.1\nv4.22.2\nv4.21.3\nv4.20.1\nv4.19.4\nv4.18.0\nv4.17.0\nv4.16.2\nv4.15.0\nv4.14.1\nv4.13.0\nv4.12.5\nv4.11.3\nv4.10.1\nv4.9.2\nv4.8.2\nv4.7.0\nv4.6.0\nv4.5.1\nv4.4.2\nv4.3.3\nv4.2.2\nv4.1.1\nv4.0.1\nv3.5.1\nv3.4.0\nv3.3.1\nv3.2.0\nv3.1.0\nv3.0.2\nv2.11.0\nv2.10.0\nv2.9.1\nv2.8.0\nv2.7.0\nv2.6.0\nv2.5.1\nv2.4.1\nv2.3.0\nv2.2.2\nv2.1.1\nv2.0.0\nv1.2.0\nv1.1.0\nv1.0.0\ndoc-builder-html\n\n\nAR\nDE\nEN\nES\nFR\nHI\nIT\nJA\nKO\nPT\nTE\nTR\nZH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet started\n\n\n\n\n\n\nTransformers\n\n\nInstallation\n\n\nQuickstart\n\n\n\n\n\n\nBase classes\n\n\n\n\n\n\nInference\n\n\n\n\n\n\nTraining\n\n\n\n\n\n\nQuantization\n\n\n\n\n\n\nExport to production\n\n\n\n\n\n\nResources\n\n\n\n\n\n\nContribute\n\n\n\n\n\n\nAPI\n\n\n\n\n\n\n\n\n\n\n\n\nJoin the Hugging Face community\n\n\nand get access to the augmented documentation experience\n\t\t\n\n\n\n\nCollaborate on models, datasets and Spaces\n\t\t\t\t\n\n\n\n\nFaster examples with accelerated inference\n\t\t\t\t\n\n\n\n\nSwitch between documentation themes\n\t\t\t\t\n\n\nSign Up\n\n\nto get started\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \nQuickstart\n \n \n \n \n \n \n \n \n \n \n \n \nTransformers is designed to be fast and easy to use so that everyone can start learning or building with transformer models.\n \nThe number of user-facing abstractions is limited to only three classes for instantiating a model, and two APIs for inference or training. This quickstart introduces you to Transformers\u2019 key features and shows you how to:\n \nload a pretrained model\n \nrun inference with \nPipeline\n \nfine-tune a model with \nTrainer\n \n \nSet up\n \nTo start, we recommend creating a Hugging Face \naccount\n. An account lets you host and access version controlled models, datasets, and \nSpaces\n on the Hugging Face \nHub\n, a collaborative platform for discovery and building.\n \nCreate a \nUser Access Token\n and log in to your account.\n \nnotebook \nCLI \n \nPaste your User Access Token into \nnotebook_login\n when prompted to log in.\n \n \n Copied\n \nfrom\n huggingface_hub \nimport\n notebook_login\n\nnotebook_login()\n \n \nInstall a machine learning framework.\n \nPyTorch \nTensorFlow \n \n \n Copied\n \n!pip install torch\n \n \nThen install an up-to-date version of Transformers and some additional libraries from the Hugging Face ecosystem for accessing datasets and vision models, evaluating training, and optimizing training for large models.\n \n \n Copied\n \n!pip install -U transformers datasets evaluate accelerate timm\n \n \nPretrained models\n \nEach pretrained model inherits from three base classes.\n \nClass\n \nDescription\n \nPretrainedConfig\n \nA file that specifies a models attributes such as the number of attention heads or vocabulary size.\n \nPreTrainedModel\n \nA model (or architecture) defined by the model attributes from the configuration file. A pretrained model only returns the raw hidden states. For a specific task, use the appropriate model head to convert the raw hidden states into a meaningful result (for example, \nLlamaModel\n versus \nLlamaForCausalLM\n).\n \nPreprocessor\n \nA class for converting raw inputs (text, images, audio, multimodal) into numerical inputs to the model. For example, \nPreTrainedTokenizer\n converts text into tensors and \nImageProcessingMixin\n converts pixels into tensors.\n \nWe recommend using the \nAutoClass\n API to load models and preprocessors because it automatically infers the appropriate architecture for each task and machine learning framework based on the name or path to the pretrained weights and configuration file.\n \nUse \nfrom_pretrained()\n to load the weights and configuration file from the Hub into the model and preprocessor class.\n \nPyTorch \nTensorFlow \n \nWhen you load a model, configure the following parameters to ensure the model is optimally loaded.\n \ndevice_map=\"auto\"\n automatically allocates the model weights to your fastest device first, which is typically the GPU.\n \ntorch_dtype=\"auto\"\n directly initializes the model weights in the data type they\u2019re stored in, which can help avoid loading the weights twice (PyTorch loads weights in \ntorch.float32\n by default).\n \n \n Copied\n \nfrom\n transformers \nimport\n AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\"meta-llama/Llama-2-7b-hf\"\n, torch_dtype=\n\"auto\"\n, device_map=\n\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n\"meta-llama/Llama-2-7b-hf\"\n)\n \nTokenize the text and return PyTorch tensors with the tokenizer. Move the model to a GPU if it\u2019s available to accelerate inference.\n \n \n Copied\n \nmodel_inputs = tokenizer([\n\"The secret to baking a good cake is \"\n], return_tensors=\n\"pt\"\n).to(\n\"cuda\"\n)\n \nThe model is now ready for inference or training.\n \nFor inference, pass the tokenized inputs to \ngenerate()\n to generate text. Decode the token ids back into text with \nbatch_decode()\n.\n \n \n Copied\n \ngenerated_ids = model.generate(**model_inputs, max_length=\n30\n)\ntokenizer.batch_decode(generated_ids)[\n0\n]\n\n'<s> The secret to baking a good cake is 100% in the preparation. There are so many recipes out there,'\n \n \nSkip ahead to the \nTrainer\n section to learn how to fine-tune a model.\n \n \nPipeline\n \nThe \nPipeline\n class is the most convenient way to inference with a pretrained model. It supports many tasks such as text generation, image segmentation, automatic speech recognition, document question answering, and more.\n \nRefer to the \nPipeline\n API reference for a complete list of available tasks.\n \nCreate a \nPipeline\n object and select a task. By default, \nPipeline\n downloads and caches a default pretrained model for a given task. Pass the model name to the \nmodel\n parameter to choose a specific model.\n \ntext generation \nimage segmentation \nautomatic speech recognition \n \nSet \ndevice=\"cuda\"\n to accelerate inference with a GPU.\n \n \n Copied\n \nfrom\n transformers \nimport\n pipeline\n\npipeline = pipeline(\n\"text-generation\"\n, model=\n\"meta-llama/Llama-2-7b-hf\"\n, device=\n\"cuda\"\n)\n \nPrompt \nPipeline\n with some initial text to generate more text.\n \n \n Copied\n \npipeline(\n\"The secret to baking a good cake is \"\n, max_length=\n50\n)\n[{\n'generated_text'\n: \n'The secret to baking a good cake is 100% in the batter. The secret to a great cake is the icing.\\nThis is why we\u2019ve created the best buttercream frosting reci'\n}]\n \n \n \nTrainer\n \nTrainer\n is a complete training and evaluation loop for PyTorch models. It abstracts away a lot of the boilerplate usually involved in manually writing a training loop, so you can start training faster and focus on training design choices. You only need a model, dataset, a preprocessor, and a data collator to build batches of data from the dataset.\n \nUse the \nTrainingArguments\n class to customize the training process. It provides many options for training, evaluation, and more. Experiment with training hyperparameters and features like batch size, learning rate, mixed precision, torch.compile, and more to meet your training needs. You could also use the default training parameters to quickly produce a baseline.\n \nLoad a model, tokenizer, and dataset for training.\n \n \n Copied\n \nfrom\n transformers \nimport\n AutoModelForSequenceClassification, AutoTokenizer\n\nfrom\n datasets \nimport\n load_dataset\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n\"distilbert/distilbert-base-uncased\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n\"distilbert/distilbert-base-uncased\"\n)\ndataset = load_dataset(\n\"rotten_tomatoes\"\n)\n \nCreate a function to tokenize the text and convert it into PyTorch tensors. Apply this function to the whole dataset with the \nmap\n method.\n \n \n Copied\n \ndef\n \ntokenize_dataset\n(\ndataset\n):\n    \nreturn\n tokenizer(dataset[\n\"text\"\n])\ndataset = dataset.\nmap\n(tokenize_dataset, batched=\nTrue\n)\n \nLoad a data collator to create batches of data and pass the tokenizer to it.\n \n \n Copied\n \nfrom\n transformers \nimport\n DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n \nNext, set up \nTrainingArguments\n with the training features and hyperparameters.\n \n \n Copied\n \nfrom\n transformers \nimport\n TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\n\"distilbert-rotten-tomatoes\"\n,\n    learning_rate=\n2e-5\n,\n    per_device_train_batch_size=\n8\n,\n    per_device_eval_batch_size=\n8\n,\n    num_train_epochs=\n2\n,\n    push_to_hub=\nTrue\n,\n)\n \nFinally, pass all these separate components to \nTrainer\n and call \ntrain()\n to start.\n \n \n Copied\n \nfrom\n transformers \nimport\n Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\n\"train\"\n],\n    eval_dataset=dataset[\n\"test\"\n],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\ntrainer.train()\n \nShare your model and tokenizer to the Hub with \npush_to_hub()\n.\n \n \n Copied\n \ntrainer.push_to_hub()\n \nCongratulations, you just trained your first model with Transformers!\n \n \nTensorFlow\n \nNot all pretrained models are available in TensorFlow. Refer to a models API doc to check whether a TensorFlow implementation is supported.\n \nTrainer\n doesn\u2019t work with TensorFlow models, but you can still train a Transformers model implemented in TensorFlow with \nKeras\n. Transformers TensorFlow models are a standard \ntf.keras.Model\n, which is compatible with Keras\u2019 \ncompile\n and \nfit\n methods.\n \nLoad a model, tokenizer, and dataset for training.\n \n \n Copied\n \nfrom\n transformers \nimport\n TFAutoModelForSequenceClassification, AutoTokenizer\n\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\n\"distilbert/distilbert-base-uncased\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n\"distilbert/distilbert-base-uncased\"\n)\n \nCreate a function to tokenize the text and convert it into TensorFlow tensors. Apply this function to the whole dataset with the \nmap\n method.\n \n \n Copied\n \ndef\n \ntokenize_dataset\n(\ndataset\n):\n    \nreturn\n tokenizer(dataset[\n\"text\"\n])\ndataset = dataset.\nmap\n(tokenize_dataset)\n \nTransformers provides the \nprepare_tf_dataset()\n method to collate and batch a dataset.\n \n \n Copied\n \ntf_dataset = model.prepare_tf_dataset(\n    dataset[\n\"train\"\n], batch_size=\n16\n, shuffle=\nTrue\n, tokenizer=tokenizer\n)\n \nFinally, call \ncompile\n to configure the model for training and \nfit\n to start.\n \n \n Copied\n \nfrom\n tensorflow.keras.optimizers \nimport\n Adam\n\nmodel.\ncompile\n(optimizer=\n\"adam\"\n)\nmodel.fit(tf_dataset)\n \n \nNext steps\n \nNow that you have a better understanding of Transformers and what it offers, it\u2019s time to keep exploring and learning what interests you the most.\n \nBase classes\n: Learn more about the configuration, model and processor classes. This will help you understand how to create and customize models, preprocess different types of inputs (audio, images, multimodal), and how to share your model.\n \nInference\n: Explore the \nPipeline\n further, inference and chatting with LLMs, agents, and how to optimize inference with your machine learning framework and hardware.\n \nTraining\n: Study the \nTrainer\n in more detail, as well as distributed training and optimizing training on specific hardware.\n \nQuantization\n: Reduce memory and storage requirements with quantization and speed up inference by representing weights with fewer bits.\n \nResources\n: Looking for end-to-end recipes for how to train and inference with a model for a specific task? Check out the task recipes!\n \n<\n \n>\n \nUpdate\n on GitHub\n \n\n\n\n\n\n\n\u2190\nInstallation\n\n\nLoading models\n\u2192\n\n\n\n\n\n\nQuickstart\n\n\nSet up\n\n\nPretrained models\n\n\nPipeline\n\n\nTrainer\n\n\nTensor\nFlow\n\n\nNext steps\n\n\n\n",
            "question": "How do I initialize the BERT tokenizer?",
            "answers": [
                {
                    "text": "BertTokenizer.from_pretrained('bert-base-uncased')",
                    "answer_start": -1
                }
            ]
        },
        {
            "context": "\n\n\n\nTransformers documentation\n\t\t\t\n\n\nTrainer\n\n\n\n\n\n\n\n\nTransformers\n\n\n\n\n\ud83c\udfe1 View all docs\nAWS Trainium & Inferentia\nAccelerate\nArgilla\nAutoTrain\nBitsandbytes\nChat UI\nDataset viewer\nDatasets\nDeploying on AWS\nDiffusers\nDistilabel\nEvaluate\nGradio\nHub\nHub Python Library\nHuggingface.js\nInference Endpoints (dedicated)\nInference Providers\nLeRobot\nLeaderboards\nLighteval\nMicrosoft Azure\nOptimum\nPEFT\nSafetensors\nSentence Transformers\nTRL\nTasks\nText Embeddings Inference\nText Generation Inference\nTokenizers\nTransformers\nTransformers.js\nsmolagents\ntimm\n\n\n\n\nSearch documentation\n\n\n\n\n\n\nmain\nv4.53.2\nv4.52.3\nv4.51.3\nv4.50.0\nv4.49.0\nv4.48.2\nv4.47.1\nv4.46.3\nv4.45.2\nv4.44.2\nv4.43.4\nv4.42.4\nv4.41.2\nv4.40.2\nv4.39.3\nv4.38.2\nv4.37.2\nv4.36.1\nv4.35.2\nv4.34.1\nv4.33.3\nv4.32.1\nv4.31.0\nv4.30.0\nv4.29.1\nv4.28.1\nv4.27.2\nv4.26.1\nv4.25.1\nv4.24.0\nv4.23.1\nv4.22.2\nv4.21.3\nv4.20.1\nv4.19.4\nv4.18.0\nv4.17.0\nv4.16.2\nv4.15.0\nv4.14.1\nv4.13.0\nv4.12.5\nv4.11.3\nv4.10.1\nv4.9.2\nv4.8.2\nv4.7.0\nv4.6.0\nv4.5.1\nv4.4.2\nv4.3.3\nv4.2.2\nv4.1.1\nv4.0.1\nv3.5.1\nv3.4.0\nv3.3.1\nv3.2.0\nv3.1.0\nv3.0.2\nv2.11.0\nv2.10.0\nv2.9.1\nv2.8.0\nv2.7.0\nv2.6.0\nv2.5.1\nv2.4.1\nv2.3.0\nv2.2.2\nv2.1.1\nv2.0.0\nv1.2.0\nv1.1.0\nv1.0.0\ndoc-builder-html\n\n\nAR\nDE\nEN\nES\nFR\nHI\nIT\nJA\nKO\nPT\nTE\nTR\nZH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet started\n\n\n\n\n\n\nTransformers\n\n\nInstallation\n\n\nQuickstart\n\n\n\n\n\n\nBase classes\n\n\n\n\n\n\nInference\n\n\n\n\n\n\nTraining\n\n\n\n\n\n\nQuantization\n\n\n\n\n\n\nExport to production\n\n\n\n\n\n\nResources\n\n\n\n\n\n\nContribute\n\n\n\n\n\n\nAPI\n\n\n\n\n\n\n\n\nMain Classes\n\n\n\n\n\n\nAuto Classes\n\n\nBackbones\n\n\nCallbacks\n\n\nConfiguration\n\n\nData Collator\n\n\nKeras callbacks\n\n\nLogging\n\n\nModels\n\n\nText Generation\n\n\nONNX\n\n\nOptimization\n\n\nModel outputs\n\n\nPEFT\n\n\nPipelines\n\n\nProcessors\n\n\nQuantization\n\n\nTokenizer\n\n\nTrainer\n\n\nDeepSpeed\n\n\nExecuTorch\n\n\nFeature Extractor\n\n\nImage Processor\n\n\nVideo Processor\n\n\n\n\n\n\nModels\n\n\n\n\n\n\nInternal helpers\n\n\n\n\n\n\nReference\n\n\n\n\n\n\n\n\n\n\n\n\nJoin the Hugging Face community\n\n\nand get access to the augmented documentation experience\n\t\t\n\n\n\n\nCollaborate on models, datasets and Spaces\n\t\t\t\t\n\n\n\n\nFaster examples with accelerated inference\n\t\t\t\t\n\n\n\n\nSwitch between documentation themes\n\t\t\t\t\n\n\nSign Up\n\n\nto get started\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \nTrainer\n \nThe \nTrainer\n class provides an API for feature-complete training in PyTorch, and it supports distributed training on multiple GPUs/TPUs, mixed precision for \nNVIDIA GPUs\n, \nAMD GPUs\n, and \ntorch.amp\n for PyTorch. \nTrainer\n goes hand-in-hand with the \nTrainingArguments\n class, which offers a wide range of options to customize how a model is trained. Together, these two classes provide a complete training API.\n \nSeq2SeqTrainer\n and \nSeq2SeqTrainingArguments\n inherit from the \nTrainer\n and \nTrainingArguments\n classes and they\u2019re adapted for training models for sequence-to-sequence tasks such as summarization or translation.\n \nThe \nTrainer\n class is optimized for \ud83e\udd17 Transformers models and can have surprising behaviors\nwhen used with other models. When using it with your own model, make sure:\n \nyour model always return tuples or subclasses of \nModelOutput\n \nyour model can compute the loss if a \nlabels\n argument is provided and that loss is returned as the first\nelement of the tuple (if your model returns tuples)\n \nyour model can accept multiple label arguments (use \nlabel_names\n in \nTrainingArguments\n to indicate their name to the \nTrainer\n) but none of them should be named \n\"label\"\n \n \nTrainer\n \n \nclass\n \ntransformers.\nTrainer\n \n \n<\n \nsource\n \n>\n \n(\n \nmodel\n: typing.Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module, NoneType] = None\n \nargs\n: TrainingArguments = None\n \ndata_collator\n: typing.Optional[transformers.data.data_collator.DataCollator] = None\n \ntrain_dataset\n: typing.Union[torch.utils.data.dataset.Dataset, torch.utils.data.dataset.IterableDataset, ForwardRef('datasets.Dataset'), NoneType] = None\n \neval_dataset\n: typing.Union[torch.utils.data.dataset.Dataset, dict[str, torch.utils.data.dataset.Dataset], ForwardRef('datasets.Dataset'), NoneType] = None\n \nprocessing_class\n: typing.Union[transformers.tokenization_utils_base.PreTrainedTokenizerBase, transformers.image_processing_utils.BaseImageProcessor, transformers.feature_extraction_utils.FeatureExtractionMixin, transformers.processing_utils.ProcessorMixin, NoneType] = None\n \nmodel_init\n: typing.Optional[typing.Callable[[], transformers.modeling_utils.PreTrainedModel]] = None\n \ncompute_loss_func\n: typing.Optional[typing.Callable] = None\n \ncompute_metrics\n: typing.Optional[typing.Callable[[transformers.trainer_utils.EvalPrediction], dict]] = None\n \ncallbacks\n: typing.Optional[list[transformers.trainer_callback.TrainerCallback]] = None\n \noptimizers\n: tuple = (None, None)\n \noptimizer_cls_and_kwargs\n: typing.Optional[tuple[type[torch.optim.optimizer.Optimizer], dict[str, typing.Any]]] = None\n \npreprocess_logits_for_metrics\n: typing.Optional[typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nPreTrainedModel\n or \ntorch.nn.Module\n, \noptional\n) \u2014\nThe model to train, evaluate or use for predictions. If not provided, a \nmodel_init\n must be passed.\n\n\n\n\nTrainer\n is optimized to work with the \nPreTrainedModel\n provided by the library. You can still use\nyour own models defined as \ntorch.nn.Module\n as long as they work the same way as the \ud83e\udd17 Transformers\nmodels.\n\n\n \n \n \nargs\n (\nTrainingArguments\n, \noptional\n) \u2014\nThe arguments to tweak for training. Will default to a basic instance of \nTrainingArguments\n with the\n\noutput_dir\n set to a directory named \ntmp_trainer\n in the current directory if not provided.\n \n \n \ndata_collator\n (\nDataCollator\n, \noptional\n) \u2014\nThe function to use to form a batch from a list of elements of \ntrain_dataset\n or \neval_dataset\n. Will\ndefault to \ndefault_data_collator()\n if no \nprocessing_class\n is provided, an instance of\n\nDataCollatorWithPadding\n otherwise if the processing_class is a feature extractor or tokenizer.\n \n \n \ntrain_dataset\n (Union[\ntorch.utils.data.Dataset\n, \ntorch.utils.data.IterableDataset\n, \ndatasets.Dataset\n], \noptional\n) \u2014\nThe dataset to use for training. If it is a \nDataset\n, columns not accepted by the\n\nmodel.forward()\n method are automatically removed.\n\n\nNote that if it\u2019s a \ntorch.utils.data.IterableDataset\n with some randomization and you are training in a\ndistributed fashion, your iterable dataset should either use a internal attribute \ngenerator\n that is a\n\ntorch.Generator\n for the randomization that must be identical on all processes (and the Trainer will\nmanually set the seed of this \ngenerator\n at each epoch) or have a \nset_epoch()\n method that internally\nsets the seed of the RNGs used.\n \n \n \neval_dataset\n (Union[\ntorch.utils.data.Dataset\n, dict[str, \ntorch.utils.data.Dataset\n, \ndatasets.Dataset\n]), \noptional\n) \u2014\nThe dataset to use for evaluation. If it is a \nDataset\n, columns not accepted by the\n\nmodel.forward()\n method are automatically removed. If it is a dictionary, it will evaluate on each\ndataset prepending the dictionary key to the metric name.\n \n \n \nprocessing_class\n (\nPreTrainedTokenizerBase\n or \nBaseImageProcessor\n or \nFeatureExtractionMixin\n or \nProcessorMixin\n, \noptional\n) \u2014\nProcessing class used to process the data. If provided, will be used to automatically process the inputs\nfor the model, and it will be saved along the model to make it easier to rerun an interrupted training or\nreuse the fine-tuned model.\nThis supersedes the \ntokenizer\n argument, which is now deprecated.\n \n \n \nmodel_init\n (\nCallable[[], PreTrainedModel]\n, \noptional\n) \u2014\nA function that instantiates the model to be used. If provided, each call to \ntrain()\n will start\nfrom a new instance of the model as given by this function.\n\n\nThe function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to\nbe able to choose different architectures according to hyper parameters (such as layer count, sizes of\ninner layers, dropout probabilities etc).\n \n \n \ncompute_loss_func\n (\nCallable\n, \noptional\n) \u2014\nA function that accepts the raw model outputs, labels, and the number of items in the entire accumulated\nbatch (batch_size * gradient_accumulation_steps) and returns the loss. For example, see the default \nloss function\n used by \nTrainer\n.\n \n \n \ncompute_metrics\n (\nCallable[[EvalPrediction], Dict]\n, \noptional\n) \u2014\nThe function that will be used to compute metrics at evaluation. Must take a \nEvalPrediction\n and return\na dictionary string to metric values. \nNote\n When passing TrainingArgs with \nbatch_eval_metrics\n set to\n\nTrue\n, your compute_metrics function must take a boolean \ncompute_result\n argument. This will be triggered\nafter the last eval batch to signal that the function needs to calculate and return the global summary\nstatistics rather than accumulating the batch-level statistics\n \n \n \ncallbacks\n (List of \nTrainerCallback\n, \noptional\n) \u2014\nA list of callbacks to customize the training loop. Will add those to the list of default callbacks\ndetailed in \nhere\n.\n\n\nIf you want to remove one of the default callbacks used, use the \nTrainer.remove_callback()\n method.\n \n \n \noptimizers\n (\ntuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]\n, \noptional\n, defaults to \n(None, None)\n) \u2014\nA tuple containing the optimizer and the scheduler to use. Will default to an instance of \nAdamW\n on your\nmodel and a scheduler given by \nget_linear_schedule_with_warmup()\n controlled by \nargs\n.\n \n \n \noptimizer_cls_and_kwargs\n (\ntuple[Type[torch.optim.Optimizer], dict[str, Any]]\n, \noptional\n) \u2014\nA tuple containing the optimizer class and keyword arguments to use.\nOverrides \noptim\n and \noptim_args\n in \nargs\n. Incompatible with the \noptimizers\n argument.\n\n\nUnlike \noptimizers\n, this argument avoids the need to place model parameters on the correct devices before initializing the Trainer.\n \n \n \npreprocess_logits_for_metrics\n (\nCallable[[torch.Tensor, torch.Tensor], torch.Tensor]\n, \noptional\n) \u2014\nA function that preprocess the logits right before caching them at each evaluation step. Must take two\ntensors, the logits and the labels, and return the logits once processed as desired. The modifications made\nby this function will be reflected in the predictions received by \ncompute_metrics\n.\n\n\nNote that the labels (second parameter) will be \nNone\n if the dataset does not have them.\n \n \n \n \nTrainer is a simple but feature-complete training and eval loop for PyTorch, optimized for \ud83e\udd17 Transformers.\n \nImportant attributes:\n \nmodel\n \u2014 Always points to the core model. If using a transformers model, it will be a \nPreTrainedModel\n\nsubclass.\n \nmodel_wrapped\n \u2014 Always points to the most external model in case one or more other modules wrap the\noriginal model. This is the model that should be used for the forward pass. For example, under \nDeepSpeed\n,\nthe inner model is wrapped in \nDeepSpeed\n and then again in \ntorch.nn.DistributedDataParallel\n. If the inner\nmodel hasn\u2019t been wrapped, then \nself.model_wrapped\n is the same as \nself.model\n.\n \nis_model_parallel\n \u2014 Whether or not a model has been switched to a model parallel mode (different from\ndata parallelism, this means some of the model layers are split on different GPUs).\n \nplace_model_on_device\n \u2014 Whether or not to automatically place the model on the device - it will be set\nto \nFalse\n if model parallel or deepspeed is used, or if the default\n\nTrainingArguments.place_model_on_device\n is overridden to return \nFalse\n .\n \nis_in_train\n \u2014 Whether or not a model is currently running \ntrain\n (e.g. when \nevaluate\n is called while\nin \ntrain\n)\n \n \nadd_callback\n \n \n<\n \nsource\n \n>\n \n(\n \ncallback\n \n \n)\n \n \n \nParameters \n \n \ncallback\n (\ntype\n or [`~transformers.TrainerCallback]`) \u2014\nA \nTrainerCallback\n class or an instance of a \nTrainerCallback\n. In the\nfirst case, will instantiate a member of that class.\n \n \n \n \nAdd a callback to the current list of \nTrainerCallback\n.\n \n \nautocast_smart_context_manager\n \n \n<\n \nsource\n \n>\n \n(\n \ncache_enabled\n: typing.Optional[bool] = True\n \n \n)\n \n \n \n \nA helper wrapper that creates an appropriate context manager for \nautocast\n while feeding it the desired\narguments, depending on the situation.\n \n \ncompute_loss\n \n \n<\n \nsource\n \n>\n \n(\n \nmodel\n: Module\n \ninputs\n: dict\n \nreturn_outputs\n: bool = False\n \nnum_items_in_batch\n: typing.Optional[torch.Tensor] = None\n \n \n)\n \n \n \nParameters \n \n \nmodel\n (\nnn.Module\n) \u2014\nThe model to compute the loss for.\n \n \n \ninputs\n (\ndict[str, Union[torch.Tensor, Any]]\n) \u2014\nThe input data for the model.\n \n \n \nreturn_outputs\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to return the model outputs along with the loss.\n \n \n \nnum_items_in_batch\n (Optional[torch.Tensor], \noptional\n) \u2014\nThe number of items in the batch. If num_items_in_batch is not passed,\n \n \n \n \nHow the loss is computed by Trainer. By default, all models return the loss in the first element.\n \nSubclass and override for custom behavior. If you are not using \nnum_items_in_batch\n when computing your loss,\nmake sure to overwrite \nself.model_accepts_loss_kwargs\n to \nFalse\n. Otherwise, the loss calculationg might be slightly inacurate when performing gradient accumulation.\n \n \ncompute_loss_context_manager\n \n \n<\n \nsource\n \n>\n \n(\n \n)\n \n \n \n \nA helper wrapper to group together context managers.\n \n \ncreate_model_card\n \n \n<\n \nsource\n \n>\n \n(\n \nlanguage\n: typing.Optional[str] = None\n \nlicense\n: typing.Optional[str] = None\n \ntags\n: typing.Union[str, list[str], NoneType] = None\n \nmodel_name\n: typing.Optional[str] = None\n \nfinetuned_from\n: typing.Optional[str] = None\n \ntasks\n: typing.Union[str, list[str], NoneType] = None\n \ndataset_tags\n: typing.Union[str, list[str], NoneType] = None\n \ndataset\n: typing.Union[str, list[str], NoneType] = None\n \ndataset_args\n: typing.Union[str, list[str], NoneType] = None\n \n \n)\n \n \n \nParameters \n \n \nlanguage\n (\nstr\n, \noptional\n) \u2014\nThe language of the model (if applicable)\n \n \n \nlicense\n (\nstr\n, \noptional\n) \u2014\nThe license of the model. Will default to the license of the pretrained model used, if the original\nmodel given to the \nTrainer\n comes from a repo on the Hub.\n \n \n \ntags\n (\nstr\n or \nlist[str]\n, \noptional\n) \u2014\nSome tags to be included in the metadata of the model card.\n \n \n \nmodel_name\n (\nstr\n, \noptional\n) \u2014\nThe name of the model.\n \n \n \nfinetuned_from\n (\nstr\n, \noptional\n) \u2014\nThe name of the model used to fine-tune this one (if applicable). Will default to the name of the repo\nof the original model given to the \nTrainer\n (if it comes from the Hub).\n \n \n \ntasks\n (\nstr\n or \nlist[str]\n, \noptional\n) \u2014\nOne or several task identifiers, to be included in the metadata of the model card.\n \n \n \ndataset_tags\n (\nstr\n or \nlist[str]\n, \noptional\n) \u2014\nOne or several dataset tags, to be included in the metadata of the model card.\n \n \n \ndataset\n (\nstr\n or \nlist[str]\n, \noptional\n) \u2014\nOne or several dataset identifiers, to be included in the metadata of the model card.\n \n \n \ndataset_args\n (\nstr\n or \nlist[str]\n, \noptional\n) \u2014\nOne or several dataset arguments, to be included in the metadata of the model card.\n \n \n \n \nCreates a draft of a model card using the information available to the \nTrainer\n.\n \n \ncreate_optimizer\n \n \n<\n \nsource\n \n>\n \n(\n \n)\n \n \n \n \nSetup the optimizer.\n \nWe provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\nTrainer\u2019s init through \noptimizers\n, or subclass and override this method in a subclass.\n \n \ncreate_optimizer_and_scheduler\n \n \n<\n \nsource\n \n>\n \n(\n \nnum_training_steps\n: int\n \n \n)\n \n \n \n \nSetup the optimizer and the learning rate scheduler.\n \nWe provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\nTrainer\u2019s init through \noptimizers\n, or subclass and override this method (or \ncreate_optimizer\n and/or\n\ncreate_scheduler\n) in a subclass.\n \n \ncreate_scheduler\n \n \n<\n \nsource\n \n>\n \n(\n \nnum_training_steps\n: int\n \noptimizer\n: Optimizer = None\n \n \n)\n \n \n \nParameters \n \n \nnum_training_steps\n (int) \u2014 The number of training steps to do.\n \n \n \n \nSetup the scheduler. The optimizer of the trainer must have been set up either before this method is called or\npassed as an argument.\n \n \nevaluate\n \n \n<\n \nsource\n \n>\n \n(\n \neval_dataset\n: typing.Union[torch.utils.data.dataset.Dataset, dict[str, torch.utils.data.dataset.Dataset], NoneType] = None\n \nignore_keys\n: typing.Optional[list[str]] = None\n \nmetric_key_prefix\n: str = 'eval'\n \n \n)\n \n \n \nParameters \n \n \neval_dataset\n (Union[\nDataset\n, dict[str, \nDataset\n]), \noptional\n) \u2014\nPass a dataset if you wish to override \nself.eval_dataset\n. If it is a \nDataset\n, columns\nnot accepted by the \nmodel.forward()\n method are automatically removed. If it is a dictionary, it will\nevaluate on each dataset, prepending the dictionary key to the metric name. Datasets must implement the\n\n__len__\n method.\n\n\n\n\nIf you pass a dictionary with names of datasets as keys and datasets as values, evaluate will run\nseparate evaluations on each dataset. This can be useful to monitor how training affects other\ndatasets or simply to get a more fine-grained evaluation.\nWhen used with \nload_best_model_at_end\n, make sure \nmetric_for_best_model\n references exactly one\nof the datasets. If you, for example, pass in \n{\"data1\": data1, \"data2\": data2}\n for two datasets\n\ndata1\n and \ndata2\n, you could specify \nmetric_for_best_model=\"eval_data1_loss\"\n for using the\nloss on \ndata1\n and \nmetric_for_best_model=\"eval_data2_loss\"\n for the loss on \ndata2\n.\n\n\n \n \n \nignore_keys\n (\nlist[str]\n, \noptional\n) \u2014\nA list of keys in the output of your model (if it is a dictionary) that should be ignored when\ngathering predictions.\n \n \n \nmetric_key_prefix\n (\nstr\n, \noptional\n, defaults to \n\"eval\"\n) \u2014\nAn optional prefix to be used as the metrics key prefix. For example the metrics \u201cbleu\u201d will be named\n\u201ceval_bleu\u201d if the prefix is \u201ceval\u201d (default)\n \n \n \n \nRun evaluation and returns metrics.\n \nThe calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n(pass it to the init \ncompute_metrics\n argument).\n \nYou can also subclass and override this method to inject custom behavior.\n \n \nevaluation_loop\n \n \n<\n \nsource\n \n>\n \n(\n \ndataloader\n: DataLoader\n \ndescription\n: str\n \nprediction_loss_only\n: typing.Optional[bool] = None\n \nignore_keys\n: typing.Optional[list[str]] = None\n \nmetric_key_prefix\n: str = 'eval'\n \n \n)\n \n \n \n \nPrediction/evaluation loop, shared by \nTrainer.evaluate()\n and \nTrainer.predict()\n.\n \nWorks both with or without labels.\n \n \nfloating_point_ops\n \n \n<\n \nsource\n \n>\n \n(\n \ninputs\n: dict\n \n \n)\n \n\u2192\n \nint\n \n \nParameters \n \n \ninputs\n (\ndict[str, Union[torch.Tensor, Any]]\n) \u2014\nThe inputs and targets of the model.\n \n \n \nReturns\n \n\n\nint\n\n\n \n \n\n\nThe number of floating-point operations.\n\n\n \n \nFor models that inherit from \nPreTrainedModel\n, uses that method to compute the number of floating point\noperations for every backward + forward pass. If using another model, either implement such a method in the\nmodel or subclass and override this method.\n \n \nget_batch_samples\n \n \n<\n \nsource\n \n>\n \n(\n \nepoch_iterator\n: Iterator\n \nnum_batches\n: int\n \ndevice\n: device\n \n \n)\n \n \n \n \nCollects a specified number of batches from the epoch iterator and optionally counts the number of items in the batches to properly scale the loss.\n \n \nget_decay_parameter_names\n \n \n<\n \nsource\n \n>\n \n(\n \nmodel\n \n \n)\n \n \n \n \nGet all parameter names that weight decay will be applied to.\n \nThis function filters out parameters in two ways:\n \nBy layer type (instances of layers specified in ALL_LAYERNORM_LAYERS)\n \nBy parameter name patterns (containing \u2018bias\u2019, or variation of \u2018norm\u2019)\n \n \nget_eval_dataloader\n \n \n<\n \nsource\n \n>\n \n(\n \neval_dataset\n: typing.Union[str, torch.utils.data.dataset.Dataset, NoneType] = None\n \n \n)\n \n \n \nParameters \n \n \neval_dataset\n (\nstr\n or \ntorch.utils.data.Dataset\n, \noptional\n) \u2014\nIf a \nstr\n, will use \nself.eval_dataset[eval_dataset]\n as the evaluation dataset. If a \nDataset\n, will override \nself.eval_dataset\n and must implement \n__len__\n. If it is a \nDataset\n, columns not accepted by the \nmodel.forward()\n method are automatically removed.\n \n \n \n \nReturns the evaluation \n~torch.utils.data.DataLoader\n.\n \nSubclass and override this method if you want to inject some custom behavior.\n \n \nget_learning_rates\n \n \n<\n \nsource\n \n>\n \n(\n \n)\n \n \n \n \nReturns the learning rate of each parameter from self.optimizer.\n \n \nget_num_trainable_parameters\n \n \n<\n \nsource\n \n>\n \n(\n \n)\n \n \n \n \nGet the number of trainable parameters.\n \n \nget_optimizer_cls_and_kwargs\n \n \n<\n \nsource\n \n>\n \n(\n \nargs\n: TrainingArguments\n \nmodel\n: typing.Optional[transformers.modeling_utils.PreTrainedModel] = None\n \n \n)\n \n \n \nParameters \n \n \nargs\n (\ntransformers.training_args.TrainingArguments\n) \u2014\nThe training arguments for the training session.\n \n \n \n \nReturns the optimizer class and optimizer parameters based on the training arguments.\n \n \nget_optimizer_group\n \n \n<\n \nsource\n \n>\n \n(\n \nparam\n: typing.Union[str, torch.nn.parameter.Parameter, NoneType] = None\n \n \n)\n \n \n \nParameters \n \n \nparam\n (\nstr\n or \ntorch.nn.parameter.Parameter\n, \noptional\n) \u2014\nThe parameter for which optimizer group needs to be returned.\n \n \n \n \nReturns optimizer group for a parameter if given, else returns all optimizer groups for params.\n \n \nget_test_dataloader\n \n \n<\n \nsource\n \n>\n \n(\n \ntest_dataset\n: Dataset\n \n \n)\n \n \n \nParameters \n \n \ntest_dataset\n (\ntorch.utils.data.Dataset\n, \noptional\n) \u2014\nThe test dataset to use. If it is a \nDataset\n, columns not accepted by the\n\nmodel.forward()\n method are automatically removed. It must implement \n__len__\n.\n \n \n \n \nReturns the test \n~torch.utils.data.DataLoader\n.\n \nSubclass and override this method if you want to inject some custom behavior.\n \n \nget_total_train_batch_size\n \n \n<\n \nsource\n \n>\n \n(\n \nargs\n \n \n)\n \n \n \n \nCalculates total batch size (micro_batch \ngrad_accum\n dp_world_size).\n \nNote: Only considers DP and TP (dp_world_size = world_size // tp_size).\n \n \nget_tp_size\n \n \n<\n \nsource\n \n>\n \n(\n \n)\n \n \n \n \nGet the tensor parallel size from either the model or DeepSpeed config.\n \n \nget_train_dataloader\n \n \n<\n \nsource\n \n>\n \n(\n \n)\n \n \n \n \nReturns the training \n~torch.utils.data.DataLoader\n.\n \nWill use no sampler if \ntrain_dataset\n does not implement \n__len__\n, a random sampler (adapted to distributed\ntraining if necessary) otherwise.\n \nSubclass and override this method if you want to inject some custom behavior.\n \n \nhyperparameter_search\n \n \n<\n \nsource\n \n>\n \n(\n \nhp_space\n: typing.Optional[typing.Callable[[ForwardRef('optuna.Trial')], dict[str, float]]] = None\n \ncompute_objective\n: typing.Optional[typing.Callable[[dict[str, float]], float]] = None\n \nn_trials\n: int = 20\n \ndirection\n: typing.Union[str, list[str]] = 'minimize'\n \nbackend\n: typing.Union[ForwardRef('str'), transformers.trainer_utils.HPSearchBackend, NoneType] = None\n \nhp_name\n: typing.Optional[typing.Callable[[ForwardRef('optuna.Trial')], str]] = None\n \n**kwargs\n \n \n)\n \n\u2192\n \n[\ntrainer_utils.BestRun\n or \nlist[trainer_utils.BestRun]\n]\n \n \nParameters \n \n \nhp_space\n (\nCallable[[\"optuna.Trial\"], dict[str, float]]\n, \noptional\n) \u2014\nA function that defines the hyperparameter search space. Will default to\n\ndefault_hp_space_optuna()\n or \ndefault_hp_space_ray()\n or\n\ndefault_hp_space_sigopt()\n depending on your backend.\n \n \n \ncompute_objective\n (\nCallable[[dict[str, float]], float]\n, \noptional\n) \u2014\nA function computing the objective to minimize or maximize from the metrics returned by the \nevaluate\n\nmethod. Will default to \ndefault_compute_objective()\n.\n \n \n \nn_trials\n (\nint\n, \noptional\n, defaults to 100) \u2014\nThe number of trial runs to test.\n \n \n \ndirection\n (\nstr\n or \nlist[str]\n, \noptional\n, defaults to \n\"minimize\"\n) \u2014\nIf it\u2019s single objective optimization, direction is \nstr\n, can be \n\"minimize\"\n or \n\"maximize\"\n, you\nshould pick \n\"minimize\"\n when optimizing the validation loss, \n\"maximize\"\n when optimizing one or\nseveral metrics. If it\u2019s multi objectives optimization, direction is \nlist[str]\n, can be List of\n\n\"minimize\"\n and \n\"maximize\"\n, you should pick \n\"minimize\"\n when optimizing the validation loss,\n\n\"maximize\"\n when optimizing one or several metrics.\n \n \n \nbackend\n (\nstr\n or \n~training_utils.HPSearchBackend\n, \noptional\n) \u2014\nThe backend to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt, depending\non which one is installed. If all are installed, will default to optuna.\n \n \n \nhp_name\n (\nCallable[[\"optuna.Trial\"], str]]\n, \noptional\n) \u2014\nA function that defines the trial/run name. Will default to None.\n \n \n \nkwargs\n (\ndict[str, Any]\n, \noptional\n) \u2014\nAdditional keyword arguments for each backend:\n\n\n\n\noptuna\n: parameters from\n\noptuna.study.create_study\n\nand also the parameters \ntimeout\n, \nn_jobs\n and \ngc_after_trial\n from\n\noptuna.study.Study.optimize\n\n\nray\n: parameters from \ntune.run\n.\nIf \nresources_per_trial\n is not set in the \nkwargs\n, it defaults to 1 CPU core and 1 GPU (if available).\nIf \nprogress_reporter\n is not set in the \nkwargs\n,\n\nray.tune.CLIReporter\n is used.\n\n\nsigopt\n: the parameter \nproxies\n from\n\nsigopt.Connection.set_proxies\n.\n\n\n \n \n \nReturns\n \n\n\n[\ntrainer_utils.BestRun\n or \nlist[trainer_utils.BestRun]\n]\n\n\n \n \n\n\nAll the information about the best run or best\nruns for multi-objective optimization. Experiment summary can be found in \nrun_summary\n attribute for Ray\nbackend.\n\n\n \n \nLaunch an hyperparameter search using \noptuna\n or \nRay Tune\n or \nSigOpt\n. The optimized quantity is determined\nby \ncompute_objective\n, which defaults to a function returning the evaluation loss when no metric is provided,\nthe sum of all metrics otherwise.\n \nTo use this method, you need to have provided a \nmodel_init\n when initializing your \nTrainer\n: we need to\nreinitialize the model at each new run. This is incompatible with the \noptimizers\n argument, so you need to\nsubclass \nTrainer\n and override the method \ncreate_optimizer_and_scheduler()\n for custom\noptimizer/scheduler.\n \n \ninit_hf_repo\n \n \n<\n \nsource\n \n>\n \n(\n \ntoken\n: typing.Optional[str] = None\n \n \n)\n \n \n \n \nInitializes a git repo in \nself.args.hub_model_id\n.\n \n \nis_local_process_zero\n \n \n<\n \nsource\n \n>\n \n(\n \n)\n \n \n \n \nWhether or not this process is the local (e.g., on one machine if training in a distributed fashion on several\nmachines) main process.\n \n \nis_world_process_zero\n \n \n<\n \nsource\n \n>\n \n(\n \n)\n \n \n \n \nWhether or not this process is the global main process (when training in a distributed fashion on several\nmachines, this is only going to be \nTrue\n for one process).\n \n \nlog\n \n \n<\n \nsource\n \n>\n \n(\n \nlogs\n: dict\n \nstart_time\n: typing.Optional[float] = None\n \n \n)\n \n \n \nParameters \n \n \nlogs\n (\ndict[str, float]\n) \u2014\nThe values to log.\n \n \n \nstart_time\n (\nOptional[float]\n) \u2014\nThe start of training.\n \n \n \n \nLog \nlogs\n on the various objects watching training.\n \nSubclass and override this method to inject custom behavior.\n \n \nlog_metrics\n \n \n<\n \nsource\n \n>\n \n(\n \nsplit\n \nmetrics\n \n \n)\n \n \n \nParameters \n \n \nsplit\n (\nstr\n) \u2014\nMode/split name: one of \ntrain\n, \neval\n, \ntest\n \n \n \nmetrics\n (\ndict[str, float]\n) \u2014\nThe metrics returned from train/evaluate/predictmetrics: metrics dict\n \n \n \n \nLog metrics in a specially formatted way.\n \nUnder distributed environment this is done only for a process with rank 0.\n \nNotes on memory reports:\n \nIn order to get memory usage report you need to install \npsutil\n. You can do that with \npip install psutil\n.\n \n \nNow when this method is run, you will see a report that will include:\n \n \n Copied\n \ninit_mem_cpu_alloc_delta\n   =     \n1301\nMB\n\ninit_mem_cpu_peaked_delta\n  =      \n154\nMB\n\ninit_mem_gpu_alloc_delta\n   =      \n230\nMB\n\ninit_mem_gpu_peaked_delta\n  =        \n0\nMB\n\ntrain_mem_cpu_alloc_delta\n  =     \n1345\nMB\n\ntrain_mem_cpu_peaked_delta\n =        \n0\nMB\n\ntrain_mem_gpu_alloc_delta\n  =      \n693\nMB\n\ntrain_mem_gpu_peaked_delta\n =        \n7\nMB\n \nUnderstanding the reports:\n \nthe first segment, e.g., \ntrain__\n, tells you which stage the metrics are for. Reports starting with \ninit_\n\nwill be added to the first stage that gets run. So that if only evaluation is run, the memory usage for the\n\n__init__\n will be reported along with the \neval_\n metrics.\n \nthe third segment, is either \ncpu\n or \ngpu\n, tells you whether it\u2019s the general RAM or the gpu0 memory\nmetric.\n \n*_alloc_delta\n - is the difference in the used/allocated memory counter between the end and the start of the\nstage - it can be negative if a function released more memory than it allocated.\n \n*_peaked_delta\n - is any extra memory that was consumed and then freed - relative to the current allocated\nmemory counter - it is never negative. When you look at the metrics of any stage you add up \nalloc_delta\n +\n\npeaked_delta\n and you know how much memory was needed to complete that stage.\n \nThe reporting happens only for process of rank 0 and gpu 0 (if there is a gpu). Typically this is enough since the\nmain process does the bulk of work, but it could be not quite so if model parallel is used and then other GPUs may\nuse a different amount of gpu memory. This is also not the same under DataParallel where gpu0 may require much more\nmemory than the rest since it stores the gradient and optimizer states for all participating GPUs. Perhaps in the\nfuture these reports will evolve to measure those too.\n \nThe CPU RAM metric measures RSS (Resident Set Size) includes both the memory which is unique to the process and the\nmemory shared with other processes. It is important to note that it does not include swapped out memory, so the\nreports could be imprecise.\n \nThe CPU peak memory is measured using a sampling thread. Due to python\u2019s GIL it may miss some of the peak memory if\nthat thread didn\u2019t get a chance to run when the highest memory was used. Therefore this report can be less than\nreality. Using \ntracemalloc\n would have reported the exact peak memory, but it doesn\u2019t report memory allocations\noutside of python. So if some C++ CUDA extension allocated its own memory it won\u2019t be reported. And therefore it\nwas dropped in favor of the memory sampling approach, which reads the current process memory usage.\n \nThe GPU allocated and peak memory reporting is done with \ntorch.cuda.memory_allocated()\n and\n\ntorch.cuda.max_memory_allocated()\n. This metric reports only \u201cdeltas\u201d for pytorch-specific allocations, as\n\ntorch.cuda\n memory management system doesn\u2019t track any memory allocated outside of pytorch. For example, the very\nfirst cuda call typically loads CUDA kernels, which may take from 0.5 to 2GB of GPU memory.\n \nNote that this tracker doesn\u2019t account for memory allocations outside of \nTrainer\n\u2019s \n__init__\n, \ntrain\n,\n\nevaluate\n and \npredict\n calls.\n \nBecause \nevaluation\n calls may happen during \ntrain\n, we can\u2019t handle nested invocations because\n\ntorch.cuda.max_memory_allocated\n is a single counter, so if it gets reset by a nested eval call, \ntrain\n\u2019s tracker\nwill report incorrect info. If this \npytorch issue\n gets resolved\nit will be possible to change this class to be re-entrant. Until then we will only track the outer level of\n\ntrain\n, \nevaluate\n and \npredict\n methods. Which means that if \neval\n is called during \ntrain\n, it\u2019s the latter\nthat will account for its memory usage and that of the former.\n \nThis also means that if any other tool that is used along the \nTrainer\n calls\n\ntorch.cuda.reset_peak_memory_stats\n, the gpu peak memory stats could be invalid. And the \nTrainer\n will disrupt\nthe normal behavior of any such tools that rely on calling \ntorch.cuda.reset_peak_memory_stats\n themselves.\n \nFor best performance you may want to consider turning the memory profiling off for production runs.\n \n \nmetrics_format\n \n \n<\n \nsource\n \n>\n \n(\n \nmetrics\n: dict\n \n \n)\n \n\u2192\n \nmetrics (\ndict[str, float]\n)\n \n \nParameters \n \n \nmetrics\n (\ndict[str, float]\n) \u2014\nThe metrics returned from train/evaluate/predict\n \n \n \nReturns\n \n\n\nmetrics (\ndict[str, float]\n)\n\n\n \n \n\n\nThe reformatted metrics\n\n\n \n \nReformat Trainer metrics values to a human-readable format.\n \n \nnum_examples\n \n \n<\n \nsource\n \n>\n \n(\n \ndataloader\n: DataLoader\n \n \n)\n \n \n \n \nHelper to get number of samples in a \n~torch.utils.data.DataLoader\n by accessing its dataset. When\ndataloader.dataset does not exist or has no length, estimates as best it can\n \n \nnum_tokens\n \n \n<\n \nsource\n \n>\n \n(\n \ntrain_dl\n: DataLoader\n \nmax_steps\n: typing.Optional[int] = None\n \n \n)\n \n \n \n \nHelper to get number of tokens in a \n~torch.utils.data.DataLoader\n by enumerating dataloader.\n \n \npop_callback\n \n \n<\n \nsource\n \n>\n \n(\n \ncallback\n \n \n)\n \n\u2192\n \nTrainerCallback\n \n \nParameters \n \n \ncallback\n (\ntype\n or [`~transformers.TrainerCallback]`) \u2014\nA \nTrainerCallback\n class or an instance of a \nTrainerCallback\n. In the\nfirst case, will pop the first member of that class found in the list of callbacks.\n \n \n \nReturns\n \n\n\nTrainerCallback\n\n\n \n \n\n\nThe callback removed, if found.\n\n\n \n \nRemove a callback from the current list of \nTrainerCallback\n and returns it.\n \nIf the callback is not found, returns \nNone\n (and no error is raised).\n \n \npredict\n \n \n<\n \nsource\n \n>\n \n(\n \ntest_dataset\n: Dataset\n \nignore_keys\n: typing.Optional[list[str]] = None\n \nmetric_key_prefix\n: str = 'test'\n \n \n)\n \n \n \nParameters \n \n \ntest_dataset\n (\nDataset\n) \u2014\nDataset to run the predictions on. If it is an \ndatasets.Dataset\n, columns not accepted by the\n\nmodel.forward()\n method are automatically removed. Has to implement the method \n__len__\n \n \n \nignore_keys\n (\nlist[str]\n, \noptional\n) \u2014\nA list of keys in the output of your model (if it is a dictionary) that should be ignored when\ngathering predictions.\n \n \n \nmetric_key_prefix\n (\nstr\n, \noptional\n, defaults to \n\"test\"\n) \u2014\nAn optional prefix to be used as the metrics key prefix. For example the metrics \u201cbleu\u201d will be named\n\u201ctest_bleu\u201d if the prefix is \u201ctest\u201d (default)\n \n \n \n \nRun prediction and returns predictions and potential metrics.\n \nDepending on the dataset and your use case, your test dataset may contain labels. In that case, this method\nwill also return metrics, like in \nevaluate()\n.\n \nIf your predictions or labels have different sequence length (for instance because you\u2019re doing dynamic padding\nin a token classification task) the predictions will be padded (on the right) to allow for concatenation into\none array. The padding index is -100.\n \nReturns: \nNamedTuple\n A namedtuple with the following keys:\n \npredictions (\nnp.ndarray\n): The predictions on \ntest_dataset\n.\n \nlabel_ids (\nnp.ndarray\n, \noptional\n): The labels (if the dataset contained some).\n \nmetrics (\ndict[str, float]\n, \noptional\n): The potential dictionary of metrics (if the dataset contained\nlabels).\n \n \nprediction_loop\n \n \n<\n \nsource\n \n>\n \n(\n \ndataloader\n: DataLoader\n \ndescription\n: str\n \nprediction_loss_only\n: typing.Optional[bool] = None\n \nignore_keys\n: typing.Optional[list[str]] = None\n \nmetric_key_prefix\n: str = 'eval'\n \n \n)\n \n \n \n \nPrediction/evaluation loop, shared by \nTrainer.evaluate()\n and \nTrainer.predict()\n.\n \nWorks both with or without labels.\n \n \nprediction_step\n \n \n<\n \nsource\n \n>\n \n(\n \nmodel\n: Module\n \ninputs\n: dict\n \nprediction_loss_only\n: bool\n \nignore_keys\n: typing.Optional[list[str]] = None\n \n \n)\n \n\u2192\n \ntuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]\n \n \nParameters \n \n \nmodel\n (\nnn.Module\n) \u2014\nThe model to evaluate.\n \n \n \ninputs\n (\ndict[str, Union[torch.Tensor, Any]]\n) \u2014\nThe inputs and targets of the model.\n\n\nThe dictionary will be unpacked before being fed to the model. Most models expect the targets under the\nargument \nlabels\n. Check your model\u2019s documentation for all accepted arguments.\n \n \n \nprediction_loss_only\n (\nbool\n) \u2014\nWhether or not to return the loss only.\n \n \n \nignore_keys\n (\nlist[str]\n, \noptional\n) \u2014\nA list of keys in the output of your model (if it is a dictionary) that should be ignored when\ngathering predictions.\n \n \n \nReturns\n \n\n\ntuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]\n\n\n \n \n\n\nA tuple with the loss,\nlogits and labels (each being optional).\n\n\n \n \nPerform an evaluation step on \nmodel\n using \ninputs\n.\n \nSubclass and override to inject custom behavior.\n \n \npropagate_args_to_deepspeed\n \n \n<\n \nsource\n \n>\n \n(\n \nauto_find_batch_size\n = False\n \n \n)\n \n \n \n \nSets values in the deepspeed plugin based on the Trainer args\n \n \npush_to_hub\n \n \n<\n \nsource\n \n>\n \n(\n \ncommit_message\n: typing.Optional[str] = 'End of training'\n \nblocking\n: bool = True\n \ntoken\n: typing.Optional[str] = None\n \nrevision\n: typing.Optional[str] = None\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \ncommit_message\n (\nstr\n, \noptional\n, defaults to \n\"End of training\"\n) \u2014\nMessage to commit while pushing.\n \n \n \nblocking\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether the function should return only when the \ngit push\n has finished.\n \n \n \ntoken\n (\nstr\n, \noptional\n, defaults to \nNone\n) \u2014\nToken with write permission to overwrite Trainer\u2019s original args.\n \n \n \nrevision\n (\nstr\n, \noptional\n) \u2014\nThe git revision to commit from. Defaults to the head of the \u201cmain\u201d branch.\n \n \n \nkwargs\n (\ndict[str, Any]\n, \noptional\n) \u2014\nAdditional keyword arguments passed along to \ncreate_model_card()\n.\n \n \n \n \nUpload \nself.model\n and \nself.processing_class\n to the \ud83e\udd17 model hub on the repo \nself.args.hub_model_id\n.\n \n \nremove_callback\n \n \n<\n \nsource\n \n>\n \n(\n \ncallback\n \n \n)\n \n \n \nParameters \n \n \ncallback\n (\ntype\n or [`~transformers.TrainerCallback]`) \u2014\nA \nTrainerCallback\n class or an instance of a \nTrainerCallback\n. In the\nfirst case, will remove the first member of that class found in the list of callbacks.\n \n \n \n \nRemove a callback from the current list of \nTrainerCallback\n.\n \n \nsave_metrics\n \n \n<\n \nsource\n \n>\n \n(\n \nsplit\n \nmetrics\n \ncombined\n = True\n \n \n)\n \n \n \nParameters \n \n \nsplit\n (\nstr\n) \u2014\nMode/split name: one of \ntrain\n, \neval\n, \ntest\n, \nall\n \n \n \nmetrics\n (\ndict[str, float]\n) \u2014\nThe metrics returned from train/evaluate/predict\n \n \n \ncombined\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nCreates combined metrics by updating \nall_results.json\n with metrics of this call\n \n \n \n \nSave metrics into a json file for that split, e.g. \ntrain_results.json\n.\n \nUnder distributed environment this is done only for a process with rank 0.\n \nTo understand the metrics please read the docstring of \nlog_metrics()\n. The only difference is that raw\nunformatted numbers are saved in the current method.\n \n \nsave_model\n \n \n<\n \nsource\n \n>\n \n(\n \noutput_dir\n: typing.Optional[str] = None\n \n_internal_call\n: bool = False\n \n \n)\n \n \n \n \nWill save the model, so you can reload it using \nfrom_pretrained()\n.\n \nWill only save from the main process.\n \n \nsave_state\n \n \n<\n \nsource\n \n>\n \n(\n \n)\n \n \n \n \nSaves the Trainer state, since Trainer.save_model saves only the tokenizer with the model.\n \nUnder distributed environment this is done only for a process with rank 0.\n \n \nset_initial_training_values\n \n \n<\n \nsource\n \n>\n \n(\n \nargs\n: TrainingArguments\n \ndataloader\n: DataLoader\n \ntotal_train_batch_size\n: int\n \n \n)\n \n \n \n \nCalculates and returns the following values:\n \nnum_train_epochs\n \nnum_update_steps_per_epoch\n \nnum_examples\n \nnum_train_samples\n \nepoch_based\n \nlen_dataloader\n \nmax_steps\n \n \ntrain\n \n \n<\n \nsource\n \n>\n \n(\n \nresume_from_checkpoint\n: typing.Union[str, bool, NoneType] = None\n \ntrial\n: typing.Union[ForwardRef('optuna.Trial'), dict[str, typing.Any], NoneType] = None\n \nignore_keys_for_eval\n: typing.Optional[list[str]] = None\n \n**kwargs\n \n \n)\n \n \n \nParameters \n \n \nresume_from_checkpoint\n (\nstr\n or \nbool\n, \noptional\n) \u2014\nIf a \nstr\n, local path to a saved checkpoint as saved by a previous instance of \nTrainer\n. If a\n\nbool\n and equals \nTrue\n, load the last checkpoint in \nargs.output_dir\n as saved by a previous instance\nof \nTrainer\n. If present, training will resume from the model/optimizer/scheduler states loaded here.\n \n \n \ntrial\n (\noptuna.Trial\n or \ndict[str, Any]\n, \noptional\n) \u2014\nThe trial run or the hyperparameter dictionary for hyperparameter search.\n \n \n \nignore_keys_for_eval\n (\nlist[str]\n, \noptional\n) \u2014\nA list of keys in the output of your model (if it is a dictionary) that should be ignored when\ngathering predictions for evaluation during the training.\n \n \n \nkwargs\n (\ndict[str, Any]\n, \noptional\n) \u2014\nAdditional keyword arguments used to hide deprecated arguments\n \n \n \n \nMain training entry point.\n \n \ntraining_step\n \n \n<\n \nsource\n \n>\n \n(\n \nmodel\n: Module\n \ninputs\n: dict\n \nnum_items_in_batch\n: typing.Optional[torch.Tensor] = None\n \n \n)\n \n\u2192\n \ntorch.Tensor\n \n \nParameters \n \n \nmodel\n (\nnn.Module\n) \u2014\nThe model to train.\n \n \n \ninputs\n (\ndict[str, Union[torch.Tensor, Any]]\n) \u2014\nThe inputs and targets of the model.\n\n\nThe dictionary will be unpacked before being fed to the model. Most models expect the targets under the\nargument \nlabels\n. Check your model\u2019s documentation for all accepted arguments.\n \n \n \nReturns\n \n\n\ntorch.Tensor\n\n\n \n \n\n\nThe tensor with training loss on this batch.\n\n\n \n \nPerform a training step on a batch of inputs.\n \nSubclass and override to inject custom behavior.\n \n \nSeq2SeqTrainer\n \n \nclass\n \ntransformers.\nSeq2SeqTrainer\n \n \n<\n \nsource\n \n>\n \n(\n \nmodel\n: typing.Union[ForwardRef('PreTrainedModel'), torch.nn.modules.module.Module] = None\n \nargs\n: TrainingArguments = None\n \ndata_collator\n: typing.Optional[ForwardRef('DataCollator')] = None\n \ntrain_dataset\n: typing.Union[torch.utils.data.dataset.Dataset, ForwardRef('IterableDataset'), ForwardRef('datasets.Dataset'), NoneType] = None\n \neval_dataset\n: typing.Union[torch.utils.data.dataset.Dataset, dict[str, torch.utils.data.dataset.Dataset], NoneType] = None\n \nprocessing_class\n: typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('BaseImageProcessor'), ForwardRef('FeatureExtractionMixin'), ForwardRef('ProcessorMixin'), NoneType] = None\n \nmodel_init\n: typing.Optional[typing.Callable[[], ForwardRef('PreTrainedModel')]] = None\n \ncompute_loss_func\n: typing.Optional[typing.Callable] = None\n \ncompute_metrics\n: typing.Optional[typing.Callable[[ForwardRef('EvalPrediction')], dict]] = None\n \ncallbacks\n: typing.Optional[list['TrainerCallback']] = None\n \noptimizers\n: tuple = (None, None)\n \npreprocess_logits_for_metrics\n: typing.Optional[typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None\n \n \n)\n \n \n \n \n \nevaluate\n \n \n<\n \nsource\n \n>\n \n(\n \neval_dataset\n: typing.Optional[torch.utils.data.dataset.Dataset] = None\n \nignore_keys\n: typing.Optional[list[str]] = None\n \nmetric_key_prefix\n: str = 'eval'\n \n**gen_kwargs\n \n \n)\n \n \n \nParameters \n \n \neval_dataset\n (\nDataset\n, \noptional\n) \u2014\nPass a dataset if you wish to override \nself.eval_dataset\n. If it is an \nDataset\n, columns\nnot accepted by the \nmodel.forward()\n method are automatically removed. It must implement the \n__len__\n\nmethod.\n \n \n \nignore_keys\n (\nlist[str]\n, \noptional\n) \u2014\nA list of keys in the output of your model (if it is a dictionary) that should be ignored when\ngathering predictions.\n \n \n \nmetric_key_prefix\n (\nstr\n, \noptional\n, defaults to \n\"eval\"\n) \u2014\nAn optional prefix to be used as the metrics key prefix. For example the metrics \u201cbleu\u201d will be named\n\u201ceval_bleu\u201d if the prefix is \n\"eval\"\n (default)\n \n \n \nmax_length\n (\nint\n, \noptional\n) \u2014\nThe maximum target length to use when predicting with the generate method.\n \n \n \nnum_beams\n (\nint\n, \noptional\n) \u2014\nNumber of beams for beam search that will be used when predicting with the generate method. 1 means no\nbeam search.\n \n \n \ngen_kwargs\n \u2014\nAdditional \ngenerate\n specific kwargs.\n \n \n \n \nRun evaluation and returns metrics.\n \nThe calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n(pass it to the init \ncompute_metrics\n argument).\n \nYou can also subclass and override this method to inject custom behavior.\n \n \npredict\n \n \n<\n \nsource\n \n>\n \n(\n \ntest_dataset\n: Dataset\n \nignore_keys\n: typing.Optional[list[str]] = None\n \nmetric_key_prefix\n: str = 'test'\n \n**gen_kwargs\n \n \n)\n \n \n \nParameters \n \n \ntest_dataset\n (\nDataset\n) \u2014\nDataset to run the predictions on. If it is a \nDataset\n, columns not accepted by the\n\nmodel.forward()\n method are automatically removed. Has to implement the method \n__len__\n \n \n \nignore_keys\n (\nlist[str]\n, \noptional\n) \u2014\nA list of keys in the output of your model (if it is a dictionary) that should be ignored when\ngathering predictions.\n \n \n \nmetric_key_prefix\n (\nstr\n, \noptional\n, defaults to \n\"eval\"\n) \u2014\nAn optional prefix to be used as the metrics key prefix. For example the metrics \u201cbleu\u201d will be named\n\u201ceval_bleu\u201d if the prefix is \n\"eval\"\n (default)\n \n \n \nmax_length\n (\nint\n, \noptional\n) \u2014\nThe maximum target length to use when predicting with the generate method.\n \n \n \nnum_beams\n (\nint\n, \noptional\n) \u2014\nNumber of beams for beam search that will be used when predicting with the generate method. 1 means no\nbeam search.\n \n \n \ngen_kwargs\n \u2014\nAdditional \ngenerate\n specific kwargs.\n \n \n \n \nRun prediction and returns predictions and potential metrics.\n \nDepending on the dataset and your use case, your test dataset may contain labels. In that case, this method\nwill also return metrics, like in \nevaluate()\n.\n \nIf your predictions or labels have different sequence lengths (for instance because you\u2019re doing dynamic\npadding in a token classification task) the predictions will be padded (on the right) to allow for\nconcatenation into one array. The padding index is -100.\n \nReturns: \nNamedTuple\n A namedtuple with the following keys:\n \npredictions (\nnp.ndarray\n): The predictions on \ntest_dataset\n.\n \nlabel_ids (\nnp.ndarray\n, \noptional\n): The labels (if the dataset contained some).\n \nmetrics (\ndict[str, float]\n, \noptional\n): The potential dictionary of metrics (if the dataset contained\nlabels).\n \n \nTrainingArguments\n \n \nclass\n \ntransformers.\nTrainingArguments\n \n \n<\n \nsource\n \n>\n \n(\n \noutput_dir\n: typing.Optional[str] = None\n \noverwrite_output_dir\n: bool = False\n \ndo_train\n: bool = False\n \ndo_eval\n: bool = False\n \ndo_predict\n: bool = False\n \neval_strategy\n: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'no'\n \nprediction_loss_only\n: bool = False\n \nper_device_train_batch_size\n: int = 8\n \nper_device_eval_batch_size\n: int = 8\n \nper_gpu_train_batch_size\n: typing.Optional[int] = None\n \nper_gpu_eval_batch_size\n: typing.Optional[int] = None\n \ngradient_accumulation_steps\n: int = 1\n \neval_accumulation_steps\n: typing.Optional[int] = None\n \neval_delay\n: typing.Optional[float] = 0\n \ntorch_empty_cache_steps\n: typing.Optional[int] = None\n \nlearning_rate\n: float = 5e-05\n \nweight_decay\n: float = 0.0\n \nadam_beta1\n: float = 0.9\n \nadam_beta2\n: float = 0.999\n \nadam_epsilon\n: float = 1e-08\n \nmax_grad_norm\n: float = 1.0\n \nnum_train_epochs\n: float = 3.0\n \nmax_steps\n: int = -1\n \nlr_scheduler_type\n: typing.Union[transformers.trainer_utils.SchedulerType, str] = 'linear'\n \nlr_scheduler_kwargs\n: typing.Union[dict[str, typing.Any], str, NoneType] = <factory>\n \nwarmup_ratio\n: float = 0.0\n \nwarmup_steps\n: int = 0\n \nlog_level\n: str = 'passive'\n \nlog_level_replica\n: str = 'warning'\n \nlog_on_each_node\n: bool = True\n \nlogging_dir\n: typing.Optional[str] = None\n \nlogging_strategy\n: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'\n \nlogging_first_step\n: bool = False\n \nlogging_steps\n: float = 500\n \nlogging_nan_inf_filter\n: bool = True\n \nsave_strategy\n: typing.Union[transformers.trainer_utils.SaveStrategy, str] = 'steps'\n \nsave_steps\n: float = 500\n \nsave_total_limit\n: typing.Optional[int] = None\n \nsave_safetensors\n: typing.Optional[bool] = True\n \nsave_on_each_node\n: bool = False\n \nsave_only_model\n: bool = False\n \nrestore_callback_states_from_checkpoint\n: bool = False\n \nno_cuda\n: bool = False\n \nuse_cpu\n: bool = False\n \nuse_mps_device\n: bool = False\n \nseed\n: int = 42\n \ndata_seed\n: typing.Optional[int] = None\n \njit_mode_eval\n: bool = False\n \nuse_ipex\n: bool = False\n \nbf16\n: bool = False\n \nfp16\n: bool = False\n \nfp16_opt_level\n: str = 'O1'\n \nhalf_precision_backend\n: str = 'auto'\n \nbf16_full_eval\n: bool = False\n \nfp16_full_eval\n: bool = False\n \ntf32\n: typing.Optional[bool] = None\n \nlocal_rank\n: int = -1\n \nddp_backend\n: typing.Optional[str] = None\n \ntpu_num_cores\n: typing.Optional[int] = None\n \ntpu_metrics_debug\n: bool = False\n \ndebug\n: typing.Union[str, list[transformers.debug_utils.DebugOption]] = ''\n \ndataloader_drop_last\n: bool = False\n \neval_steps\n: typing.Optional[float] = None\n \ndataloader_num_workers\n: int = 0\n \ndataloader_prefetch_factor\n: typing.Optional[int] = None\n \npast_index\n: int = -1\n \nrun_name\n: typing.Optional[str] = None\n \ndisable_tqdm\n: typing.Optional[bool] = None\n \nremove_unused_columns\n: typing.Optional[bool] = True\n \nlabel_names\n: typing.Optional[list[str]] = None\n \nload_best_model_at_end\n: typing.Optional[bool] = False\n \nmetric_for_best_model\n: typing.Optional[str] = None\n \ngreater_is_better\n: typing.Optional[bool] = None\n \nignore_data_skip\n: bool = False\n \nfsdp\n: typing.Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = ''\n \nfsdp_min_num_params\n: int = 0\n \nfsdp_config\n: typing.Union[dict[str, typing.Any], str, NoneType] = None\n \nfsdp_transformer_layer_cls_to_wrap\n: typing.Optional[str] = None\n \naccelerator_config\n: typing.Union[dict, str, NoneType] = None\n \ndeepspeed\n: typing.Union[dict, str, NoneType] = None\n \nlabel_smoothing_factor\n: float = 0.0\n \noptim\n: typing.Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch'\n \noptim_args\n: typing.Optional[str] = None\n \nadafactor\n: bool = False\n \ngroup_by_length\n: bool = False\n \nlength_column_name\n: typing.Optional[str] = 'length'\n \nreport_to\n: typing.Union[NoneType, str, list[str]] = None\n \nddp_find_unused_parameters\n: typing.Optional[bool] = None\n \nddp_bucket_cap_mb\n: typing.Optional[int] = None\n \nddp_broadcast_buffers\n: typing.Optional[bool] = None\n \ndataloader_pin_memory\n: bool = True\n \ndataloader_persistent_workers\n: bool = False\n \nskip_memory_metrics\n: bool = True\n \nuse_legacy_prediction_loop\n: bool = False\n \npush_to_hub\n: bool = False\n \nresume_from_checkpoint\n: typing.Optional[str] = None\n \nhub_model_id\n: typing.Optional[str] = None\n \nhub_strategy\n: typing.Union[transformers.trainer_utils.HubStrategy, str] = 'every_save'\n \nhub_token\n: typing.Optional[str] = None\n \nhub_private_repo\n: typing.Optional[bool] = None\n \nhub_always_push\n: bool = False\n \nhub_revision\n: typing.Optional[str] = None\n \ngradient_checkpointing\n: bool = False\n \ngradient_checkpointing_kwargs\n: typing.Union[dict[str, typing.Any], str, NoneType] = None\n \ninclude_inputs_for_metrics\n: bool = False\n \ninclude_for_metrics\n: list = <factory>\n \neval_do_concat_batches\n: bool = True\n \nfp16_backend\n: str = 'auto'\n \npush_to_hub_model_id\n: typing.Optional[str] = None\n \npush_to_hub_organization\n: typing.Optional[str] = None\n \npush_to_hub_token\n: typing.Optional[str] = None\n \nmp_parameters\n: str = ''\n \nauto_find_batch_size\n: bool = False\n \nfull_determinism\n: bool = False\n \ntorchdynamo\n: typing.Optional[str] = None\n \nray_scope\n: typing.Optional[str] = 'last'\n \nddp_timeout\n: int = 1800\n \ntorch_compile\n: bool = False\n \ntorch_compile_backend\n: typing.Optional[str] = None\n \ntorch_compile_mode\n: typing.Optional[str] = None\n \ninclude_tokens_per_second\n: typing.Optional[bool] = False\n \ninclude_num_input_tokens_seen\n: typing.Optional[bool] = False\n \nneftune_noise_alpha\n: typing.Optional[float] = None\n \noptim_target_modules\n: typing.Union[NoneType, str, list[str]] = None\n \nbatch_eval_metrics\n: bool = False\n \neval_on_start\n: bool = False\n \nuse_liger_kernel\n: typing.Optional[bool] = False\n \nliger_kernel_config\n: typing.Optional[dict[str, bool]] = None\n \neval_use_gather_object\n: typing.Optional[bool] = False\n \naverage_tokens_across_devices\n: typing.Optional[bool] = False\n \n \n)\n \n \n \nParameters \n \n \noutput_dir\n (\nstr\n, \noptional\n, defaults to \n\"trainer_output\"\n) \u2014\nThe output directory where the model predictions and checkpoints will be written.\n \n \n \noverwrite_output_dir\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nIf \nTrue\n, overwrite the content of the output directory. Use this to continue training if \noutput_dir\n\npoints to a checkpoint directory.\n \n \n \ndo_train\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to run training or not. This argument is not directly used by \nTrainer\n, it\u2019s intended to be used\nby your training/evaluation scripts instead. See the \nexample\nscripts\n for more details.\n \n \n \ndo_eval\n (\nbool\n, \noptional\n) \u2014\nWhether to run evaluation on the validation set or not. Will be set to \nTrue\n if \neval_strategy\n is\ndifferent from \n\"no\"\n. This argument is not directly used by \nTrainer\n, it\u2019s intended to be used by your\ntraining/evaluation scripts instead. See the \nexample\nscripts\n for more details.\n \n \n \ndo_predict\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to run predictions on the test set or not. This argument is not directly used by \nTrainer\n, it\u2019s\nintended to be used by your training/evaluation scripts instead. See the \nexample\nscripts\n for more details.\n \n \n \neval_strategy\n (\nstr\n or \nIntervalStrategy\n, \noptional\n, defaults to \n\"no\"\n) \u2014\nThe evaluation strategy to adopt during training. Possible values are:\n\n\n\n\n\"no\"\n: No evaluation is done during training.\n\n\n\"steps\"\n: Evaluation is done (and logged) every \neval_steps\n.\n\n\n\"epoch\"\n: Evaluation is done at the end of each epoch.\n\n\n \n \n \nprediction_loss_only\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhen performing evaluation and generating predictions, only returns the loss.\n \n \n \nper_device_train_batch_size\n (\nint\n, \noptional\n, defaults to 8) \u2014\nThe batch size per device accelerator core/CPU for training.\n \n \n \nper_device_eval_batch_size\n (\nint\n, \noptional\n, defaults to 8) \u2014\nThe batch size per device accelerator core/CPU for evaluation.\n \n \n \ngradient_accumulation_steps\n (\nint\n, \noptional\n, defaults to 1) \u2014\nNumber of updates steps to accumulate the gradients for, before performing a backward/update pass.\n\n\n\n\nWhen using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,\nevaluation, save will be conducted every \ngradient_accumulation_steps * xxx_step\n training examples.\n\n\n \n \n \neval_accumulation_steps\n (\nint\n, \noptional\n) \u2014\nNumber of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\nleft unset, the whole predictions are accumulated on the device accelerator before being moved to the CPU (faster but\nrequires more memory).\n \n \n \neval_delay\n (\nfloat\n, \noptional\n) \u2014\nNumber of epochs or steps to wait for before the first evaluation can be performed, depending on the\neval_strategy.\n \n \n \ntorch_empty_cache_steps\n (\nint\n, \noptional\n) \u2014\nNumber of steps to wait before calling \ntorch.<device>.empty_cache()\n. If left unset or set to None, cache will not be emptied.\n\n\n\n\nThis can help avoid CUDA out-of-memory errors by lowering peak VRAM usage at a cost of about \n10% slower performance\n.\n\n\n \n \n \nlearning_rate\n (\nfloat\n, \noptional\n, defaults to 5e-5) \u2014\nThe initial learning rate for \nAdamW\n optimizer.\n \n \n \nweight_decay\n (\nfloat\n, \noptional\n, defaults to 0) \u2014\nThe weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in \nAdamW\n\noptimizer.\n \n \n \nadam_beta1\n (\nfloat\n, \noptional\n, defaults to 0.9) \u2014\nThe beta1 hyperparameter for the \nAdamW\n optimizer.\n \n \n \nadam_beta2\n (\nfloat\n, \noptional\n, defaults to 0.999) \u2014\nThe beta2 hyperparameter for the \nAdamW\n optimizer.\n \n \n \nadam_epsilon\n (\nfloat\n, \noptional\n, defaults to 1e-8) \u2014\nThe epsilon hyperparameter for the \nAdamW\n optimizer.\n \n \n \nmax_grad_norm\n (\nfloat\n, \noptional\n, defaults to 1.0) \u2014\nMaximum gradient norm (for gradient clipping).\n \n \n \nnum_train_epochs(\nfloat\n,\n \noptional\n, defaults to 3.0) \u2014\nTotal number of training epochs to perform (if not an integer, will perform the decimal part percents of\nthe last epoch before stopping training).\n \n \n \nmax_steps\n (\nint\n, \noptional\n, defaults to -1) \u2014\nIf set to a positive number, the total number of training steps to perform. Overrides \nnum_train_epochs\n.\nFor a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n\nmax_steps\n is reached.\n \n \n \nlr_scheduler_type\n (\nstr\n or \nSchedulerType\n, \noptional\n, defaults to \n\"linear\"\n) \u2014\nThe scheduler type to use. See the documentation of \nSchedulerType\n for all possible values.\n \n \n \nlr_scheduler_kwargs\n (\u2018dict\u2019, \noptional\n, defaults to {}) \u2014\nThe extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values.\n \n \n \nwarmup_ratio\n (\nfloat\n, \noptional\n, defaults to 0.0) \u2014\nRatio of total training steps used for a linear warmup from 0 to \nlearning_rate\n.\n \n \n \nwarmup_steps\n (\nint\n, \noptional\n, defaults to 0) \u2014\nNumber of steps used for a linear warmup from 0 to \nlearning_rate\n. Overrides any effect of \nwarmup_ratio\n.\n \n \n \nlog_level\n (\nstr\n, \noptional\n, defaults to \npassive\n) \u2014\nLogger log level to use on the main process. Possible choices are the log levels as strings: \u2018debug\u2019,\n\u2018info\u2019, \u2018warning\u2019, \u2018error\u2019 and \u2018critical\u2019, plus a \u2018passive\u2019 level which doesn\u2019t set anything and keeps the\ncurrent log level for the Transformers library (which will be \n\"warning\"\n by default).\n \n \n \nlog_level_replica\n (\nstr\n, \noptional\n, defaults to \n\"warning\"\n) \u2014\nLogger log level to use on replicas. Same choices as \nlog_level\n\u201d\n \n \n \nlog_on_each_node\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nIn multinode distributed training, whether to log using \nlog_level\n once per node, or only on the main\nnode.\n \n \n \nlogging_dir\n (\nstr\n, \noptional\n) \u2014\n\nTensorBoard\n log directory. Will default to\n*output_dir/runs/\nCURRENT_DATETIME_HOSTNAME*\n.\n \n \n \nlogging_strategy\n (\nstr\n or \nIntervalStrategy\n, \noptional\n, defaults to \n\"steps\"\n) \u2014\nThe logging strategy to adopt during training. Possible values are:\n\n\n\n\n\"no\"\n: No logging is done during training.\n\n\n\"epoch\"\n: Logging is done at the end of each epoch.\n\n\n\"steps\"\n: Logging is done every \nlogging_steps\n.\n\n\n \n \n \nlogging_first_step\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to log the first \nglobal_step\n or not.\n \n \n \nlogging_steps\n (\nint\n or \nfloat\n, \noptional\n, defaults to 500) \u2014\nNumber of update steps between two logs if \nlogging_strategy=\"steps\"\n. Should be an integer or a float in\nrange \n[0,1)\n. If smaller than 1, will be interpreted as ratio of total training steps.\n \n \n \nlogging_nan_inf_filter\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether to filter \nnan\n and \ninf\n losses for logging. If set to \nTrue\n the loss of every step that is \nnan\n\nor \ninf\n is filtered and the average loss of the current logging window is taken instead.\n\n\n\n\nlogging_nan_inf_filter\n only influences the logging of loss values, it does not change the behavior the\ngradient is computed or applied to the model.\n\n\n \n \n \nsave_strategy\n (\nstr\n or \nSaveStrategy\n, \noptional\n, defaults to \n\"steps\"\n) \u2014\nThe checkpoint save strategy to adopt during training. Possible values are:\n\n\n\n\n\"no\"\n: No save is done during training.\n\n\n\"epoch\"\n: Save is done at the end of each epoch.\n\n\n\"steps\"\n: Save is done every \nsave_steps\n.\n\n\n\"best\"\n: Save is done whenever a new \nbest_metric\n is achieved.\n\n\n\n\nIf \n\"epoch\"\n or \n\"steps\"\n is chosen, saving will also be performed at the\nvery end of training, always.\n \n \n \nsave_steps\n (\nint\n or \nfloat\n, \noptional\n, defaults to 500) \u2014\nNumber of updates steps before two checkpoint saves if \nsave_strategy=\"steps\"\n. Should be an integer or a\nfloat in range \n[0,1)\n. If smaller than 1, will be interpreted as ratio of total training steps.\n \n \n \nsave_total_limit\n (\nint\n, \noptional\n) \u2014\nIf a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n\noutput_dir\n. When \nload_best_model_at_end\n is enabled, the \u201cbest\u201d checkpoint according to\n\nmetric_for_best_model\n will always be retained in addition to the most recent ones. For example, for\n\nsave_total_limit=5\n and \nload_best_model_at_end\n, the four last checkpoints will always be retained\nalongside the best model. When \nsave_total_limit=1\n and \nload_best_model_at_end\n, it is possible that two\ncheckpoints are saved: the last one and the best one (if they are different).\n \n \n \nsave_safetensors\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nUse \nsafetensors\n saving and loading for state dicts instead of\ndefault \ntorch.load\n and \ntorch.save\n.\n \n \n \nsave_on_each_node\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhen doing multi-node distributed training, whether to save models and checkpoints on each node, or only on\nthe main one.\n\n\nThis should not be activated when the different nodes use the same storage as the files will be saved with\nthe same names for each node.\n \n \n \nsave_only_model\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhen checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.\nNote that when this is true, you won\u2019t be able to resume training from checkpoint.\nThis enables you to save storage by not storing the optimizer, scheduler & rng state.\nYou can only load the model using \nfrom_pretrained\n with this option set to \nTrue\n.\n \n \n \nrestore_callback_states_from_checkpoint\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to restore the callback states from the checkpoint. If \nTrue\n, will override\ncallbacks passed to the \nTrainer\n if they exist in the checkpoint.\u201d\n \n \n \nuse_cpu\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to use cpu. If set to False, we will use cuda or mps device if available.\n \n \n \nseed\n (\nint\n, \noptional\n, defaults to 42) \u2014\nRandom seed that will be set at the beginning of training. To ensure reproducibility across runs, use the\n\n~Trainer.model_init\n function to instantiate the model if it has some randomly initialized parameters.\n \n \n \ndata_seed\n (\nint\n, \noptional\n) \u2014\nRandom seed to be used with data samplers. If not set, random generators for data sampling will use the\nsame seed as \nseed\n. This can be used to ensure reproducibility of data sampling, independent of the model\nseed.\n \n \n \njit_mode_eval\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to use PyTorch jit trace for inference.\n \n \n \nuse_ipex\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nUse Intel extension for PyTorch when it is available. \nIPEX\ninstallation\n.\n \n \n \nbf16\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\nNVIDIA architecture or Intel XPU or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\n \n \n \nfp16\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n \n \n \nfp16_opt_level\n (\nstr\n, \noptional\n, defaults to \u2018O1\u2019) \u2014\nFor \nfp16\n training, Apex AMP optimization level selected in [\u2018O0\u2019, \u2018O1\u2019, \u2018O2\u2019, and \u2018O3\u2019]. See details on\nthe \nApex documentation\n.\n \n \n \nfp16_backend\n (\nstr\n, \noptional\n, defaults to \n\"auto\"\n) \u2014\nThis argument is deprecated. Use \nhalf_precision_backend\n instead.\n \n \n \nhalf_precision_backend\n (\nstr\n, \noptional\n, defaults to \n\"auto\"\n) \u2014\nThe backend to use for mixed precision training. Must be one of \n\"auto\", \"apex\", \"cpu_amp\"\n. \n\"auto\"\n will\nuse CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the\nrequested backend.\n \n \n \nbf16_full_eval\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\nmetric values. This is an experimental API and it may change.\n \n \n \nfp16_full_eval\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm\nmetric values.\n \n \n \ntf32\n (\nbool\n, \noptional\n) \u2014\nWhether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends\non PyTorch\u2019s version default of \ntorch.backends.cuda.matmul.allow_tf32\n. For more details please refer to\nthe \nTF32\n documentation. This is an\nexperimental API and it may change.\n \n \n \nlocal_rank\n (\nint\n, \noptional\n, defaults to -1) \u2014\nRank of the process during distributed training.\n \n \n \nddp_backend\n (\nstr\n, \noptional\n) \u2014\nThe backend to use for distributed training. Must be one of \n\"nccl\"\n, \n\"mpi\"\n, \n\"ccl\"\n, \n\"gloo\"\n, \n\"hccl\"\n.\n \n \n \ntpu_num_cores\n (\nint\n, \noptional\n) \u2014\nWhen training on TPU, the number of TPU cores (automatically passed by launcher script).\n \n \n \ndataloader_drop_last\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\nor not.\n \n \n \neval_steps\n (\nint\n or \nfloat\n, \noptional\n) \u2014\nNumber of update steps between two evaluations if \neval_strategy=\"steps\"\n. Will default to the same\nvalue as \nlogging_steps\n if not set. Should be an integer or a float in range \n[0,1)\n. If smaller than 1,\nwill be interpreted as ratio of total training steps.\n \n \n \ndataloader_num_workers\n (\nint\n, \noptional\n, defaults to 0) \u2014\nNumber of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\nmain process.\n \n \n \npast_index\n (\nint\n, \noptional\n, defaults to -1) \u2014\nSome models like \nTransformerXL\n or \nXLNet\n can make use of\nthe past hidden states for their predictions. If this argument is set to a positive int, the \nTrainer\n will\nuse the corresponding output (usually index 2) as the past state and feed it to the model at the next\ntraining step under the keyword argument \nmems\n.\n \n \n \nrun_name\n (\nstr\n, \noptional\n, defaults to \noutput_dir\n) \u2014\nA descriptor for the run. Typically used for \nwandb\n,\n\nmlflow\n, \ncomet\n and \nswanlab\n\nlogging. If not specified, will be the same as \noutput_dir\n.\n \n \n \ndisable_tqdm\n (\nbool\n, \noptional\n) \u2014\nWhether or not to disable the tqdm progress bars and table of metrics produced by\n\n~notebook.NotebookTrainingTracker\n in Jupyter Notebooks. Will default to \nTrue\n if the logging level is\nset to warn or lower (default), \nFalse\n otherwise.\n \n \n \nremove_unused_columns\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether or not to automatically remove the columns unused by the model forward method.\n \n \n \nlabel_names\n (\nlist[str]\n, \noptional\n) \u2014\nThe list of keys in your dictionary of inputs that correspond to the labels.\n\n\nWill eventually default to the list of argument names accepted by the model that contain the word \u201clabel\u201d,\nexcept if the model used is one of the \nXxxForQuestionAnswering\n in which case it will also include the\n\n[\"start_positions\", \"end_positions\"]\n keys.\n \n \n \nload_best_model_at_end\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to load the best model found during training at the end of training. When this option is\nenabled, the best checkpoint will always be saved. See\n\nsave_total_limit\n\nfor more.\n\n\n\n\nWhen set to \nTrue\n, the parameters \nsave_strategy\n needs to be the same as \neval_strategy\n, and in\nthe case it is \u201csteps\u201d, \nsave_steps\n must be a round multiple of \neval_steps\n.\n\n\n \n \n \nmetric_for_best_model\n (\nstr\n, \noptional\n) \u2014\nUse in conjunction with \nload_best_model_at_end\n to specify the metric to use to compare two different\nmodels. Must be the name of a metric returned by the evaluation with or without the prefix \n\"eval_\"\n.\n\n\nIf not specified, this will default to \n\"loss\"\n when either \nload_best_model_at_end == True\n\nor \nlr_scheduler_type == SchedulerType.REDUCE_ON_PLATEAU\n (to use the evaluation loss).\n\n\nIf you set this value, \ngreater_is_better\n will default to \nTrue\n unless the name ends with \u201closs\u201d.\nDon\u2019t forget to set it to \nFalse\n if your metric is better when lower.\n \n \n \ngreater_is_better\n (\nbool\n, \noptional\n) \u2014\nUse in conjunction with \nload_best_model_at_end\n and \nmetric_for_best_model\n to specify if better models\nshould have a greater metric or not. Will default to:\n\n\n\n\nTrue\n if \nmetric_for_best_model\n is set to a value that doesn\u2019t end in \n\"loss\"\n.\n\n\nFalse\n if \nmetric_for_best_model\n is not set, or set to a value that ends in \n\"loss\"\n.\n\n\n \n \n \nignore_data_skip\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhen resuming training, whether or not to skip the epochs and batches to get the data loading at the same\nstage as in the previous training. If set to \nTrue\n, the training will begin faster (as that skipping step\ncan take a long time) but will not yield the same results as the interrupted training would have.\n \n \n \nfsdp\n (\nbool\n, \nstr\n or list of \nFSDPOption\n, \noptional\n, defaults to \n''\n) \u2014\nUse PyTorch Distributed Parallel Training (in distributed training only).\n\n\nA list of options along the following:\n\n\n\n\n\"full_shard\"\n: Shard parameters, gradients and optimizer states.\n\n\n\"shard_grad_op\"\n: Shard optimizer states and gradients.\n\n\n\"hybrid_shard\"\n: Apply \nFULL_SHARD\n within a node, and replicate parameters across nodes.\n\n\n\"hybrid_shard_zero2\"\n: Apply \nSHARD_GRAD_OP\n within a node, and replicate parameters across nodes.\n\n\n\"offload\"\n: Offload parameters and gradients to CPUs (only compatible with \n\"full_shard\"\n and\n\n\"shard_grad_op\"\n).\n\n\n\"auto_wrap\"\n: Automatically recursively wrap layers with FSDP using \ndefault_auto_wrap_policy\n.\n\n\n \n \n \nfsdp_config\n (\nstr\n or \ndict\n, \noptional\n) \u2014\nConfig to be used with fsdp (Pytorch Distributed Parallel Training). The value is either a location of\nfsdp json config file (e.g., \nfsdp_config.json\n) or an already loaded json file as \ndict\n.\n\n\nA List of config and its options:\n\n\n\n\n\n\nmin_num_params (\nint\n, \noptional\n, defaults to \n0\n):\nFSDP\u2019s minimum number of parameters for Default Auto Wrapping. (useful only when \nfsdp\n field is\npassed).\n\n\n\n\n\n\ntransformer_layer_cls_to_wrap (\nlist[str]\n, \noptional\n):\nList of transformer layer class names (case-sensitive) to wrap, e.g, \nBertLayer\n, \nGPTJBlock\n,\n\nT5Block\n \u2026 (useful only when \nfsdp\n flag is passed).\n\n\n\n\n\n\nbackward_prefetch (\nstr\n, \noptional\n)\nFSDP\u2019s backward prefetch mode. Controls when to prefetch next set of parameters (useful only when\n\nfsdp\n field is passed).\n\n\nA list of options along the following:\n\n\n\n\n\"backward_pre\"\n : Prefetches the next set of parameters before the current set of parameter\u2019s\ngradient\ncomputation.\n\n\n\"backward_post\"\n : This prefetches the next set of parameters after the current set of\nparameter\u2019s\ngradient computation.\n\n\n\n\n\n\n\n\nforward_prefetch (\nbool\n, \noptional\n, defaults to \nFalse\n)\nFSDP\u2019s forward prefetch mode (useful only when \nfsdp\n field is passed).\nIf \n\"True\"\n, then FSDP explicitly prefetches the next upcoming all-gather while executing in the\nforward pass.\n\n\n\n\n\n\nlimit_all_gathers (\nbool\n, \noptional\n, defaults to \nFalse\n)\nFSDP\u2019s limit_all_gathers (useful only when \nfsdp\n field is passed).\nIf \n\"True\"\n, FSDP explicitly synchronizes the CPU thread to prevent too many in-flight\nall-gathers.\n\n\n\n\n\n\nuse_orig_params (\nbool\n, \noptional\n, defaults to \nTrue\n)\nIf \n\"True\"\n, allows non-uniform \nrequires_grad\n during init, which means support for interspersed\nfrozen and trainable parameters. Useful in cases such as parameter-efficient fine-tuning. Please\nrefer this\n[blog](\nhttps://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019\n\n\n\n\n\n\nsync_module_states (\nbool\n, \noptional\n, defaults to \nTrue\n)\nIf \n\"True\"\n, each individually wrapped FSDP unit will broadcast module parameters from rank 0 to\nensure they are the same across all ranks after initialization\n\n\n\n\n\n\ncpu_ram_efficient_loading (\nbool\n, \noptional\n, defaults to \nFalse\n)\nIf \n\"True\"\n, only the first process loads the pretrained model checkpoint while all other processes\nhave empty weights.  When this setting as \n\"True\"\n, \nsync_module_states\n also must to be \n\"True\"\n,\notherwise all the processes except the main process would have random weights leading to unexpected\nbehaviour during training.\n\n\n\n\n\n\nactivation_checkpointing (\nbool\n, \noptional\n, defaults to \nFalse\n):\nIf \n\"True\"\n, activation checkpointing is a technique to reduce memory usage by clearing activations of\ncertain layers and recomputing them during a backward pass. Effectively, this trades extra\ncomputation time for reduced memory usage.\n\n\n\n\n\n\nxla (\nbool\n, \noptional\n, defaults to \nFalse\n):\nWhether to use PyTorch/XLA Fully Sharded Data Parallel Training. This is an experimental feature\nand its API may evolve in the future.\n\n\n\n\n\n\nxla_fsdp_settings (\ndict\n, \noptional\n)\nThe value is a dictionary which stores the XLA FSDP wrapping parameters.\n\n\nFor a complete list of options, please see \nhere\n.\n\n\n\n\n\n\nxla_fsdp_grad_ckpt (\nbool\n, \noptional\n, defaults to \nFalse\n):\nWill use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be\nused when the xla flag is set to true, and an auto wrapping policy is specified through\nfsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.\n\n\n\n\n \n \n \ndeepspeed\n (\nstr\n or \ndict\n, \noptional\n) \u2014\nUse \nDeepspeed\n. This is an experimental feature and its API may\nevolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\n\nds_config.json\n) or an already loaded json file as a \ndict\n\u201d\n\n\n\n\t\t\t\t\t\t\nIf enabling any Zero-init, make sure that your model is not initialized until\n*after* initializing the `TrainingArguments`, else it will not be applied.\n\n\t\t\t\t\t\n \n \n \naccelerator_config\n (\nstr\n, \ndict\n, or \nAcceleratorConfig\n, \noptional\n) \u2014\nConfig to be used with the internal \nAccelerator\n implementation. The value is either a location of\naccelerator json config file (e.g., \naccelerator_config.json\n), an already loaded json file as \ndict\n,\nor an instance of \nAcceleratorConfig\n.\n\n\nA list of config and its options:\n\n\n\n\nsplit_batches (\nbool\n, \noptional\n, defaults to \nFalse\n):\nWhether or not the accelerator should split the batches yielded by the dataloaders across the devices. If\n\nTrue\n the actual batch size used will be the same on any kind of distributed processes, but it must be a\nround multiple of the \nnum_processes\n you are using. If \nFalse\n, actual batch size used will be the one set\nin your script multiplied by the number of processes.\n\n\ndispatch_batches (\nbool\n, \noptional\n):\nIf set to \nTrue\n, the dataloader prepared by the Accelerator is only iterated through on the main process\nand then the batches are split and broadcast to each process. Will default to \nTrue\n for \nDataLoader\n whose\nunderlying dataset is an \nIterableDataset\n, \nFalse\n otherwise.\n\n\neven_batches (\nbool\n, \noptional\n, defaults to \nTrue\n):\nIf set to \nTrue\n, in cases where the total batch size across all processes does not exactly divide the\ndataset, samples at the start of the dataset will be duplicated so the batch can be divided equally among\nall workers.\n\n\nuse_seedable_sampler (\nbool\n, \noptional\n, defaults to \nTrue\n):\nWhether or not use a fully seedable random sampler (\naccelerate.data_loader.SeedableRandomSampler\n). Ensures\ntraining results are fully reproducible using a different sampling technique. While seed-to-seed results\nmay differ, on average the differences are negligible when using multiple different seeds to compare. Should\nalso be ran with \n~utils.set_seed\n for the best results.\n\n\nuse_configured_state (\nbool\n, \noptional\n, defaults to \nFalse\n):\nWhether or not to use a pre-configured \nAcceleratorState\n or \nPartialState\n defined before calling \nTrainingArguments\n.\nIf \nTrue\n, an \nAccelerator\n or \nPartialState\n must be initialized. Note that by doing so, this could lead to issues\nwith hyperparameter tuning.\n\n\n \n \n \nlabel_smoothing_factor\n (\nfloat\n, \noptional\n, defaults to 0.0) \u2014\nThe label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded\nlabels are changed from 0s and 1s to \nlabel_smoothing_factor/num_labels\n and \n1 - label_smoothing_factor + label_smoothing_factor/num_labels\n respectively.\n \n \n \ndebug\n (\nstr\n or list of \nDebugOption\n, \noptional\n, defaults to \n\"\"\n) \u2014\nEnable one or more debug features. This is an experimental feature.\n\n\nPossible options are:\n\n\n\n\n\"underflow_overflow\"\n: detects overflow in model\u2019s input/outputs and reports the last frames that led to\nthe event\n\n\n\"tpu_metrics_debug\"\n: print debug metrics on TPU\n\n\n\n\nThe options should be separated by whitespaces.\n \n \n \noptim\n (\nstr\n or \ntraining_args.OptimizerNames\n, \noptional\n, defaults to \n\"adamw_torch\"\n) \u2014\nThe optimizer to use, such as \u201cadamw_torch\u201d, \u201cadamw_torch_fused\u201d, \u201cadamw_apex_fused\u201d, \u201cadamw_anyprecision\u201d,\n\u201cadafactor\u201d. See \nOptimizerNames\n in \ntraining_args.py\n\nfor a full list of optimizers.\n \n \n \noptim_args\n (\nstr\n, \noptional\n) \u2014\nOptional arguments that are supplied to optimizers such as AnyPrecisionAdamW, AdEMAMix, and GaLore.\n \n \n \ngroup_by_length\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to group together samples of roughly the same length in the training dataset (to minimize\npadding applied and be more efficient). Only useful if applying dynamic padding.\n \n \n \nlength_column_name\n (\nstr\n, \noptional\n, defaults to \n\"length\"\n) \u2014\nColumn name for precomputed lengths. If the column exists, grouping by length will use these values rather\nthan computing them on train startup. Ignored unless \ngroup_by_length\n is \nTrue\n and the dataset is an\ninstance of \nDataset\n.\n \n \n \nreport_to\n (\nstr\n or \nlist[str]\n, \noptional\n, defaults to \n\"all\"\n) \u2014\nThe list of integrations to report the results and logs to. Supported platforms are \n\"azure_ml\"\n,\n\n\"clearml\"\n, \n\"codecarbon\"\n, \n\"comet_ml\"\n, \n\"dagshub\"\n, \n\"dvclive\"\n, \n\"flyte\"\n, \n\"mlflow\"\n, \n\"neptune\"\n,\n\n\"swanlab\"\n, \n\"tensorboard\"\n, and \n\"wandb\"\n. Use \n\"all\"\n to report to all integrations installed, \n\"none\"\n\nfor no integrations.\n \n \n \nddp_find_unused_parameters\n (\nbool\n, \noptional\n) \u2014\nWhen using distributed training, the value of the flag \nfind_unused_parameters\n passed to\n\nDistributedDataParallel\n. Will default to \nFalse\n if gradient checkpointing is used, \nTrue\n otherwise.\n \n \n \nddp_bucket_cap_mb\n (\nint\n, \noptional\n) \u2014\nWhen using distributed training, the value of the flag \nbucket_cap_mb\n passed to \nDistributedDataParallel\n.\n \n \n \nddp_broadcast_buffers\n (\nbool\n, \noptional\n) \u2014\nWhen using distributed training, the value of the flag \nbroadcast_buffers\n passed to\n\nDistributedDataParallel\n. Will default to \nFalse\n if gradient checkpointing is used, \nTrue\n otherwise.\n \n \n \ndataloader_pin_memory\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether you want to pin memory in data loaders or not. Will default to \nTrue\n.\n \n \n \ndataloader_persistent_workers\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nIf True, the data loader will not shut down the worker processes after a dataset has been consumed once.\nThis allows to maintain the workers Dataset instances alive. Can potentially speed up training, but will\nincrease RAM usage. Will default to \nFalse\n.\n \n \n \ndataloader_prefetch_factor\n (\nint\n, \noptional\n) \u2014\nNumber of batches loaded in advance by each worker.\n2 means there will be a total of 2 * num_workers batches prefetched across all workers.\n \n \n \nskip_memory_metrics\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows\ndown the training and evaluation speed.\n \n \n \npush_to_hub\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to push the model to the Hub every time the model is saved. If this is activated,\n\noutput_dir\n will begin a git directory synced with the repo (determined by \nhub_model_id\n) and the content\nwill be pushed each time a save is triggered (depending on your \nsave_strategy\n). Calling\n\nsave_model()\n will also trigger a push.\n\n\n\n\nIf \noutput_dir\n exists, it needs to be a local clone of the repository to which the \nTrainer\n will be\npushed.\n\n\n \n \n \nresume_from_checkpoint\n (\nstr\n, \noptional\n) \u2014\nThe path to a folder with a valid checkpoint for your model. This argument is not directly used by\n\nTrainer\n, it\u2019s intended to be used by your training/evaluation scripts instead. See the \nexample\nscripts\n for more details.\n \n \n \nhub_model_id\n (\nstr\n, \noptional\n) \u2014\nThe name of the repository to keep in sync with the local \noutput_dir\n. It can be a simple model ID in\nwhich case the model will be pushed in your namespace. Otherwise it should be the whole repository name,\nfor instance \n\"user_name/model\"\n, which allows you to push to an organization you are a member of with\n\n\"organization_name/model\"\n. Will default to \nuser_name/output_dir_name\n with \noutput_dir_name\n being the\nname of \noutput_dir\n.\n\n\nWill default to the name of \noutput_dir\n.\n \n \n \nhub_strategy\n (\nstr\n or \nHubStrategy\n, \noptional\n, defaults to \n\"every_save\"\n) \u2014\nDefines the scope of what is pushed to the Hub and when. Possible values are:\n\n\n\n\n\"end\"\n: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the \nTrainer\n) and a\ndraft of a model card when the \nsave_model()\n method is called.\n\n\n\"every_save\"\n: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the \nTrainer\n) and\na draft of a model card each time there is a model save. The pushes are asynchronous to not block\ntraining, and in case the save are very frequent, a new push is only attempted if the previous one is\nfinished. A last push is made with the final model at the end of training.\n\n\n\"checkpoint\"\n: like \n\"every_save\"\n but the latest checkpoint is also pushed in a subfolder named\nlast-checkpoint, allowing you to resume training easily with\n\ntrainer.train(resume_from_checkpoint=\"last-checkpoint\")\n.\n\n\n\"all_checkpoints\"\n: like \n\"checkpoint\"\n but all checkpoints are pushed like they appear in the output\nfolder (so you will get one checkpoint folder per folder in your final repository)\n\n\n \n \n \nhub_token\n (\nstr\n, \noptional\n) \u2014\nThe token to use to push the model to the Hub. Will default to the token in the cache folder obtained with\n\nhuggingface-cli login\n.\n \n \n \nhub_private_repo\n (\nbool\n, \noptional\n) \u2014\nWhether to make the repo private. If \nNone\n (default), the repo will be public unless the organization\u2019s default is private. This value is ignored if the repo already exists.\n \n \n \nhub_always_push\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nUnless this is \nTrue\n, the \nTrainer\n will skip pushing a checkpoint when the previous push is not finished.\n \n \n \nhub_revision\n (\nstr\n, \noptional\n) \u2014\nThe revision to use when pushing to the Hub. Can be a branch name, a tag, or a commit hash.\n \n \n \ngradient_checkpointing\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nIf True, use gradient checkpointing to save memory at the expense of slower backward pass.\n \n \n \ngradient_checkpointing_kwargs\n (\ndict\n, \noptional\n, defaults to \nNone\n) \u2014\nKey word arguments to be passed to the \ngradient_checkpointing_enable\n method.\n \n \n \ninclude_inputs_for_metrics\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nThis argument is deprecated. Use \ninclude_for_metrics\n instead, e.g, \ninclude_for_metrics = [\"inputs\"]\n.\n \n \n \ninclude_for_metrics\n (\nlist[str]\n, \noptional\n, defaults to \n[]\n) \u2014\nInclude additional data in the \ncompute_metrics\n function if needed for metrics computation.\nPossible options to add to \ninclude_for_metrics\n list:\n\n\n\n\n\"inputs\"\n: Input data passed to the model, intended for calculating input dependent metrics.\n\n\n\"loss\"\n: Loss values computed during evaluation, intended for calculating loss dependent metrics.\n\n\n \n \n \neval_do_concat_batches\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether to recursively concat inputs/losses/labels/predictions across batches. If \nFalse\n,\nwill instead store them as lists, with each batch kept separate.\n \n \n \nauto_find_batch_size\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to find a batch size that will fit into memory automatically through exponential decay, avoiding\nCUDA Out-of-Memory errors. Requires accelerate to be installed (\npip install accelerate\n)\n \n \n \nfull_determinism\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nIf \nTrue\n, \nenable_full_determinism()\n is called instead of \nset_seed()\n to ensure reproducible results in\ndistributed training. Important: this will negatively impact the performance, so only use it for debugging.\n \n \n \ntorchdynamo\n (\nstr\n, \noptional\n) \u2014\nIf set, the backend compiler for TorchDynamo. Possible choices are \n\"eager\"\n, \n\"aot_eager\"\n, \n\"inductor\"\n,\n\n\"nvfuser\"\n, \n\"aot_nvfuser\"\n, \n\"aot_cudagraphs\"\n, \n\"ofi\"\n, \n\"fx2trt\"\n, \n\"onnxrt\"\n and \n\"ipex\"\n.\n \n \n \nray_scope\n (\nstr\n, \noptional\n, defaults to \n\"last\"\n) \u2014\nThe scope to use when doing hyperparameter search with Ray. By default, \n\"last\"\n will be used. Ray will\nthen use the last checkpoint of all trials, compare those, and select the best one. However, other options\nare also available. See the \nRay documentation\n for\nmore options.\n \n \n \nddp_timeout\n (\nint\n, \noptional\n, defaults to 1800) \u2014\nThe timeout for \ntorch.distributed.init_process_group\n calls, used to avoid GPU socket timeouts when\nperforming slow operations in distributed runnings. Please refer the [PyTorch documentation]\n(\nhttps://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group\n) for more\ninformation.\n \n \n \nuse_mps_device\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nThis argument is deprecated.\nmps\n device will be used if it is available similar to \ncuda\n device.\n \n \n \ntorch_compile\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to compile the model using PyTorch 2.0\n\ntorch.compile\n.\n\n\nThis will use the best defaults for the \ntorch.compile\n\nAPI\n.\nYou can customize the defaults with the argument \ntorch_compile_backend\n and \ntorch_compile_mode\n but we\ndon\u2019t guarantee any of them will work as the support is progressively rolled in in PyTorch.\n\n\nThis flag and the whole compile API is experimental and subject to change in future releases.\n \n \n \ntorch_compile_backend\n (\nstr\n, \noptional\n) \u2014\nThe backend to use in \ntorch.compile\n. If set to any value, \ntorch_compile\n will be set to \nTrue\n.\n\n\nRefer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n\n\nThis flag is experimental and subject to change in future releases.\n \n \n \ntorch_compile_mode\n (\nstr\n, \noptional\n) \u2014\nThe mode to use in \ntorch.compile\n. If set to any value, \ntorch_compile\n will be set to \nTrue\n.\n\n\nRefer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n\n\nThis flag is experimental and subject to change in future releases.\n \n \n \ninclude_tokens_per_second\n (\nbool\n, \noptional\n) \u2014\nWhether or not to compute the number of tokens per second per device for training speed metrics.\n\n\nThis will iterate over the entire training dataloader once beforehand,\n\n\nand will slow down the entire process.\n \n \n \ninclude_num_input_tokens_seen\n (\nbool\n, \noptional\n) \u2014\nWhether or not to track the number of input tokens seen throughout training.\n\n\nMay be slower in distributed training as gather operations must be called.\n \n \n \nneftune_noise_alpha\n (\nOptional[float]\n) \u2014\nIf not \nNone\n, this will activate NEFTune noise embeddings. This can drastically improve model performance\nfor instruction fine-tuning. Check out the \noriginal paper\n and the\n\noriginal code\n. Support transformers \nPreTrainedModel\n and also\n\nPeftModel\n from peft. The original paper used values in the range [5.0, 15.0].\n \n \n \noptim_target_modules\n (\nUnion[str, list[str]]\n, \noptional\n) \u2014\nThe target modules to optimize, i.e. the module names that you would like to train.\nCurrently used for the GaLore algorithm (\nhttps://huggingface.co/papers/2403.03507\n) and APOLLO algorithm (\nhttps://huggingface.co/papers/2412.05270\n).\nSee GaLore implementation (\nhttps://github.com/jiaweizzhao/GaLore\n) and APOLLO implementation (\nhttps://github.com/zhuhanqing/APOLLO\n) for more details.\nYou need to make sure to pass a valid GaLore or APOLLO optimizer, e.g., one of: \u201capollo_adamw\u201d, \u201cgalore_adamw\u201d, \u201cgalore_adamw_8bit\u201d, \u201cgalore_adafactor\u201d and make sure that the target modules are \nnn.Linear\n modules only.\n \n \n \nbatch_eval_metrics\n (\nOptional[bool]\n, defaults to \nFalse\n) \u2014\nIf set to \nTrue\n, evaluation will call compute_metrics at the end of each batch to accumulate statistics\nrather than saving all eval logits in memory. When set to \nTrue\n, you must pass a compute_metrics function\nthat takes a boolean argument \ncompute_result\n, which when passed \nTrue\n, will trigger the final global\nsummary statistics from the batch-level summary statistics you\u2019ve accumulated over the evaluation set.\n \n \n \neval_on_start\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to perform a evaluation step (sanity check) before the training to ensure the validation steps works correctly.\n \n \n \neval_use_gather_object\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to run recursively gather object in a nested list/tuple/dictionary of objects from all devices. This should only be enabled if users are not just returning tensors, and this is actively discouraged by PyTorch.\n \n \n \nuse_liger_kernel\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether enable \nLiger\n Kernel for LLM model training.\nIt can effectively increase multi-GPU training throughput by ~20% and reduces memory usage by ~60%, works out of the box with\nflash attention, PyTorch FSDP, and Microsoft DeepSpeed. Currently, it supports llama, mistral, mixtral and gemma models.\n \n \n \nliger_kernel_config\n (\nOptional[dict]\n, \noptional\n) \u2014\nConfiguration to be used for Liger Kernel. When use_liger_kernel=True, this dict is passed as keyword arguments to the\n\n_apply_liger_kernel_to_instance\n function, which specifies which kernels to apply. Available options vary by model but typically\ninclude: \u2018rope\u2019, \u2018swiglu\u2019, \u2018cross_entropy\u2019, \u2018fused_linear_cross_entropy\u2019, \u2018rms_norm\u2019, etc. If \nNone\n, use the default kernel configurations.\n \n \n \naverage_tokens_across_devices\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to average tokens across devices. If enabled, will use all_reduce to synchronize\nnum_tokens_in_batch for precise loss calculation. Reference:\n\nhttps://github.com/huggingface/transformers/issues/34242\n \n \n \n \nTrainingArguments is the subset of the arguments we use in our example scripts \nwhich relate to the training loop\nitself\n.\n \nUsing \nHfArgumentParser\n we can turn this class into\n\nargparse\n arguments that can be specified on the\ncommand line.\n \n \nget_process_log_level\n \n \n<\n \nsource\n \n>\n \n(\n \n)\n \n \n \n \nReturns the log level to be used depending on whether this process is the main process of node 0, main process\nof node non-0, or a non-main process.\n \nFor the main process the log level defaults to the logging level set (\nlogging.WARNING\n if you didn\u2019t do\nanything) unless overridden by \nlog_level\n argument.\n \nFor the replica processes the log level defaults to \nlogging.WARNING\n unless overridden by \nlog_level_replica\n\nargument.\n \nThe choice between the main and replica process settings is made according to the return value of \nshould_log\n.\n \n \nget_warmup_steps\n \n \n<\n \nsource\n \n>\n \n(\n \nnum_training_steps\n: int\n \n \n)\n \n \n \n \nGet number of steps used for a linear warmup.\n \n \nmain_process_first\n \n \n<\n \nsource\n \n>\n \n(\n \nlocal\n = True\n \ndesc\n = 'work'\n \n \n)\n \n \n \nParameters \n \n \nlocal\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nif \nTrue\n first means process of rank 0 of each node if \nFalse\n first means process of rank 0 of node\nrank 0 In multi-node environment with a shared filesystem you most likely will want to use\n\nlocal=False\n so that only the main process of the first node will do the processing. If however, the\nfilesystem is not shared, then the main process of each node will need to do the processing, which is\nthe default behavior.\n \n \n \ndesc\n (\nstr\n, \noptional\n, defaults to \n\"work\"\n) \u2014\na work description to be used in debug logs\n \n \n \n \nA context manager for torch distributed environment where on needs to do something on the main process, while\nblocking replicas, and when it\u2019s finished releasing the replicas.\n \nOne such use is for \ndatasets\n\u2019s \nmap\n feature which to be efficient should be run once on the main process,\nwhich upon completion saves a cached version of results and which then automatically gets loaded by the\nreplicas.\n \n \nset_dataloader\n \n \n<\n \nsource\n \n>\n \n(\n \ntrain_batch_size\n: int = 8\n \neval_batch_size\n: int = 8\n \ndrop_last\n: bool = False\n \nnum_workers\n: int = 0\n \npin_memory\n: bool = True\n \npersistent_workers\n: bool = False\n \nprefetch_factor\n: typing.Optional[int] = None\n \nauto_find_batch_size\n: bool = False\n \nignore_data_skip\n: bool = False\n \nsampler_seed\n: typing.Optional[int] = None\n \n \n)\n \n \n \nParameters \n \n \ndrop_last\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch\nsize) or not.\n \n \n \nnum_workers\n (\nint\n, \noptional\n, defaults to 0) \u2014\nNumber of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in\nthe main process.\n \n \n \npin_memory\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether you want to pin memory in data loaders or not. Will default to \nTrue\n.\n \n \n \npersistent_workers\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nIf True, the data loader will not shut down the worker processes after a dataset has been consumed\nonce. This allows to maintain the workers Dataset instances alive. Can potentially speed up training,\nbut will increase RAM usage. Will default to \nFalse\n.\n \n \n \nprefetch_factor\n (\nint\n, \noptional\n) \u2014\nNumber of batches loaded in advance by each worker.\n2 means there will be a total of 2 * num_workers batches prefetched across all workers.\n \n \n \nauto_find_batch_size\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to find a batch size that will fit into memory automatically through exponential decay,\navoiding CUDA Out-of-Memory errors. Requires accelerate to be installed (\npip install accelerate\n)\n \n \n \nignore_data_skip\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhen resuming training, whether or not to skip the epochs and batches to get the data loading at the\nsame stage as in the previous training. If set to \nTrue\n, the training will begin faster (as that\nskipping step can take a long time) but will not yield the same results as the interrupted training\nwould have.\n \n \n \nsampler_seed\n (\nint\n, \noptional\n) \u2014\nRandom seed to be used with data samplers. If not set, random generators for data sampling will use the\nsame seed as \nself.seed\n. This can be used to ensure reproducibility of data sampling, independent of\nthe model seed.\n \n \n \n \nA method that regroups all arguments linked to the dataloaders creation.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n TrainingArguments\n\n\n>>> \nargs = TrainingArguments(\n\"working_dir\"\n)\n\n>>> \nargs = args.set_dataloader(train_batch_size=\n16\n, eval_batch_size=\n64\n)\n\n>>> \nargs.per_device_train_batch_size\n\n16\n \n \nset_evaluate\n \n \n<\n \nsource\n \n>\n \n(\n \nstrategy\n: typing.Union[str, transformers.trainer_utils.IntervalStrategy] = 'no'\n \nsteps\n: int = 500\n \nbatch_size\n: int = 8\n \naccumulation_steps\n: typing.Optional[int] = None\n \ndelay\n: typing.Optional[float] = None\n \nloss_only\n: bool = False\n \njit_mode\n: bool = False\n \n \n)\n \n \n \nParameters \n \n \nstrategy\n (\nstr\n or \nIntervalStrategy\n, \noptional\n, defaults to \n\"no\"\n) \u2014\nThe evaluation strategy to adopt during training. Possible values are:\n\n\n\n\n\"no\"\n: No evaluation is done during training.\n\n\n\"steps\"\n: Evaluation is done (and logged) every \nsteps\n.\n\n\n\"epoch\"\n: Evaluation is done at the end of each epoch.\n\n\n\n\nSetting a \nstrategy\n different from \n\"no\"\n will set \nself.do_eval\n to \nTrue\n.\n \n \n \nsteps\n (\nint\n, \noptional\n, defaults to 500) \u2014\nNumber of update steps between two evaluations if \nstrategy=\"steps\"\n.\n \n \n \nbatch_size\n (\nint\n \noptional\n, defaults to 8) \u2014\nThe batch size per device (GPU/TPU core/CPU\u2026) used for evaluation.\n \n \n \naccumulation_steps\n (\nint\n, \noptional\n) \u2014\nNumber of predictions steps to accumulate the output tensors for, before moving the results to the CPU.\nIf left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster\nbut requires more memory).\n \n \n \ndelay\n (\nfloat\n, \noptional\n) \u2014\nNumber of epochs or steps to wait for before the first evaluation can be performed, depending on the\neval_strategy.\n \n \n \nloss_only\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nIgnores all outputs except the loss.\n \n \n \njit_mode\n (\nbool\n, \noptional\n) \u2014\nWhether or not to use PyTorch jit trace for inference.\n \n \n \n \nA method that regroups all arguments linked to evaluation.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n TrainingArguments\n\n\n>>> \nargs = TrainingArguments(\n\"working_dir\"\n)\n\n>>> \nargs = args.set_evaluate(strategy=\n\"steps\"\n, steps=\n100\n)\n\n>>> \nargs.eval_steps\n\n100\n \n \nset_logging\n \n \n<\n \nsource\n \n>\n \n(\n \nstrategy\n: typing.Union[str, transformers.trainer_utils.IntervalStrategy] = 'steps'\n \nsteps\n: int = 500\n \nreport_to\n: typing.Union[str, list[str]] = 'none'\n \nlevel\n: str = 'passive'\n \nfirst_step\n: bool = False\n \nnan_inf_filter\n: bool = False\n \non_each_node\n: bool = False\n \nreplica_level\n: str = 'passive'\n \n \n)\n \n \n \nParameters \n \n \nstrategy\n (\nstr\n or \nIntervalStrategy\n, \noptional\n, defaults to \n\"steps\"\n) \u2014\nThe logging strategy to adopt during training. Possible values are:\n\n\n\n\n\"no\"\n: No logging is done during training.\n\n\n\"epoch\"\n: Logging is done at the end of each epoch.\n\n\n\"steps\"\n: Logging is done every \nlogging_steps\n.\n\n\n \n \n \nsteps\n (\nint\n, \noptional\n, defaults to 500) \u2014\nNumber of update steps between two logs if \nstrategy=\"steps\"\n.\n \n \n \nlevel\n (\nstr\n, \noptional\n, defaults to \n\"passive\"\n) \u2014\nLogger log level to use on the main process. Possible choices are the log levels as strings: \n\"debug\"\n,\n\n\"info\"\n, \n\"warning\"\n, \n\"error\"\n and \n\"critical\"\n, plus a \n\"passive\"\n level which doesn\u2019t set anything\nand lets the application set the level.\n \n \n \nreport_to\n (\nstr\n or \nlist[str]\n, \noptional\n, defaults to \n\"all\"\n) \u2014\nThe list of integrations to report the results and logs to. Supported platforms are \n\"azure_ml\"\n,\n\n\"clearml\"\n, \n\"codecarbon\"\n, \n\"comet_ml\"\n, \n\"dagshub\"\n, \n\"dvclive\"\n, \n\"flyte\"\n, \n\"mlflow\"\n,\n\n\"neptune\"\n, \n\"swanlab\"\n, \n\"tensorboard\"\n, and \n\"wandb\"\n. Use \n\"all\"\n to report to all integrations\ninstalled, \n\"none\"\n for no integrations.\n \n \n \nfirst_step\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to log and evaluate the first \nglobal_step\n or not.\n \n \n \nnan_inf_filter\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether to filter \nnan\n and \ninf\n losses for logging. If set to \nTrue\n the loss of every step that is\n\nnan\n or \ninf\n is filtered and the average loss of the current logging window is taken instead.\n\n\n\n\nnan_inf_filter\n only influences the logging of loss values, it does not change the behavior the\ngradient is computed or applied to the model.\n\n\n \n \n \non_each_node\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nIn multinode distributed training, whether to log using \nlog_level\n once per node, or only on the main\nnode.\n \n \n \nreplica_level\n (\nstr\n, \noptional\n, defaults to \n\"passive\"\n) \u2014\nLogger log level to use on replicas. Same choices as \nlog_level\n \n \n \n \nA method that regroups all arguments linked to logging.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n TrainingArguments\n\n\n>>> \nargs = TrainingArguments(\n\"working_dir\"\n)\n\n>>> \nargs = args.set_logging(strategy=\n\"steps\"\n, steps=\n100\n)\n\n>>> \nargs.logging_steps\n\n100\n \n \nset_lr_scheduler\n \n \n<\n \nsource\n \n>\n \n(\n \nname\n: typing.Union[str, transformers.trainer_utils.SchedulerType] = 'linear'\n \nnum_epochs\n: float = 3.0\n \nmax_steps\n: int = -1\n \nwarmup_ratio\n: float = 0\n \nwarmup_steps\n: int = 0\n \n \n)\n \n \n \nParameters \n \n \nname\n (\nstr\n or \nSchedulerType\n, \noptional\n, defaults to \n\"linear\"\n) \u2014\nThe scheduler type to use. See the documentation of \nSchedulerType\n for all possible values.\n \n \n \nnum_epochs(\nfloat\n,\n \noptional\n, defaults to 3.0) \u2014\nTotal number of training epochs to perform (if not an integer, will perform the decimal part percents\nof the last epoch before stopping training).\n \n \n \nmax_steps\n (\nint\n, \noptional\n, defaults to -1) \u2014\nIf set to a positive number, the total number of training steps to perform. Overrides \nnum_train_epochs\n.\nFor a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n\nmax_steps\n is reached.\n \n \n \nwarmup_ratio\n (\nfloat\n, \noptional\n, defaults to 0.0) \u2014\nRatio of total training steps used for a linear warmup from 0 to \nlearning_rate\n.\n \n \n \nwarmup_steps\n (\nint\n, \noptional\n, defaults to 0) \u2014\nNumber of steps used for a linear warmup from 0 to \nlearning_rate\n. Overrides any effect of\n\nwarmup_ratio\n.\n \n \n \n \nA method that regroups all arguments linked to the learning rate scheduler and its hyperparameters.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n TrainingArguments\n\n\n>>> \nargs = TrainingArguments(\n\"working_dir\"\n)\n\n>>> \nargs = args.set_lr_scheduler(name=\n\"cosine\"\n, warmup_ratio=\n0.05\n)\n\n>>> \nargs.warmup_ratio\n\n0.05\n \n \nset_optimizer\n \n \n<\n \nsource\n \n>\n \n(\n \nname\n: typing.Union[str, transformers.training_args.OptimizerNames] = 'adamw_torch'\n \nlearning_rate\n: float = 5e-05\n \nweight_decay\n: float = 0\n \nbeta1\n: float = 0.9\n \nbeta2\n: float = 0.999\n \nepsilon\n: float = 1e-08\n \nargs\n: typing.Optional[str] = None\n \n \n)\n \n \n \nParameters \n \n \nname\n (\nstr\n or \ntraining_args.OptimizerNames\n, \noptional\n, defaults to \n\"adamw_torch\"\n) \u2014\nThe optimizer to use: \n\"adamw_torch\"\n, \n\"adamw_torch_fused\"\n, \n\"adamw_apex_fused\"\n,\n\n\"adamw_anyprecision\"\n or \n\"adafactor\"\n.\n \n \n \nlearning_rate\n (\nfloat\n, \noptional\n, defaults to 5e-5) \u2014\nThe initial learning rate.\n \n \n \nweight_decay\n (\nfloat\n, \noptional\n, defaults to 0) \u2014\nThe weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights.\n \n \n \nbeta1\n (\nfloat\n, \noptional\n, defaults to 0.9) \u2014\nThe beta1 hyperparameter for the adam optimizer or its variants.\n \n \n \nbeta2\n (\nfloat\n, \noptional\n, defaults to 0.999) \u2014\nThe beta2 hyperparameter for the adam optimizer or its variants.\n \n \n \nepsilon\n (\nfloat\n, \noptional\n, defaults to 1e-8) \u2014\nThe epsilon hyperparameter for the adam optimizer or its variants.\n \n \n \nargs\n (\nstr\n, \noptional\n) \u2014\nOptional arguments that are supplied to AnyPrecisionAdamW (only useful when\n\noptim=\"adamw_anyprecision\"\n).\n \n \n \n \nA method that regroups all arguments linked to the optimizer and its hyperparameters.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n TrainingArguments\n\n\n>>> \nargs = TrainingArguments(\n\"working_dir\"\n)\n\n>>> \nargs = args.set_optimizer(name=\n\"adamw_torch\"\n, beta1=\n0.8\n)\n\n>>> \nargs.optim\n\n'adamw_torch'\n \n \nset_push_to_hub\n \n \n<\n \nsource\n \n>\n \n(\n \nmodel_id\n: str\n \nstrategy\n: typing.Union[str, transformers.trainer_utils.HubStrategy] = 'every_save'\n \ntoken\n: typing.Optional[str] = None\n \nprivate_repo\n: typing.Optional[bool] = None\n \nalways_push\n: bool = False\n \nrevision\n: typing.Optional[str] = None\n \n \n)\n \n \n \nParameters \n \n \nmodel_id\n (\nstr\n) \u2014\nThe name of the repository to keep in sync with the local \noutput_dir\n. It can be a simple model ID in\nwhich case the model will be pushed in your namespace. Otherwise it should be the whole repository\nname, for instance \n\"user_name/model\"\n, which allows you to push to an organization you are a member of\nwith \n\"organization_name/model\"\n.\n \n \n \nstrategy\n (\nstr\n or \nHubStrategy\n, \noptional\n, defaults to \n\"every_save\"\n) \u2014\nDefines the scope of what is pushed to the Hub and when. Possible values are:\n\n\n\n\n\"end\"\n: push the model, its configuration, the processing_class e.g. tokenizer (if passed along to the \nTrainer\n) and a\ndraft of a model card when the \nsave_model()\n method is called.\n\n\n\"every_save\"\n: push the model, its configuration, the processing_class e.g. tokenizer (if passed along to the \nTrainer\n)\nand\na draft of a model card each time there is a model save. The pushes are asynchronous to not block\ntraining, and in case the save are very frequent, a new push is only attempted if the previous one is\nfinished. A last push is made with the final model at the end of training.\n\n\n\"checkpoint\"\n: like \n\"every_save\"\n but the latest checkpoint is also pushed in a subfolder named\nlast-checkpoint, allowing you to resume training easily with\n\ntrainer.train(resume_from_checkpoint=\"last-checkpoint\")\n.\n\n\n\"all_checkpoints\"\n: like \n\"checkpoint\"\n but all checkpoints are pushed like they appear in the\noutput\nfolder (so you will get one checkpoint folder per folder in your final repository)\n\n\n \n \n \ntoken\n (\nstr\n, \noptional\n) \u2014\nThe token to use to push the model to the Hub. Will default to the token in the cache folder obtained\nwith \nhuggingface-cli login\n.\n \n \n \nprivate_repo\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to make the repo private. If \nNone\n (default), the repo will be public unless the organization\u2019s default is private. This value is ignored if the repo already exists.\n \n \n \nalways_push\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nUnless this is \nTrue\n, the \nTrainer\n will skip pushing a checkpoint when the previous push is not\nfinished.\n \n \n \nrevision\n (\nstr\n, \noptional\n) \u2014\nThe revision to use when pushing to the Hub. Can be a branch name, a tag, or a commit hash.\n \n \n \n \nA method that regroups all arguments linked to synchronizing checkpoints with the Hub.\n \nCalling this method will set \nself.push_to_hub\n to \nTrue\n, which means the \noutput_dir\n will begin a git\ndirectory synced with the repo (determined by \nmodel_id\n) and the content will be pushed each time a save is\ntriggered (depending on your \nself.save_strategy\n). Calling \nsave_model()\n will also trigger a push.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n TrainingArguments\n\n\n>>> \nargs = TrainingArguments(\n\"working_dir\"\n)\n\n>>> \nargs = args.set_push_to_hub(\n\"me/awesome-model\"\n)\n\n>>> \nargs.hub_model_id\n\n'me/awesome-model'\n \n \nset_save\n \n \n<\n \nsource\n \n>\n \n(\n \nstrategy\n: typing.Union[str, transformers.trainer_utils.IntervalStrategy] = 'steps'\n \nsteps\n: int = 500\n \ntotal_limit\n: typing.Optional[int] = None\n \non_each_node\n: bool = False\n \n \n)\n \n \n \nParameters \n \n \nstrategy\n (\nstr\n or \nIntervalStrategy\n, \noptional\n, defaults to \n\"steps\"\n) \u2014\nThe checkpoint save strategy to adopt during training. Possible values are:\n\n\n\n\n\"no\"\n: No save is done during training.\n\n\n\"epoch\"\n: Save is done at the end of each epoch.\n\n\n\"steps\"\n: Save is done every \nsave_steps\n.\n\n\n \n \n \nsteps\n (\nint\n, \noptional\n, defaults to 500) \u2014\nNumber of updates steps before two checkpoint saves if \nstrategy=\"steps\"\n.\n \n \n \ntotal_limit\n (\nint\n, \noptional\n) \u2014\nIf a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n\noutput_dir\n.\n \n \n \non_each_node\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhen doing multi-node distributed training, whether to save models and checkpoints on each node, or\nonly on the main one.\n\n\nThis should not be activated when the different nodes use the same storage as the files will be saved\nwith the same names for each node.\n \n \n \n \nA method that regroups all arguments linked to checkpoint saving.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n TrainingArguments\n\n\n>>> \nargs = TrainingArguments(\n\"working_dir\"\n)\n\n>>> \nargs = args.set_save(strategy=\n\"steps\"\n, steps=\n100\n)\n\n>>> \nargs.save_steps\n\n100\n \n \nset_testing\n \n \n<\n \nsource\n \n>\n \n(\n \nbatch_size\n: int = 8\n \nloss_only\n: bool = False\n \njit_mode\n: bool = False\n \n \n)\n \n \n \nParameters \n \n \nbatch_size\n (\nint\n \noptional\n, defaults to 8) \u2014\nThe batch size per device (GPU/TPU core/CPU\u2026) used for testing.\n \n \n \nloss_only\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nIgnores all outputs except the loss.\n \n \n \njit_mode\n (\nbool\n, \noptional\n) \u2014\nWhether or not to use PyTorch jit trace for inference.\n \n \n \n \nA method that regroups all basic arguments linked to testing on a held-out dataset.\n \nCalling this method will automatically set \nself.do_predict\n to \nTrue\n.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n TrainingArguments\n\n\n>>> \nargs = TrainingArguments(\n\"working_dir\"\n)\n\n>>> \nargs = args.set_testing(batch_size=\n32\n)\n\n>>> \nargs.per_device_eval_batch_size\n\n32\n \n \nset_training\n \n \n<\n \nsource\n \n>\n \n(\n \nlearning_rate\n: float = 5e-05\n \nbatch_size\n: int = 8\n \nweight_decay\n: float = 0\n \nnum_epochs\n: float = 3\n \nmax_steps\n: int = -1\n \ngradient_accumulation_steps\n: int = 1\n \nseed\n: int = 42\n \ngradient_checkpointing\n: bool = False\n \n \n)\n \n \n \nParameters \n \n \nlearning_rate\n (\nfloat\n, \noptional\n, defaults to 5e-5) \u2014\nThe initial learning rate for the optimizer.\n \n \n \nbatch_size\n (\nint\n \noptional\n, defaults to 8) \u2014\nThe batch size per device (GPU/TPU core/CPU\u2026) used for training.\n \n \n \nweight_decay\n (\nfloat\n, \noptional\n, defaults to 0) \u2014\nThe weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in the\noptimizer.\n \n \n \nnum_train_epochs(\nfloat\n,\n \noptional\n, defaults to 3.0) \u2014\nTotal number of training epochs to perform (if not an integer, will perform the decimal part percents\nof the last epoch before stopping training).\n \n \n \nmax_steps\n (\nint\n, \noptional\n, defaults to -1) \u2014\nIf set to a positive number, the total number of training steps to perform. Overrides \nnum_train_epochs\n.\nFor a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n\nmax_steps\n is reached.\n \n \n \ngradient_accumulation_steps\n (\nint\n, \noptional\n, defaults to 1) \u2014\nNumber of updates steps to accumulate the gradients for, before performing a backward/update pass.\n\n\n\n\nWhen using gradient accumulation, one step is counted as one step with backward pass. Therefore,\nlogging, evaluation, save will be conducted every \ngradient_accumulation_steps * xxx_step\n training\nexamples.\n\n\n \n \n \nseed\n (\nint\n, \noptional\n, defaults to 42) \u2014\nRandom seed that will be set at the beginning of training. To ensure reproducibility across runs, use\nthe \n~Trainer.model_init\n function to instantiate the model if it has some randomly initialized\nparameters.\n \n \n \ngradient_checkpointing\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nIf True, use gradient checkpointing to save memory at the expense of slower backward pass.\n \n \n \n \nA method that regroups all basic arguments linked to the training.\n \nCalling this method will automatically set \nself.do_train\n to \nTrue\n.\n \n \nExample:\n \n \n Copied\n \n>>> \nfrom\n transformers \nimport\n TrainingArguments\n\n\n>>> \nargs = TrainingArguments(\n\"working_dir\"\n)\n\n>>> \nargs = args.set_training(learning_rate=\n1e-4\n, batch_size=\n32\n)\n\n>>> \nargs.learning_rate\n\n1e-4\n \n \nto_dict\n \n \n<\n \nsource\n \n>\n \n(\n \n)\n \n \n \n \nSerializes this instance while replace \nEnum\n by their values (for JSON serialization support). It obfuscates\nthe token values by removing their value.\n \n \nto_json_string\n \n \n<\n \nsource\n \n>\n \n(\n \n)\n \n \n \n \nSerializes this instance to a JSON string.\n \n \nto_sanitized_dict\n \n \n<\n \nsource\n \n>\n \n(\n \n)\n \n \n \n \nSanitized serialization to use with TensorBoard\u2019s hparams\n \n \nSeq2SeqTrainingArguments\n \n \nclass\n \ntransformers.\nSeq2SeqTrainingArguments\n \n \n<\n \nsource\n \n>\n \n(\n \noutput_dir\n: typing.Optional[str] = None\n \noverwrite_output_dir\n: bool = False\n \ndo_train\n: bool = False\n \ndo_eval\n: bool = False\n \ndo_predict\n: bool = False\n \neval_strategy\n: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'no'\n \nprediction_loss_only\n: bool = False\n \nper_device_train_batch_size\n: int = 8\n \nper_device_eval_batch_size\n: int = 8\n \nper_gpu_train_batch_size\n: typing.Optional[int] = None\n \nper_gpu_eval_batch_size\n: typing.Optional[int] = None\n \ngradient_accumulation_steps\n: int = 1\n \neval_accumulation_steps\n: typing.Optional[int] = None\n \neval_delay\n: typing.Optional[float] = 0\n \ntorch_empty_cache_steps\n: typing.Optional[int] = None\n \nlearning_rate\n: float = 5e-05\n \nweight_decay\n: float = 0.0\n \nadam_beta1\n: float = 0.9\n \nadam_beta2\n: float = 0.999\n \nadam_epsilon\n: float = 1e-08\n \nmax_grad_norm\n: float = 1.0\n \nnum_train_epochs\n: float = 3.0\n \nmax_steps\n: int = -1\n \nlr_scheduler_type\n: typing.Union[transformers.trainer_utils.SchedulerType, str] = 'linear'\n \nlr_scheduler_kwargs\n: typing.Union[dict[str, typing.Any], str, NoneType] = <factory>\n \nwarmup_ratio\n: float = 0.0\n \nwarmup_steps\n: int = 0\n \nlog_level\n: str = 'passive'\n \nlog_level_replica\n: str = 'warning'\n \nlog_on_each_node\n: bool = True\n \nlogging_dir\n: typing.Optional[str] = None\n \nlogging_strategy\n: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'\n \nlogging_first_step\n: bool = False\n \nlogging_steps\n: float = 500\n \nlogging_nan_inf_filter\n: bool = True\n \nsave_strategy\n: typing.Union[transformers.trainer_utils.SaveStrategy, str] = 'steps'\n \nsave_steps\n: float = 500\n \nsave_total_limit\n: typing.Optional[int] = None\n \nsave_safetensors\n: typing.Optional[bool] = True\n \nsave_on_each_node\n: bool = False\n \nsave_only_model\n: bool = False\n \nrestore_callback_states_from_checkpoint\n: bool = False\n \nno_cuda\n: bool = False\n \nuse_cpu\n: bool = False\n \nuse_mps_device\n: bool = False\n \nseed\n: int = 42\n \ndata_seed\n: typing.Optional[int] = None\n \njit_mode_eval\n: bool = False\n \nuse_ipex\n: bool = False\n \nbf16\n: bool = False\n \nfp16\n: bool = False\n \nfp16_opt_level\n: str = 'O1'\n \nhalf_precision_backend\n: str = 'auto'\n \nbf16_full_eval\n: bool = False\n \nfp16_full_eval\n: bool = False\n \ntf32\n: typing.Optional[bool] = None\n \nlocal_rank\n: int = -1\n \nddp_backend\n: typing.Optional[str] = None\n \ntpu_num_cores\n: typing.Optional[int] = None\n \ntpu_metrics_debug\n: bool = False\n \ndebug\n: typing.Union[str, list[transformers.debug_utils.DebugOption]] = ''\n \ndataloader_drop_last\n: bool = False\n \neval_steps\n: typing.Optional[float] = None\n \ndataloader_num_workers\n: int = 0\n \ndataloader_prefetch_factor\n: typing.Optional[int] = None\n \npast_index\n: int = -1\n \nrun_name\n: typing.Optional[str] = None\n \ndisable_tqdm\n: typing.Optional[bool] = None\n \nremove_unused_columns\n: typing.Optional[bool] = True\n \nlabel_names\n: typing.Optional[list[str]] = None\n \nload_best_model_at_end\n: typing.Optional[bool] = False\n \nmetric_for_best_model\n: typing.Optional[str] = None\n \ngreater_is_better\n: typing.Optional[bool] = None\n \nignore_data_skip\n: bool = False\n \nfsdp\n: typing.Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = ''\n \nfsdp_min_num_params\n: int = 0\n \nfsdp_config\n: typing.Union[dict[str, typing.Any], str, NoneType] = None\n \nfsdp_transformer_layer_cls_to_wrap\n: typing.Optional[str] = None\n \naccelerator_config\n: typing.Union[dict, str, NoneType] = None\n \ndeepspeed\n: typing.Union[dict, str, NoneType] = None\n \nlabel_smoothing_factor\n: float = 0.0\n \noptim\n: typing.Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch'\n \noptim_args\n: typing.Optional[str] = None\n \nadafactor\n: bool = False\n \ngroup_by_length\n: bool = False\n \nlength_column_name\n: typing.Optional[str] = 'length'\n \nreport_to\n: typing.Union[NoneType, str, list[str]] = None\n \nddp_find_unused_parameters\n: typing.Optional[bool] = None\n \nddp_bucket_cap_mb\n: typing.Optional[int] = None\n \nddp_broadcast_buffers\n: typing.Optional[bool] = None\n \ndataloader_pin_memory\n: bool = True\n \ndataloader_persistent_workers\n: bool = False\n \nskip_memory_metrics\n: bool = True\n \nuse_legacy_prediction_loop\n: bool = False\n \npush_to_hub\n: bool = False\n \nresume_from_checkpoint\n: typing.Optional[str] = None\n \nhub_model_id\n: typing.Optional[str] = None\n \nhub_strategy\n: typing.Union[transformers.trainer_utils.HubStrategy, str] = 'every_save'\n \nhub_token\n: typing.Optional[str] = None\n \nhub_private_repo\n: typing.Optional[bool] = None\n \nhub_always_push\n: bool = False\n \nhub_revision\n: typing.Optional[str] = None\n \ngradient_checkpointing\n: bool = False\n \ngradient_checkpointing_kwargs\n: typing.Union[dict[str, typing.Any], str, NoneType] = None\n \ninclude_inputs_for_metrics\n: bool = False\n \ninclude_for_metrics\n: list = <factory>\n \neval_do_concat_batches\n: bool = True\n \nfp16_backend\n: str = 'auto'\n \npush_to_hub_model_id\n: typing.Optional[str] = None\n \npush_to_hub_organization\n: typing.Optional[str] = None\n \npush_to_hub_token\n: typing.Optional[str] = None\n \nmp_parameters\n: str = ''\n \nauto_find_batch_size\n: bool = False\n \nfull_determinism\n: bool = False\n \ntorchdynamo\n: typing.Optional[str] = None\n \nray_scope\n: typing.Optional[str] = 'last'\n \nddp_timeout\n: int = 1800\n \ntorch_compile\n: bool = False\n \ntorch_compile_backend\n: typing.Optional[str] = None\n \ntorch_compile_mode\n: typing.Optional[str] = None\n \ninclude_tokens_per_second\n: typing.Optional[bool] = False\n \ninclude_num_input_tokens_seen\n: typing.Optional[bool] = False\n \nneftune_noise_alpha\n: typing.Optional[float] = None\n \noptim_target_modules\n: typing.Union[NoneType, str, list[str]] = None\n \nbatch_eval_metrics\n: bool = False\n \neval_on_start\n: bool = False\n \nuse_liger_kernel\n: typing.Optional[bool] = False\n \nliger_kernel_config\n: typing.Optional[dict[str, bool]] = None\n \neval_use_gather_object\n: typing.Optional[bool] = False\n \naverage_tokens_across_devices\n: typing.Optional[bool] = False\n \nsortish_sampler\n: bool = False\n \npredict_with_generate\n: bool = False\n \ngeneration_max_length\n: typing.Optional[int] = None\n \ngeneration_num_beams\n: typing.Optional[int] = None\n \ngeneration_config\n: typing.Union[str, pathlib.Path, transformers.generation.configuration_utils.GenerationConfig, NoneType] = None\n \n \n)\n \n \n \nParameters \n \n \noutput_dir\n (\nstr\n, \noptional\n, defaults to \n\"trainer_output\"\n) \u2014\nThe output directory where the model predictions and checkpoints will be written.\n \n \n \noverwrite_output_dir\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nIf \nTrue\n, overwrite the content of the output directory. Use this to continue training if \noutput_dir\n\npoints to a checkpoint directory.\n \n \n \ndo_train\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to run training or not. This argument is not directly used by \nTrainer\n, it\u2019s intended to be used\nby your training/evaluation scripts instead. See the \nexample\nscripts\n for more details.\n \n \n \ndo_eval\n (\nbool\n, \noptional\n) \u2014\nWhether to run evaluation on the validation set or not. Will be set to \nTrue\n if \neval_strategy\n is\ndifferent from \n\"no\"\n. This argument is not directly used by \nTrainer\n, it\u2019s intended to be used by your\ntraining/evaluation scripts instead. See the \nexample\nscripts\n for more details.\n \n \n \ndo_predict\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to run predictions on the test set or not. This argument is not directly used by \nTrainer\n, it\u2019s\nintended to be used by your training/evaluation scripts instead. See the \nexample\nscripts\n for more details.\n \n \n \neval_strategy\n (\nstr\n or \nIntervalStrategy\n, \noptional\n, defaults to \n\"no\"\n) \u2014\nThe evaluation strategy to adopt during training. Possible values are:\n\n\n\n\n\"no\"\n: No evaluation is done during training.\n\n\n\"steps\"\n: Evaluation is done (and logged) every \neval_steps\n.\n\n\n\"epoch\"\n: Evaluation is done at the end of each epoch.\n\n\n \n \n \nprediction_loss_only\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhen performing evaluation and generating predictions, only returns the loss.\n \n \n \nper_device_train_batch_size\n (\nint\n, \noptional\n, defaults to 8) \u2014\nThe batch size per device accelerator core/CPU for training.\n \n \n \nper_device_eval_batch_size\n (\nint\n, \noptional\n, defaults to 8) \u2014\nThe batch size per device accelerator core/CPU for evaluation.\n \n \n \ngradient_accumulation_steps\n (\nint\n, \noptional\n, defaults to 1) \u2014\nNumber of updates steps to accumulate the gradients for, before performing a backward/update pass.\n\n\n\n\nWhen using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,\nevaluation, save will be conducted every \ngradient_accumulation_steps * xxx_step\n training examples.\n\n\n \n \n \neval_accumulation_steps\n (\nint\n, \noptional\n) \u2014\nNumber of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\nleft unset, the whole predictions are accumulated on the device accelerator before being moved to the CPU (faster but\nrequires more memory).\n \n \n \neval_delay\n (\nfloat\n, \noptional\n) \u2014\nNumber of epochs or steps to wait for before the first evaluation can be performed, depending on the\neval_strategy.\n \n \n \ntorch_empty_cache_steps\n (\nint\n, \noptional\n) \u2014\nNumber of steps to wait before calling \ntorch.<device>.empty_cache()\n. If left unset or set to None, cache will not be emptied.\n\n\n\n\nThis can help avoid CUDA out-of-memory errors by lowering peak VRAM usage at a cost of about \n10% slower performance\n.\n\n\n \n \n \nlearning_rate\n (\nfloat\n, \noptional\n, defaults to 5e-5) \u2014\nThe initial learning rate for \nAdamW\n optimizer.\n \n \n \nweight_decay\n (\nfloat\n, \noptional\n, defaults to 0) \u2014\nThe weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in \nAdamW\n\noptimizer.\n \n \n \nadam_beta1\n (\nfloat\n, \noptional\n, defaults to 0.9) \u2014\nThe beta1 hyperparameter for the \nAdamW\n optimizer.\n \n \n \nadam_beta2\n (\nfloat\n, \noptional\n, defaults to 0.999) \u2014\nThe beta2 hyperparameter for the \nAdamW\n optimizer.\n \n \n \nadam_epsilon\n (\nfloat\n, \noptional\n, defaults to 1e-8) \u2014\nThe epsilon hyperparameter for the \nAdamW\n optimizer.\n \n \n \nmax_grad_norm\n (\nfloat\n, \noptional\n, defaults to 1.0) \u2014\nMaximum gradient norm (for gradient clipping).\n \n \n \nnum_train_epochs(\nfloat\n,\n \noptional\n, defaults to 3.0) \u2014\nTotal number of training epochs to perform (if not an integer, will perform the decimal part percents of\nthe last epoch before stopping training).\n \n \n \nmax_steps\n (\nint\n, \noptional\n, defaults to -1) \u2014\nIf set to a positive number, the total number of training steps to perform. Overrides \nnum_train_epochs\n.\nFor a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n\nmax_steps\n is reached.\n \n \n \nlr_scheduler_type\n (\nstr\n or \nSchedulerType\n, \noptional\n, defaults to \n\"linear\"\n) \u2014\nThe scheduler type to use. See the documentation of \nSchedulerType\n for all possible values.\n \n \n \nlr_scheduler_kwargs\n (\u2018dict\u2019, \noptional\n, defaults to {}) \u2014\nThe extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values.\n \n \n \nwarmup_ratio\n (\nfloat\n, \noptional\n, defaults to 0.0) \u2014\nRatio of total training steps used for a linear warmup from 0 to \nlearning_rate\n.\n \n \n \nwarmup_steps\n (\nint\n, \noptional\n, defaults to 0) \u2014\nNumber of steps used for a linear warmup from 0 to \nlearning_rate\n. Overrides any effect of \nwarmup_ratio\n.\n \n \n \nlog_level\n (\nstr\n, \noptional\n, defaults to \npassive\n) \u2014\nLogger log level to use on the main process. Possible choices are the log levels as strings: \u2018debug\u2019,\n\u2018info\u2019, \u2018warning\u2019, \u2018error\u2019 and \u2018critical\u2019, plus a \u2018passive\u2019 level which doesn\u2019t set anything and keeps the\ncurrent log level for the Transformers library (which will be \n\"warning\"\n by default).\n \n \n \nlog_level_replica\n (\nstr\n, \noptional\n, defaults to \n\"warning\"\n) \u2014\nLogger log level to use on replicas. Same choices as \nlog_level\n\u201d\n \n \n \nlog_on_each_node\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nIn multinode distributed training, whether to log using \nlog_level\n once per node, or only on the main\nnode.\n \n \n \nlogging_dir\n (\nstr\n, \noptional\n) \u2014\n\nTensorBoard\n log directory. Will default to\n*output_dir/runs/\nCURRENT_DATETIME_HOSTNAME*\n.\n \n \n \nlogging_strategy\n (\nstr\n or \nIntervalStrategy\n, \noptional\n, defaults to \n\"steps\"\n) \u2014\nThe logging strategy to adopt during training. Possible values are:\n\n\n\n\n\"no\"\n: No logging is done during training.\n\n\n\"epoch\"\n: Logging is done at the end of each epoch.\n\n\n\"steps\"\n: Logging is done every \nlogging_steps\n.\n\n\n \n \n \nlogging_first_step\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to log the first \nglobal_step\n or not.\n \n \n \nlogging_steps\n (\nint\n or \nfloat\n, \noptional\n, defaults to 500) \u2014\nNumber of update steps between two logs if \nlogging_strategy=\"steps\"\n. Should be an integer or a float in\nrange \n[0,1)\n. If smaller than 1, will be interpreted as ratio of total training steps.\n \n \n \nlogging_nan_inf_filter\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether to filter \nnan\n and \ninf\n losses for logging. If set to \nTrue\n the loss of every step that is \nnan\n\nor \ninf\n is filtered and the average loss of the current logging window is taken instead.\n\n\n\n\nlogging_nan_inf_filter\n only influences the logging of loss values, it does not change the behavior the\ngradient is computed or applied to the model.\n\n\n \n \n \nsave_strategy\n (\nstr\n or \nSaveStrategy\n, \noptional\n, defaults to \n\"steps\"\n) \u2014\nThe checkpoint save strategy to adopt during training. Possible values are:\n\n\n\n\n\"no\"\n: No save is done during training.\n\n\n\"epoch\"\n: Save is done at the end of each epoch.\n\n\n\"steps\"\n: Save is done every \nsave_steps\n.\n\n\n\"best\"\n: Save is done whenever a new \nbest_metric\n is achieved.\n\n\n\n\nIf \n\"epoch\"\n or \n\"steps\"\n is chosen, saving will also be performed at the\nvery end of training, always.\n \n \n \nsave_steps\n (\nint\n or \nfloat\n, \noptional\n, defaults to 500) \u2014\nNumber of updates steps before two checkpoint saves if \nsave_strategy=\"steps\"\n. Should be an integer or a\nfloat in range \n[0,1)\n. If smaller than 1, will be interpreted as ratio of total training steps.\n \n \n \nsave_total_limit\n (\nint\n, \noptional\n) \u2014\nIf a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n\noutput_dir\n. When \nload_best_model_at_end\n is enabled, the \u201cbest\u201d checkpoint according to\n\nmetric_for_best_model\n will always be retained in addition to the most recent ones. For example, for\n\nsave_total_limit=5\n and \nload_best_model_at_end\n, the four last checkpoints will always be retained\nalongside the best model. When \nsave_total_limit=1\n and \nload_best_model_at_end\n, it is possible that two\ncheckpoints are saved: the last one and the best one (if they are different).\n \n \n \nsave_safetensors\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nUse \nsafetensors\n saving and loading for state dicts instead of\ndefault \ntorch.load\n and \ntorch.save\n.\n \n \n \nsave_on_each_node\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhen doing multi-node distributed training, whether to save models and checkpoints on each node, or only on\nthe main one.\n\n\nThis should not be activated when the different nodes use the same storage as the files will be saved with\nthe same names for each node.\n \n \n \nsave_only_model\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhen checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.\nNote that when this is true, you won\u2019t be able to resume training from checkpoint.\nThis enables you to save storage by not storing the optimizer, scheduler & rng state.\nYou can only load the model using \nfrom_pretrained\n with this option set to \nTrue\n.\n \n \n \nrestore_callback_states_from_checkpoint\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to restore the callback states from the checkpoint. If \nTrue\n, will override\ncallbacks passed to the \nTrainer\n if they exist in the checkpoint.\u201d\n \n \n \nuse_cpu\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to use cpu. If set to False, we will use cuda or mps device if available.\n \n \n \nseed\n (\nint\n, \noptional\n, defaults to 42) \u2014\nRandom seed that will be set at the beginning of training. To ensure reproducibility across runs, use the\n\n~Trainer.model_init\n function to instantiate the model if it has some randomly initialized parameters.\n \n \n \ndata_seed\n (\nint\n, \noptional\n) \u2014\nRandom seed to be used with data samplers. If not set, random generators for data sampling will use the\nsame seed as \nseed\n. This can be used to ensure reproducibility of data sampling, independent of the model\nseed.\n \n \n \njit_mode_eval\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to use PyTorch jit trace for inference.\n \n \n \nuse_ipex\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nUse Intel extension for PyTorch when it is available. \nIPEX\ninstallation\n.\n \n \n \nbf16\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\nNVIDIA architecture or Intel XPU or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\n \n \n \nfp16\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n \n \n \nfp16_opt_level\n (\nstr\n, \noptional\n, defaults to \u2018O1\u2019) \u2014\nFor \nfp16\n training, Apex AMP optimization level selected in [\u2018O0\u2019, \u2018O1\u2019, \u2018O2\u2019, and \u2018O3\u2019]. See details on\nthe \nApex documentation\n.\n \n \n \nfp16_backend\n (\nstr\n, \noptional\n, defaults to \n\"auto\"\n) \u2014\nThis argument is deprecated. Use \nhalf_precision_backend\n instead.\n \n \n \nhalf_precision_backend\n (\nstr\n, \noptional\n, defaults to \n\"auto\"\n) \u2014\nThe backend to use for mixed precision training. Must be one of \n\"auto\", \"apex\", \"cpu_amp\"\n. \n\"auto\"\n will\nuse CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the\nrequested backend.\n \n \n \nbf16_full_eval\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\nmetric values. This is an experimental API and it may change.\n \n \n \nfp16_full_eval\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm\nmetric values.\n \n \n \ntf32\n (\nbool\n, \noptional\n) \u2014\nWhether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends\non PyTorch\u2019s version default of \ntorch.backends.cuda.matmul.allow_tf32\n. For more details please refer to\nthe \nTF32\n documentation. This is an\nexperimental API and it may change.\n \n \n \nlocal_rank\n (\nint\n, \noptional\n, defaults to -1) \u2014\nRank of the process during distributed training.\n \n \n \nddp_backend\n (\nstr\n, \noptional\n) \u2014\nThe backend to use for distributed training. Must be one of \n\"nccl\"\n, \n\"mpi\"\n, \n\"ccl\"\n, \n\"gloo\"\n, \n\"hccl\"\n.\n \n \n \ntpu_num_cores\n (\nint\n, \noptional\n) \u2014\nWhen training on TPU, the number of TPU cores (automatically passed by launcher script).\n \n \n \ndataloader_drop_last\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\nor not.\n \n \n \neval_steps\n (\nint\n or \nfloat\n, \noptional\n) \u2014\nNumber of update steps between two evaluations if \neval_strategy=\"steps\"\n. Will default to the same\nvalue as \nlogging_steps\n if not set. Should be an integer or a float in range \n[0,1)\n. If smaller than 1,\nwill be interpreted as ratio of total training steps.\n \n \n \ndataloader_num_workers\n (\nint\n, \noptional\n, defaults to 0) \u2014\nNumber of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\nmain process.\n \n \n \npast_index\n (\nint\n, \noptional\n, defaults to -1) \u2014\nSome models like \nTransformerXL\n or \nXLNet\n can make use of\nthe past hidden states for their predictions. If this argument is set to a positive int, the \nTrainer\n will\nuse the corresponding output (usually index 2) as the past state and feed it to the model at the next\ntraining step under the keyword argument \nmems\n.\n \n \n \nrun_name\n (\nstr\n, \noptional\n, defaults to \noutput_dir\n) \u2014\nA descriptor for the run. Typically used for \nwandb\n,\n\nmlflow\n, \ncomet\n and \nswanlab\n\nlogging. If not specified, will be the same as \noutput_dir\n.\n \n \n \ndisable_tqdm\n (\nbool\n, \noptional\n) \u2014\nWhether or not to disable the tqdm progress bars and table of metrics produced by\n\n~notebook.NotebookTrainingTracker\n in Jupyter Notebooks. Will default to \nTrue\n if the logging level is\nset to warn or lower (default), \nFalse\n otherwise.\n \n \n \nremove_unused_columns\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether or not to automatically remove the columns unused by the model forward method.\n \n \n \nlabel_names\n (\nlist[str]\n, \noptional\n) \u2014\nThe list of keys in your dictionary of inputs that correspond to the labels.\n\n\nWill eventually default to the list of argument names accepted by the model that contain the word \u201clabel\u201d,\nexcept if the model used is one of the \nXxxForQuestionAnswering\n in which case it will also include the\n\n[\"start_positions\", \"end_positions\"]\n keys.\n \n \n \nload_best_model_at_end\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to load the best model found during training at the end of training. When this option is\nenabled, the best checkpoint will always be saved. See\n\nsave_total_limit\n\nfor more.\n\n\n\n\nWhen set to \nTrue\n, the parameters \nsave_strategy\n needs to be the same as \neval_strategy\n, and in\nthe case it is \u201csteps\u201d, \nsave_steps\n must be a round multiple of \neval_steps\n.\n\n\n \n \n \nmetric_for_best_model\n (\nstr\n, \noptional\n) \u2014\nUse in conjunction with \nload_best_model_at_end\n to specify the metric to use to compare two different\nmodels. Must be the name of a metric returned by the evaluation with or without the prefix \n\"eval_\"\n.\n\n\nIf not specified, this will default to \n\"loss\"\n when either \nload_best_model_at_end == True\n\nor \nlr_scheduler_type == SchedulerType.REDUCE_ON_PLATEAU\n (to use the evaluation loss).\n\n\nIf you set this value, \ngreater_is_better\n will default to \nTrue\n unless the name ends with \u201closs\u201d.\nDon\u2019t forget to set it to \nFalse\n if your metric is better when lower.\n \n \n \ngreater_is_better\n (\nbool\n, \noptional\n) \u2014\nUse in conjunction with \nload_best_model_at_end\n and \nmetric_for_best_model\n to specify if better models\nshould have a greater metric or not. Will default to:\n\n\n\n\nTrue\n if \nmetric_for_best_model\n is set to a value that doesn\u2019t end in \n\"loss\"\n.\n\n\nFalse\n if \nmetric_for_best_model\n is not set, or set to a value that ends in \n\"loss\"\n.\n\n\n \n \n \nignore_data_skip\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhen resuming training, whether or not to skip the epochs and batches to get the data loading at the same\nstage as in the previous training. If set to \nTrue\n, the training will begin faster (as that skipping step\ncan take a long time) but will not yield the same results as the interrupted training would have.\n \n \n \nfsdp\n (\nbool\n, \nstr\n or list of \nFSDPOption\n, \noptional\n, defaults to \n''\n) \u2014\nUse PyTorch Distributed Parallel Training (in distributed training only).\n\n\nA list of options along the following:\n\n\n\n\n\"full_shard\"\n: Shard parameters, gradients and optimizer states.\n\n\n\"shard_grad_op\"\n: Shard optimizer states and gradients.\n\n\n\"hybrid_shard\"\n: Apply \nFULL_SHARD\n within a node, and replicate parameters across nodes.\n\n\n\"hybrid_shard_zero2\"\n: Apply \nSHARD_GRAD_OP\n within a node, and replicate parameters across nodes.\n\n\n\"offload\"\n: Offload parameters and gradients to CPUs (only compatible with \n\"full_shard\"\n and\n\n\"shard_grad_op\"\n).\n\n\n\"auto_wrap\"\n: Automatically recursively wrap layers with FSDP using \ndefault_auto_wrap_policy\n.\n\n\n \n \n \nfsdp_config\n (\nstr\n or \ndict\n, \noptional\n) \u2014\nConfig to be used with fsdp (Pytorch Distributed Parallel Training). The value is either a location of\nfsdp json config file (e.g., \nfsdp_config.json\n) or an already loaded json file as \ndict\n.\n\n\nA List of config and its options:\n\n\n\n\n\n\nmin_num_params (\nint\n, \noptional\n, defaults to \n0\n):\nFSDP\u2019s minimum number of parameters for Default Auto Wrapping. (useful only when \nfsdp\n field is\npassed).\n\n\n\n\n\n\ntransformer_layer_cls_to_wrap (\nlist[str]\n, \noptional\n):\nList of transformer layer class names (case-sensitive) to wrap, e.g, \nBertLayer\n, \nGPTJBlock\n,\n\nT5Block\n \u2026 (useful only when \nfsdp\n flag is passed).\n\n\n\n\n\n\nbackward_prefetch (\nstr\n, \noptional\n)\nFSDP\u2019s backward prefetch mode. Controls when to prefetch next set of parameters (useful only when\n\nfsdp\n field is passed).\n\n\nA list of options along the following:\n\n\n\n\n\"backward_pre\"\n : Prefetches the next set of parameters before the current set of parameter\u2019s\ngradient\ncomputation.\n\n\n\"backward_post\"\n : This prefetches the next set of parameters after the current set of\nparameter\u2019s\ngradient computation.\n\n\n\n\n\n\n\n\nforward_prefetch (\nbool\n, \noptional\n, defaults to \nFalse\n)\nFSDP\u2019s forward prefetch mode (useful only when \nfsdp\n field is passed).\nIf \n\"True\"\n, then FSDP explicitly prefetches the next upcoming all-gather while executing in the\nforward pass.\n\n\n\n\n\n\nlimit_all_gathers (\nbool\n, \noptional\n, defaults to \nFalse\n)\nFSDP\u2019s limit_all_gathers (useful only when \nfsdp\n field is passed).\nIf \n\"True\"\n, FSDP explicitly synchronizes the CPU thread to prevent too many in-flight\nall-gathers.\n\n\n\n\n\n\nuse_orig_params (\nbool\n, \noptional\n, defaults to \nTrue\n)\nIf \n\"True\"\n, allows non-uniform \nrequires_grad\n during init, which means support for interspersed\nfrozen and trainable parameters. Useful in cases such as parameter-efficient fine-tuning. Please\nrefer this\n[blog](\nhttps://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019\n\n\n\n\n\n\nsync_module_states (\nbool\n, \noptional\n, defaults to \nTrue\n)\nIf \n\"True\"\n, each individually wrapped FSDP unit will broadcast module parameters from rank 0 to\nensure they are the same across all ranks after initialization\n\n\n\n\n\n\ncpu_ram_efficient_loading (\nbool\n, \noptional\n, defaults to \nFalse\n)\nIf \n\"True\"\n, only the first process loads the pretrained model checkpoint while all other processes\nhave empty weights.  When this setting as \n\"True\"\n, \nsync_module_states\n also must to be \n\"True\"\n,\notherwise all the processes except the main process would have random weights leading to unexpected\nbehaviour during training.\n\n\n\n\n\n\nactivation_checkpointing (\nbool\n, \noptional\n, defaults to \nFalse\n):\nIf \n\"True\"\n, activation checkpointing is a technique to reduce memory usage by clearing activations of\ncertain layers and recomputing them during a backward pass. Effectively, this trades extra\ncomputation time for reduced memory usage.\n\n\n\n\n\n\nxla (\nbool\n, \noptional\n, defaults to \nFalse\n):\nWhether to use PyTorch/XLA Fully Sharded Data Parallel Training. This is an experimental feature\nand its API may evolve in the future.\n\n\n\n\n\n\nxla_fsdp_settings (\ndict\n, \noptional\n)\nThe value is a dictionary which stores the XLA FSDP wrapping parameters.\n\n\nFor a complete list of options, please see \nhere\n.\n\n\n\n\n\n\nxla_fsdp_grad_ckpt (\nbool\n, \noptional\n, defaults to \nFalse\n):\nWill use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be\nused when the xla flag is set to true, and an auto wrapping policy is specified through\nfsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.\n\n\n\n\n \n \n \ndeepspeed\n (\nstr\n or \ndict\n, \noptional\n) \u2014\nUse \nDeepspeed\n. This is an experimental feature and its API may\nevolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\n\nds_config.json\n) or an already loaded json file as a \ndict\n\u201d\n\n\n\n\t\t\t\t\t\t\nIf enabling any Zero-init, make sure that your model is not initialized until\n*after* initializing the `TrainingArguments`, else it will not be applied.\n\n\t\t\t\t\t\n \n \n \naccelerator_config\n (\nstr\n, \ndict\n, or \nAcceleratorConfig\n, \noptional\n) \u2014\nConfig to be used with the internal \nAccelerator\n implementation. The value is either a location of\naccelerator json config file (e.g., \naccelerator_config.json\n), an already loaded json file as \ndict\n,\nor an instance of \nAcceleratorConfig\n.\n\n\nA list of config and its options:\n\n\n\n\nsplit_batches (\nbool\n, \noptional\n, defaults to \nFalse\n):\nWhether or not the accelerator should split the batches yielded by the dataloaders across the devices. If\n\nTrue\n the actual batch size used will be the same on any kind of distributed processes, but it must be a\nround multiple of the \nnum_processes\n you are using. If \nFalse\n, actual batch size used will be the one set\nin your script multiplied by the number of processes.\n\n\ndispatch_batches (\nbool\n, \noptional\n):\nIf set to \nTrue\n, the dataloader prepared by the Accelerator is only iterated through on the main process\nand then the batches are split and broadcast to each process. Will default to \nTrue\n for \nDataLoader\n whose\nunderlying dataset is an \nIterableDataset\n, \nFalse\n otherwise.\n\n\neven_batches (\nbool\n, \noptional\n, defaults to \nTrue\n):\nIf set to \nTrue\n, in cases where the total batch size across all processes does not exactly divide the\ndataset, samples at the start of the dataset will be duplicated so the batch can be divided equally among\nall workers.\n\n\nuse_seedable_sampler (\nbool\n, \noptional\n, defaults to \nTrue\n):\nWhether or not use a fully seedable random sampler (\naccelerate.data_loader.SeedableRandomSampler\n). Ensures\ntraining results are fully reproducible using a different sampling technique. While seed-to-seed results\nmay differ, on average the differences are negligible when using multiple different seeds to compare. Should\nalso be ran with \n~utils.set_seed\n for the best results.\n\n\nuse_configured_state (\nbool\n, \noptional\n, defaults to \nFalse\n):\nWhether or not to use a pre-configured \nAcceleratorState\n or \nPartialState\n defined before calling \nTrainingArguments\n.\nIf \nTrue\n, an \nAccelerator\n or \nPartialState\n must be initialized. Note that by doing so, this could lead to issues\nwith hyperparameter tuning.\n\n\n \n \n \nlabel_smoothing_factor\n (\nfloat\n, \noptional\n, defaults to 0.0) \u2014\nThe label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded\nlabels are changed from 0s and 1s to \nlabel_smoothing_factor/num_labels\n and \n1 - label_smoothing_factor + label_smoothing_factor/num_labels\n respectively.\n \n \n \ndebug\n (\nstr\n or list of \nDebugOption\n, \noptional\n, defaults to \n\"\"\n) \u2014\nEnable one or more debug features. This is an experimental feature.\n\n\nPossible options are:\n\n\n\n\n\"underflow_overflow\"\n: detects overflow in model\u2019s input/outputs and reports the last frames that led to\nthe event\n\n\n\"tpu_metrics_debug\"\n: print debug metrics on TPU\n\n\n\n\nThe options should be separated by whitespaces.\n \n \n \noptim\n (\nstr\n or \ntraining_args.OptimizerNames\n, \noptional\n, defaults to \n\"adamw_torch\"\n) \u2014\nThe optimizer to use, such as \u201cadamw_torch\u201d, \u201cadamw_torch_fused\u201d, \u201cadamw_apex_fused\u201d, \u201cadamw_anyprecision\u201d,\n\u201cadafactor\u201d. See \nOptimizerNames\n in \ntraining_args.py\n\nfor a full list of optimizers.\n \n \n \noptim_args\n (\nstr\n, \noptional\n) \u2014\nOptional arguments that are supplied to optimizers such as AnyPrecisionAdamW, AdEMAMix, and GaLore.\n \n \n \ngroup_by_length\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to group together samples of roughly the same length in the training dataset (to minimize\npadding applied and be more efficient). Only useful if applying dynamic padding.\n \n \n \nlength_column_name\n (\nstr\n, \noptional\n, defaults to \n\"length\"\n) \u2014\nColumn name for precomputed lengths. If the column exists, grouping by length will use these values rather\nthan computing them on train startup. Ignored unless \ngroup_by_length\n is \nTrue\n and the dataset is an\ninstance of \nDataset\n.\n \n \n \nreport_to\n (\nstr\n or \nlist[str]\n, \noptional\n, defaults to \n\"all\"\n) \u2014\nThe list of integrations to report the results and logs to. Supported platforms are \n\"azure_ml\"\n,\n\n\"clearml\"\n, \n\"codecarbon\"\n, \n\"comet_ml\"\n, \n\"dagshub\"\n, \n\"dvclive\"\n, \n\"flyte\"\n, \n\"mlflow\"\n, \n\"neptune\"\n,\n\n\"swanlab\"\n, \n\"tensorboard\"\n, and \n\"wandb\"\n. Use \n\"all\"\n to report to all integrations installed, \n\"none\"\n\nfor no integrations.\n \n \n \nddp_find_unused_parameters\n (\nbool\n, \noptional\n) \u2014\nWhen using distributed training, the value of the flag \nfind_unused_parameters\n passed to\n\nDistributedDataParallel\n. Will default to \nFalse\n if gradient checkpointing is used, \nTrue\n otherwise.\n \n \n \nddp_bucket_cap_mb\n (\nint\n, \noptional\n) \u2014\nWhen using distributed training, the value of the flag \nbucket_cap_mb\n passed to \nDistributedDataParallel\n.\n \n \n \nddp_broadcast_buffers\n (\nbool\n, \noptional\n) \u2014\nWhen using distributed training, the value of the flag \nbroadcast_buffers\n passed to\n\nDistributedDataParallel\n. Will default to \nFalse\n if gradient checkpointing is used, \nTrue\n otherwise.\n \n \n \ndataloader_pin_memory\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether you want to pin memory in data loaders or not. Will default to \nTrue\n.\n \n \n \ndataloader_persistent_workers\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nIf True, the data loader will not shut down the worker processes after a dataset has been consumed once.\nThis allows to maintain the workers Dataset instances alive. Can potentially speed up training, but will\nincrease RAM usage. Will default to \nFalse\n.\n \n \n \ndataloader_prefetch_factor\n (\nint\n, \noptional\n) \u2014\nNumber of batches loaded in advance by each worker.\n2 means there will be a total of 2 * num_workers batches prefetched across all workers.\n \n \n \nskip_memory_metrics\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows\ndown the training and evaluation speed.\n \n \n \npush_to_hub\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to push the model to the Hub every time the model is saved. If this is activated,\n\noutput_dir\n will begin a git directory synced with the repo (determined by \nhub_model_id\n) and the content\nwill be pushed each time a save is triggered (depending on your \nsave_strategy\n). Calling\n\nsave_model()\n will also trigger a push.\n\n\n\n\nIf \noutput_dir\n exists, it needs to be a local clone of the repository to which the \nTrainer\n will be\npushed.\n\n\n \n \n \nresume_from_checkpoint\n (\nstr\n, \noptional\n) \u2014\nThe path to a folder with a valid checkpoint for your model. This argument is not directly used by\n\nTrainer\n, it\u2019s intended to be used by your training/evaluation scripts instead. See the \nexample\nscripts\n for more details.\n \n \n \nhub_model_id\n (\nstr\n, \noptional\n) \u2014\nThe name of the repository to keep in sync with the local \noutput_dir\n. It can be a simple model ID in\nwhich case the model will be pushed in your namespace. Otherwise it should be the whole repository name,\nfor instance \n\"user_name/model\"\n, which allows you to push to an organization you are a member of with\n\n\"organization_name/model\"\n. Will default to \nuser_name/output_dir_name\n with \noutput_dir_name\n being the\nname of \noutput_dir\n.\n\n\nWill default to the name of \noutput_dir\n.\n \n \n \nhub_strategy\n (\nstr\n or \nHubStrategy\n, \noptional\n, defaults to \n\"every_save\"\n) \u2014\nDefines the scope of what is pushed to the Hub and when. Possible values are:\n\n\n\n\n\"end\"\n: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the \nTrainer\n) and a\ndraft of a model card when the \nsave_model()\n method is called.\n\n\n\"every_save\"\n: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the \nTrainer\n) and\na draft of a model card each time there is a model save. The pushes are asynchronous to not block\ntraining, and in case the save are very frequent, a new push is only attempted if the previous one is\nfinished. A last push is made with the final model at the end of training.\n\n\n\"checkpoint\"\n: like \n\"every_save\"\n but the latest checkpoint is also pushed in a subfolder named\nlast-checkpoint, allowing you to resume training easily with\n\ntrainer.train(resume_from_checkpoint=\"last-checkpoint\")\n.\n\n\n\"all_checkpoints\"\n: like \n\"checkpoint\"\n but all checkpoints are pushed like they appear in the output\nfolder (so you will get one checkpoint folder per folder in your final repository)\n\n\n \n \n \nhub_token\n (\nstr\n, \noptional\n) \u2014\nThe token to use to push the model to the Hub. Will default to the token in the cache folder obtained with\n\nhuggingface-cli login\n.\n \n \n \nhub_private_repo\n (\nbool\n, \noptional\n) \u2014\nWhether to make the repo private. If \nNone\n (default), the repo will be public unless the organization\u2019s default is private. This value is ignored if the repo already exists.\n \n \n \nhub_always_push\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nUnless this is \nTrue\n, the \nTrainer\n will skip pushing a checkpoint when the previous push is not finished.\n \n \n \nhub_revision\n (\nstr\n, \noptional\n) \u2014\nThe revision to use when pushing to the Hub. Can be a branch name, a tag, or a commit hash.\n \n \n \ngradient_checkpointing\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nIf True, use gradient checkpointing to save memory at the expense of slower backward pass.\n \n \n \ngradient_checkpointing_kwargs\n (\ndict\n, \noptional\n, defaults to \nNone\n) \u2014\nKey word arguments to be passed to the \ngradient_checkpointing_enable\n method.\n \n \n \ninclude_inputs_for_metrics\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nThis argument is deprecated. Use \ninclude_for_metrics\n instead, e.g, \ninclude_for_metrics = [\"inputs\"]\n.\n \n \n \ninclude_for_metrics\n (\nlist[str]\n, \noptional\n, defaults to \n[]\n) \u2014\nInclude additional data in the \ncompute_metrics\n function if needed for metrics computation.\nPossible options to add to \ninclude_for_metrics\n list:\n\n\n\n\n\"inputs\"\n: Input data passed to the model, intended for calculating input dependent metrics.\n\n\n\"loss\"\n: Loss values computed during evaluation, intended for calculating loss dependent metrics.\n\n\n \n \n \neval_do_concat_batches\n (\nbool\n, \noptional\n, defaults to \nTrue\n) \u2014\nWhether to recursively concat inputs/losses/labels/predictions across batches. If \nFalse\n,\nwill instead store them as lists, with each batch kept separate.\n \n \n \nauto_find_batch_size\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to find a batch size that will fit into memory automatically through exponential decay, avoiding\nCUDA Out-of-Memory errors. Requires accelerate to be installed (\npip install accelerate\n)\n \n \n \nfull_determinism\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nIf \nTrue\n, \nenable_full_determinism()\n is called instead of \nset_seed()\n to ensure reproducible results in\ndistributed training. Important: this will negatively impact the performance, so only use it for debugging.\n \n \n \ntorchdynamo\n (\nstr\n, \noptional\n) \u2014\nIf set, the backend compiler for TorchDynamo. Possible choices are \n\"eager\"\n, \n\"aot_eager\"\n, \n\"inductor\"\n,\n\n\"nvfuser\"\n, \n\"aot_nvfuser\"\n, \n\"aot_cudagraphs\"\n, \n\"ofi\"\n, \n\"fx2trt\"\n, \n\"onnxrt\"\n and \n\"ipex\"\n.\n \n \n \nray_scope\n (\nstr\n, \noptional\n, defaults to \n\"last\"\n) \u2014\nThe scope to use when doing hyperparameter search with Ray. By default, \n\"last\"\n will be used. Ray will\nthen use the last checkpoint of all trials, compare those, and select the best one. However, other options\nare also available. See the \nRay documentation\n for\nmore options.\n \n \n \nddp_timeout\n (\nint\n, \noptional\n, defaults to 1800) \u2014\nThe timeout for \ntorch.distributed.init_process_group\n calls, used to avoid GPU socket timeouts when\nperforming slow operations in distributed runnings. Please refer the [PyTorch documentation]\n(\nhttps://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group\n) for more\ninformation.\n \n \n \nuse_mps_device\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nThis argument is deprecated.\nmps\n device will be used if it is available similar to \ncuda\n device.\n \n \n \ntorch_compile\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to compile the model using PyTorch 2.0\n\ntorch.compile\n.\n\n\nThis will use the best defaults for the \ntorch.compile\n\nAPI\n.\nYou can customize the defaults with the argument \ntorch_compile_backend\n and \ntorch_compile_mode\n but we\ndon\u2019t guarantee any of them will work as the support is progressively rolled in in PyTorch.\n\n\nThis flag and the whole compile API is experimental and subject to change in future releases.\n \n \n \ntorch_compile_backend\n (\nstr\n, \noptional\n) \u2014\nThe backend to use in \ntorch.compile\n. If set to any value, \ntorch_compile\n will be set to \nTrue\n.\n\n\nRefer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n\n\nThis flag is experimental and subject to change in future releases.\n \n \n \ntorch_compile_mode\n (\nstr\n, \noptional\n) \u2014\nThe mode to use in \ntorch.compile\n. If set to any value, \ntorch_compile\n will be set to \nTrue\n.\n\n\nRefer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n\n\nThis flag is experimental and subject to change in future releases.\n \n \n \ninclude_tokens_per_second\n (\nbool\n, \noptional\n) \u2014\nWhether or not to compute the number of tokens per second per device for training speed metrics.\n\n\nThis will iterate over the entire training dataloader once beforehand,\n\n\nand will slow down the entire process.\n \n \n \ninclude_num_input_tokens_seen\n (\nbool\n, \noptional\n) \u2014\nWhether or not to track the number of input tokens seen throughout training.\n\n\nMay be slower in distributed training as gather operations must be called.\n \n \n \nneftune_noise_alpha\n (\nOptional[float]\n) \u2014\nIf not \nNone\n, this will activate NEFTune noise embeddings. This can drastically improve model performance\nfor instruction fine-tuning. Check out the \noriginal paper\n and the\n\noriginal code\n. Support transformers \nPreTrainedModel\n and also\n\nPeftModel\n from peft. The original paper used values in the range [5.0, 15.0].\n \n \n \noptim_target_modules\n (\nUnion[str, list[str]]\n, \noptional\n) \u2014\nThe target modules to optimize, i.e. the module names that you would like to train.\nCurrently used for the GaLore algorithm (\nhttps://huggingface.co/papers/2403.03507\n) and APOLLO algorithm (\nhttps://huggingface.co/papers/2412.05270\n).\nSee GaLore implementation (\nhttps://github.com/jiaweizzhao/GaLore\n) and APOLLO implementation (\nhttps://github.com/zhuhanqing/APOLLO\n) for more details.\nYou need to make sure to pass a valid GaLore or APOLLO optimizer, e.g., one of: \u201capollo_adamw\u201d, \u201cgalore_adamw\u201d, \u201cgalore_adamw_8bit\u201d, \u201cgalore_adafactor\u201d and make sure that the target modules are \nnn.Linear\n modules only.\n \n \n \nbatch_eval_metrics\n (\nOptional[bool]\n, defaults to \nFalse\n) \u2014\nIf set to \nTrue\n, evaluation will call compute_metrics at the end of each batch to accumulate statistics\nrather than saving all eval logits in memory. When set to \nTrue\n, you must pass a compute_metrics function\nthat takes a boolean argument \ncompute_result\n, which when passed \nTrue\n, will trigger the final global\nsummary statistics from the batch-level summary statistics you\u2019ve accumulated over the evaluation set.\n \n \n \neval_on_start\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to perform a evaluation step (sanity check) before the training to ensure the validation steps works correctly.\n \n \n \neval_use_gather_object\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to run recursively gather object in a nested list/tuple/dictionary of objects from all devices. This should only be enabled if users are not just returning tensors, and this is actively discouraged by PyTorch.\n \n \n \nuse_liger_kernel\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether enable \nLiger\n Kernel for LLM model training.\nIt can effectively increase multi-GPU training throughput by ~20% and reduces memory usage by ~60%, works out of the box with\nflash attention, PyTorch FSDP, and Microsoft DeepSpeed. Currently, it supports llama, mistral, mixtral and gemma models.\n \n \n \nliger_kernel_config\n (\nOptional[dict]\n, \noptional\n) \u2014\nConfiguration to be used for Liger Kernel. When use_liger_kernel=True, this dict is passed as keyword arguments to the\n\n_apply_liger_kernel_to_instance\n function, which specifies which kernels to apply. Available options vary by model but typically\ninclude: \u2018rope\u2019, \u2018swiglu\u2019, \u2018cross_entropy\u2019, \u2018fused_linear_cross_entropy\u2019, \u2018rms_norm\u2019, etc. If \nNone\n, use the default kernel configurations.\n \n \n \naverage_tokens_across_devices\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether or not to average tokens across devices. If enabled, will use all_reduce to synchronize\nnum_tokens_in_batch for precise loss calculation. Reference:\n\nhttps://github.com/huggingface/transformers/issues/34242\n \n \n \npredict_with_generate\n (\nbool\n, \noptional\n, defaults to \nFalse\n) \u2014\nWhether to use generate to calculate generative metrics (ROUGE, BLEU).\n \n \n \ngeneration_max_length\n (\nint\n, \noptional\n) \u2014\nThe \nmax_length\n to use on each evaluation loop when \npredict_with_generate=True\n. Will default to the\n\nmax_length\n value of the model configuration.\n \n \n \ngeneration_num_beams\n (\nint\n, \noptional\n) \u2014\nThe \nnum_beams\n to use on each evaluation loop when \npredict_with_generate=True\n. Will default to the\n\nnum_beams\n value of the model configuration.\n \n \n \ngeneration_config\n (\nstr\n or \nPath\n or \nGenerationConfig\n, \noptional\n) \u2014\nAllows to load a \nGenerationConfig\n from the \nfrom_pretrained\n method. This can be either:\n\n\n\n\na string, the \nmodel id\n of a pretrained model configuration hosted inside a model repo on\nhuggingface.co.\n\n\na path to a \ndirectory\n containing a configuration file saved using the\n\nsave_pretrained()\n method, e.g., \n./my_model_directory/\n.\n\n\na \nGenerationConfig\n object.\n\n\n \n \n \n \nTrainingArguments is the subset of the arguments we use in our example scripts \nwhich relate to the training loop\nitself\n.\n \nUsing \nHfArgumentParser\n we can turn this class into\n\nargparse\n arguments that can be specified on the\ncommand line.\n \n \nto_dict\n \n \n<\n \nsource\n \n>\n \n(\n \n)\n \n \n \n \nSerializes this instance while replace \nEnum\n by their values and \nGenerationConfig\n by dictionaries (for JSON\nserialization support). It obfuscates the token values by removing their value.\n \n<\n \n>\n \nUpdate\n on GitHub\n \n\n\n\n\n\n\n\u2190\nTokenizer\n\n\nDeepSpeed\n\u2192\n\n\n\n\n\n\nTrainer\n\n\nTrainer\n\n\nSeq2\nSeq\nTrainer\n\n\nTraining\nArguments\n\n\nSeq2\nSeq\nTraining\nArguments\n\n\n\n",
            "question": "How do I initialize the BERT tokenizer?",
            "answers": [
                {
                    "text": "BertTokenizer.from_pretrained('bert-base-uncased')",
                    "answer_start": -1
                }
            ]
        }
    ]
}